{
  "page": 62,
  "title": "Web Agents: Feedback and Reflection",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 6.5.2 &middot; Self-Evolving (continued)</div>\n                        <h1>Agentic Feedback and Reflection for Web Agents</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>Long-term Memory for Research Agents</h3>\n                        <p>Long-term memory is crucial for autonomous research agents because it enables accumulation and reuse of prior knowledge, fostering continuity across research cycles. For example, Agent Laboratory [654] retains prior experiment code, results, and interpretation across its multi-phase workflow, enabling later stages to build on earlier work. GPT Researcher [635] generates reports with embedded citations and provides context for planning and extension of research topics.</p>\n                        <p>Chain of Ideas [636] structures relevant literature into a chain scaffold that reflects a field's progression and can be revisited as new evidence arises. The AI Scientist-v2 [530] incorporates a progressive agentic tree-search approach that enables branching, backtracking and follow-up experimentation across iterations.</p>\n                        <h3>Agentic Feedback and Reflection</h3>\n                        <p>Modern web agents treat interaction as a continual learning process, using feedback signals and reflection modules to refine their reasoning and recover from failures over time. Agent Q [113] combines guided Monte Carlo tree search with a self-critique stage, so that rollouts provide not only action sequences but also preference-style supervision. ReAP [668] makes reflection explicit by treating it as a retrieval problem: it stores task-reflection key-value pairs summarizing what was learned from past trajectories, then, at inference time, retrieves the most relevant reflections and appends them to the agent's prompt to guide planning on new web-navigation tasks.</p>\n                        <p>Agent-E [669] introduces an automatic validation pipeline that detects execution errors across text and vision, and then triggers self-refinement, enabling agents to iteratively correct their own workflows. Recon-Act [670] uses a dual-team architecture in which a Reconnaissance team extracts generalized tools from successful and failed trajectories, and an Action team applies these tools to re-plan tasks, forming a closed feedback loop.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Reflection Mechanisms</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Feedback Patterns</h5>\n                                <ul>\n                                    <li><strong>Self-Critique:</strong> Monte Carlo tree search with evaluation</li>\n                                    <li><strong>Reflection Retrieval:</strong> Past lessons applied to new tasks</li>\n                                    <li><strong>Error Detection:</strong> Automatic validation and correction</li>\n                                    <li><strong>Dual-Team Learning:</strong> Tool extraction + application</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}