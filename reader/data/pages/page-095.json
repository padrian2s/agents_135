{
  "page": 95,
  "title": "References (Part 21)",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">References</div>\n                        <h1>References (Part 21)</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>Continued References [270-282]</h3>\n                        <ul>\n                            <li><strong>[270]</strong> Aman Madaan et al. - Self-refine: Iterative refinement with self-feedback (2024)</li>\n                            <li><strong>[271]</strong> Ziqi Wang et al. - Enable language models to implicitly learn self-improvement from data (2024)</li>\n                            <li><strong>[272]</strong> Xuezhi Wang et al. - Self-consistency improves chain of thought reasoning in language models (2023)</li>\n                            <li><strong>[273]</strong> Wenhu Chen et al. - Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (2023)</li>\n                            <li><strong>[274]</strong> Aohan Zeng et al. - Agenttuning: Enabling generalized agent abilities for llms (2024)</li>\n                            <li><strong>[275]</strong> Cheng-Yu Hsieh et al. - Distilling step-by-step! Outperforming larger language models with less training data and smaller model sizes (2023)</li>\n                            <li><strong>[276]</strong> Paul F Christiano et al. - Deep reinforcement learning from human preferences (2017)</li>\n                            <li><strong>[277]</strong> Rafael Rafailov et al. - Direct preference optimization: Your language model is secretly a reward model (2023)</li>\n                            <li><strong>[278]</strong> Yuntao Bai et al. - Constitutional ai: Harmlessness from ai feedback (2022)</li>\n                            <li><strong>[279]</strong> Jiaqi Li et al. - Reflecevo: Improving meta introspection of small llms by learning self-reflection (2025)</li>\n                            <li><strong>[280]</strong> Zhi Zheng and Wee Sun Lee - Reasoning-cv: Fine-tuning powerful reasoning llms for knowledge-assisted claim verification (2025)</li>\n                            <li><strong>[281]</strong> Alan Dao and Thinh Le - Retero: Enhancing llm search ability by trying one-more-time (2025)</li>\n                            <li><strong>[282]</strong> Nearcbos Potamitis and Akhil Arora - Are retrials all you need? Enhancing large language model reasoning without verbalized feedback (2025)</li>\n                        </ul>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Self-Improvement and Alignment</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Key Methods</h5>\n                                <ul>\n                                    <li><strong>Self-Refine:</strong> Iterative self-improvement</li>\n                                    <li><strong>DPO:</strong> Direct preference optimization</li>\n                                    <li><strong>Constitutional AI:</strong> AI feedback alignment</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}