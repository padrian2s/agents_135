{
  "page": 71,
  "title": "Scientific Discovery and Research Agent Benchmarks",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 7.2.2-7.2.3 &middot; Scientific & Research</div>\n                        <h1>Scientific Discovery and Autonomous Research Benchmarks</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>7.2.2. Scientific Discovery Agents</h3>\n                        <p>Scientific benchmarks aim to test agents' capabilities in knowledge acquisition, hypothesis generation, and experimental automation. DISCOVERYWORLD [757] introduces a virtual lab where agents explore scientific phenomena in biology, chemistry, and physics through simulated tools and instruments. ScienceWorld [758] focuses on elementary science experiments using textual instructions and environment interaction, requiring step-by-step hypothesis testing.</p>\n                        <p>ScienceAgentBench [759] builds a benchmark from real-world scientific papers, translating tasks like code implementation, figure generation, and variable extraction into executable subtasks, assessing agents' ability to automate the research process. The AI Scientist [651] simulates a full end-to-end research pipeline, where agents perform literature review, method writing, experiment execution, and peer-review simulation.</p>\n                        <h3>7.2.3. Autonomous Research Agents</h3>\n                        <p>This category benchmarks agents designed for long-horizon workflows across general-purpose research, office, or planning tasks. WorkArena [762] and its extension WorkArena++ [763] propose enterprise task benchmarks where agents must complete ticket-based workflows involving retrieval, summarization, and coordination across documents.</p>\n                        <p>OfficeBench [764] simulates a productivity software suite environment with tasks such as creating meeting memos, modifying spreadsheets, and replying to emails, emphasizing goal decomposition and tool selection. PlanBench [723] and FlowBench [728] test general workflow planning skills with abstracted task graphs and structured dependencies. ACPBench [724] evaluates agents in assistant-collaborator-planner triads, tracking performance in a hybrid role hierarchy.</p>\n                        <p>TRAIL [765] focuses on multi-agent trace debugging and error attribution [766] in LLM-based systems, providing dense annotations for reasoning chains. CLIN [767] introduces lifelong few-shot learning benchmarks where agents adapt to distribution shift and task evolution. Agent-as-a-Judge [768] studies peer-review style evaluation with agents grading reasoning chains and correctness of other agents' outputs.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Research Agent Evaluation</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Benchmark Scope</h5>\n                                <ul>\n                                    <li><strong>Scientific Discovery:</strong> Hypothesis testing, experimentation</li>\n                                    <li><strong>Autonomous Research:</strong> Full pipeline automation</li>\n                                    <li><strong>Enterprise Tasks:</strong> Office workflows, ticket resolution</li>\n                                    <li><strong>Lifelong Learning:</strong> Adaptation over time</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}