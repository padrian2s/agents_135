{
  "page": 65,
  "title": "Tool Use Benchmarks (Figure 12)",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 7.1.1 &middot; Tool Use</div>\n                        <h1>Tool Use Benchmarks</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <div class=\"figure-box\">\n                            <h4>Figure 12: Overview of Benchmarks on Agentic Reasoning</h4>\n                            <p>The figure categorizes benchmarks into two main groups:</p>\n                            <ul>\n                                <li><strong>Ability-centric Benchmarks (Section 7.1):</strong> Tool Use, Search, Memory and Planning, Multi-Agent, Language-Anchored</li>\n                                <li><strong>Application-centric Benchmarks (Section 7.2):</strong> Embodied Agents, Scientific Discovery, Autonomous Research, Medical/Clinical, Web Agents, General Purpose</li>\n                            </ul>\n                        </div>\n                        <h3>7.1.1. Tool Use</h3>\n                        <p>Evaluating tool-using models remains an open challenge due to the diversity of tasks, tools, and usage scenarios involved [687]. The key difficulties arise from the wide range of available tools, varying levels of scenario complexity, and the prevalence requirements specifically for the task domain.</p>\n                        <p><strong>Single-Turn Tool Use.</strong> While agentic reasoning often focuses on multi-turn or long-horizon interactions, single-turn tool use remains a foundational capability for evaluating LLMs' basic tool invocation skills. ToolQA [688] constructs a dataset of 1,530 dialogues involving 13 specialized tools, designed to assess LLMs' ability to interface with external knowledge sources in a question-answering context.</p>\n                        <p>APIBench [78] introduces a large-scale benchmark grounded in real-world APIs from HuggingFace, TorchHub, and TensorHub, comprising 1,645 unique APIs and 16,450 instruction-API pairs. It is used to train and evaluate Gorilla, an LLM capable of invoking a broad range of APIs, emphasizing generalization across diverse tool interfaces.</p>\n                        <p><strong>Multi-Turn Tool Use.</strong> Multi-turn tool use offers a more realistic simulation of real-world applications, where agents autonomously select and sequence tools to solve complex tasks. ToolAlpaca [204] is one of the earliest efforts in this direction, using multi-agent simulation to generate 3,938 tool-use instances from over 400 real-world APIs across 50 distinct categories.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Tool Use Evaluation</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Benchmark Categories</h5>\n                                <ul>\n                                    <li><strong>Single-Turn:</strong> Basic tool invocation skills</li>\n                                    <li><strong>Multi-Turn:</strong> Sequential tool selection and composition</li>\n                                    <li><strong>Real-World APIs:</strong> Generalization across diverse interfaces</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}