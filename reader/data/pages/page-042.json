{
  "page": 42,
  "title": "Training Multi-Agent to Evolve",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 5.3.3 &middot; Training Multi-agent to Evolve</div>\n                        <h1>Training Multi-Agent Systems to Evolve</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <p>Recent advancements have shifted multi-agent systems from fixed, hand-designed coordination toward training paradigms that enable agents to evolve over time. Training multi-agent systems to evolve represents a critical step toward realizing adaptive, long-horizon intelligence beyond static coordination.</p>\n                        <p>In this emerging paradigm, agents improve collectively through interaction, feedback, and shared memory, rather than isolated or independently optimized behaviors. By embedding reasoning into the learning loop, via reinforcement learning, self-play, curriculum evolution, and verifier-driven feedback, multi-agent systems can internalize coordination strategies, address inter-agent credit assignment, and progressively refine divisions of labor.</p>\n                        <h3>Co-evolution via Interaction and Intrinsic Feedback</h3>\n                        <p>A growing body of work has operationalized multi-agent evolution through explicit training objectives that couple interaction, feedback, and role specialization. For instance, Multi-Agent Evolve [446] instantiates a closed-loop co-evolution framework containing three interacting roles (Proposer, Solver, and Judge), all of which are derived from a shared LLM backbone and jointly optimized via reinforcement learning.</p>\n                        <p>This forms a self-improving curriculum that enables collective skill growth without external supervision. In a related spirit, CoMAS [451] emphasizes intrinsic interaction rewards, extracting learning signals directly from multi-agent discussion dynamics through an LLM-based judge, thereby enabling decentralized co-evolution driven purely by collaborative interaction.</p>\n                        <h3>Multi-Agent Reinforcement Fine-Tuning for Collective Adaptation</h3>\n                        <p>Additional works have focused on principled reinforcement fine-tuning frameworks tailored to LLM-based multi-agent systems. For example, MARFT [447] formalizes multi-agent reinforcement fine-tuning by highlighting key mismatches between classical MARL assumptions and LLM-based agent organizations, such as role heterogeneity, dynamic coordination, and long-horizon dialogue.</p>\n                        <h3>Role Specialization and Joint Credit Assignment</h3>\n                        <p>Other approaches have explored structured role specialization and joint credit assignment. MALT [452] trains sequential pipelines of heterogeneous agents using trajectory expansion and outcome-based reinforcement signals, allowing each agent to improve its specialized function while optimizing end-to-end collaborative performance.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Evolution Mechanisms</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Key Training Approaches</h5>\n                                <ul>\n                                    <li><strong>Co-evolution:</strong> Agents learn together through interaction feedback</li>\n                                    <li><strong>Reinforcement Fine-Tuning:</strong> RL-based optimization for multi-agent coordination</li>\n                                    <li><strong>Role Specialization:</strong> Agents develop distinct specialized capabilities</li>\n                                    <li><strong>Credit Assignment:</strong> Properly attributing success to individual agent contributions</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}