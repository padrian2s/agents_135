{
  "page": 66,
  "title": "Search Benchmarks",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 7.1.2 &middot; Search</div>\n                        <h1>Search Benchmarks</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>7.1.2. Search</h3>\n                        <p>To systematically assess an agent's ability to acquire information through interaction, recent benchmarks cast search as a sequential reasoning problem and can be broadly categorized into unimodal and multimodal settings, differing in the nature of evidence sources, interaction spaces, and grounding requirements.</p>\n                        <h3>Unimodal Search</h3>\n                        <p>Recent benchmarks for single-modal agentic search increasingly frame information seeking as a sequential, decision-driven process, emphasizing planning, interaction, and evidence synthesis. For example, WebWalker [697] emphasizes structured website traversal, explicitly modeling search as coordinated horizontal exploration and vertical drilling across interconnected pages.</p>\n                        <p>To reflect realistic open-world information seeking, InfoDeepSeek [698] introduces a dynamic Web setting with verifiable yet non-curated answers, highlighting robustness to noise and distributional shift. Several benchmarks scale along temporal and informational dimensions: Mind2Web 2 [50] focuses on long-horizon browsing and citation-grounded synthesis, whereas RAVine [699] augments answer quality with process-level efficiency and interaction fidelity.</p>\n                        <p>Complementarily, WideSearch [700] and DeepWideSearch [701] distinguish between breadth-oriented large-scale fact aggregation and depth-oriented multi-hop reasoning, revealing the difficulty of jointly optimizing coverage and reasoning coherence.</p>\n                        <h3>Multimodal Search</h3>\n                        <p>Recent benchmarks on multimodal agentic search move beyond static multimodal question answering to systematically evaluate an agent's ability to actively retrieve, browse, and reason over heterogeneous information sources under realistic constraints. Benchmarks such as MMSearch [705] and its extension MMSearch-Plus [706] frame multimodal search as an end-to-end process, where agents must interpret multimodal queries and synthesize answers by jointly leveraging textual and visual evidence, explicitly modeling different input-output modality configurations.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Search Benchmark Dimensions</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Key Distinctions</h5>\n                                <ul>\n                                    <li><strong>Unimodal vs Multimodal:</strong> Text-only vs. text+vision</li>\n                                    <li><strong>Breadth vs Depth:</strong> Wide fact gathering vs. multi-hop reasoning</li>\n                                    <li><strong>Static vs Dynamic:</strong> Fixed corpora vs. real-world web</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}