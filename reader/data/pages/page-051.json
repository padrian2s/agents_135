{
  "page": 51,
  "title": "Collective Multi-Agent Scientific Reasoning",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 6.2.3 &middot; Collective</div>\n                        <h1>Collective Multi-Agent Reasoning in Science</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>6.2.3. Collective Multi-agent Reasoning</h3>\n                        <p>Multi-agent frameworks for scientific discovery distribute labor across specialized LLM-driven roles, where advanced LLM reasoning not only orchestrates coordination between scientific agents but also adjudicates conflicting evidence to maintain coherence in the process.</p>\n                        <p>To illustrate, we introduce some important multi-agent frameworks as follows. Firstly, ProtAgents [507] exemplifies this pattern in protein design. The framework involve agents for literature retrieval, structure analysis, physics simulation, and results analysis. Specifically, the backbone LLM directs reasoning over multi-modal outputs, choosing when to iterate or convergence-check based on feedback signals.</p>\n                        <p>PaFlow [538], on the other hand, instantiates reasoning as principle-aware uncertainty reduction with a multi-agent loop in which a Planner agent relays strategy to a Hypothesis agent and a validation loop, explicitly tying multi-agent communication to hypothesis-evidence alignment. AtomAgents [523] also brings similar role specialization to alloy discovery. In particular, the agent uses LLM-guided reasoning to control over when to trigger simulations and how to evaluate multi-modal results, letting reasoning allocate computational resources and prune alloy candidates.</p>\n                        <h3>6.3. Embodied Agents</h3>\n                        <p>Embodied agents extend reasoning beyond text, anchoring language in robotic perception, manipulation and navigation. By embedding LLMs within robotic and simulated bodies, these embodied agents tackle real-world generalization, continual adaptation and multi-modal grounding.</p>\n                        <p>In this subsection, we begin with the foundational layer (Section 6.3.1), which covers long-horizon embodied planning, tool-assisted perception, manipulation and execution. Building upon these capabilities, the self-evolving layer (Section 6.3.2) introduces agentic memory, feedback and self-reflection capabilities enabling robots to refine control policies, adapt to novel environments and improve performance through continual interaction.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Scientific Multi-Agent Patterns</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Role Distribution</h5>\n                                <ul>\n                                    <li><strong>Planner:</strong> High-level strategy and goal decomposition</li>\n                                    <li><strong>Hypothesis Agent:</strong> Generates and refines scientific hypotheses</li>\n                                    <li><strong>Validator:</strong> Checks evidence and confirms/rejects ideas</li>\n                                    <li><strong>Specialist:</strong> Domain-specific analysis (structure, simulation)</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}