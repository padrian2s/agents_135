{
  "page": 82,
  "title": "References (Part 8)",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">References</div>\n                        <h1>References (Part 8)</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>Continued References [104-118]</h3>\n                        <ul>\n                            <li><strong>[104]</strong> Xue Jiang et al. - Self-planning code generation with large language models (2024)</li>\n                            <li><strong>[105]</strong> Dhruv Shah et al. - Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action (2023)</li>\n                            <li><strong>[106]</strong> Elan Markowitz et al. - Tree-of-traversals: A zero-shot reasoning algorithm for augmenting black-box language models with knowledge graphs (2024)</li>\n                            <li><strong>[107]</strong> Jieyi Long - Large language model guided tree-of-thought (2023)</li>\n                            <li><strong>[108]</strong> Jing Yu Koh et al. - Tree search for language model agents (2024)</li>\n                            <li><strong>[109]</strong> Chaojie Wang et al. - Q*: Improving multi-step reasoning for llms with deliberative planning (2024)</li>\n                            <li><strong>[110]</strong> Silin Meng et al. - Llm-a*: Large language model enhanced incremental heuristic search on path planning (2024)</li>\n                            <li><strong>[111]</strong> Gang Liu et al. - Multimodal large language models for inverse molecular design with retrosynthetic planning (2024)</li>\n                            <li><strong>[112]</strong> Shibo Hao et al. - Reasoning with language model is planning with world model (2023)</li>\n                            <li><strong>[113]</strong> Pranav Putta et al. - Agent q: Advanced reasoning and learning for autonomous AI agents (2024)</li>\n                            <li><strong>[114]</strong> Henry W Sprueill et al. - Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design (2024)</li>\n                            <li><strong>[115]</strong> Xiao Yu et al. - Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning (2023)</li>\n                            <li><strong>[116]</strong> Ziniu Zhao et al. - Large language models as commonsense knowledge for large-scale task planning (2023)</li>\n                            <li><strong>[117]</strong> Ruomeng Ding et al. - Everything of thoughts: Defying the law of penrose triangle for thought generation (2023)</li>\n                            <li><strong>[118]</strong> Ziru Chen et al. - When is tree search useful for llm planning? It depends on the discriminator (2024)</li>\n                        </ul>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Tree Search and Planning</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Research Focus</h5>\n                                <ul>\n                                    <li><strong>Search Methods:</strong> Tree-of-thought, MCTS, A*</li>\n                                    <li><strong>Domain Applications:</strong> Robotics, chemistry, dialogue</li>\n                                    <li><strong>World Models:</strong> Planning with learned world models</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}