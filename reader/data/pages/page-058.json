{
  "page": 58,
  "title": "Web Agents: Foundational Reasoning",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 6.5.1 &middot; Web Agents Foundational</div>\n                        <h1>Foundational Agentic Reasoning for Web Agents</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <p>GUI agents go further by manipulating software interfaces and multi-modal dashboards directly (i.e. clicking, typing, navigating) to execute experiments, data workflows and interface-based tasks. Autonomous research agents sit at the top of this hierarchy, pairing LLM reasoners with scientific workflows, tool-chains and meta-loops to drive hypothesis generation, data synthesis and paper writing.</p>\n                        <p>The core connection is a progression of autonomy: first web agents retrieve evidence from online resources, then GUI agents operationalize actions inside software interfaces, and finally autonomous research agents orchestrate full scientific workflows end-to-end. While web agents, GUI agents and autonomous agents share common themes of goal-directed autonomy, tool-use and iterative improvement, they differ in where they act on, how they manipulate their environment and what goal they aim to achieve.</p>\n                        <h3>6.5.1. Foundational Agentic Reasoning</h3>\n                        <p><strong>Planning.</strong> Planning is essential for web agents because they must decompose long-horizon tasks into manageable steps, adapt to dynamic pages and coordinate tool/invocation strategies. Early work such as WebGPT [258] fine-tuned GPT-3 [603] to answer open-ended questions via a text-based web-browser interface. Then, various web-based methods deepened the planning paradigm: for example, SEEACT [604] explored large multi-modal models as generalists that integrating visual and HTML grounding for web-based tasks, and AutoWebGLM [605] introduced HTML simplification and various learning techniques for open-domain web task decomposition and navigation.</p>\n                        <p>These works paved the way for recent systems such as Agent Q [113] that integrate guided MCTS, self-critique and off-policy preference optimization on web-task benchmarks, and set the stage for even more advanced long-horizon web planners such as WebExplorer [606] and WebSailor [41].</p>\n                        <p>In addition, reinforcement learning has become a core tool for improving the decision-making and planning behavior of web-based LLM agents. WebRL [437] introduces a self-evolving online curriculum that generates new tasks from unsuccessful attempts and trains an outcome-supervised reward model to guide policy optimization.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Web Agent Hierarchy</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Agent Progression</h5>\n                                <ul>\n                                    <li><strong>Web Agents:</strong> Navigate and retrieve from web resources</li>\n                                    <li><strong>GUI Agents:</strong> Interact with software interfaces</li>\n                                    <li><strong>Research Agents:</strong> Orchestrate full scientific workflows</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}