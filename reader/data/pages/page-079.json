{
  "page": 79,
  "title": "References (Part 5)",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">References</div>\n                        <h1>References (Part 5)</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>Continued References [64-76]</h3>\n                        <ul>\n                            <li><strong>[64]</strong> Fanbin Lu et al. - Arpo: End-to-end policy optimization for GUI agents with experience replay (2025)</li>\n                            <li><strong>[65]</strong> Qiying Yu et al. - Daxpo: An open-source LLM reinforcement learning system at scale (2025)</li>\n                            <li><strong>[66]</strong> Qingyun Wu et al. - Autogen: Enabling next-gen LLM applications via multi-agent conversations (2024)</li>\n                            <li><strong>[67]</strong> Guohao Li et al. - Camel: Communicative agents for mind exploration of large language model society (2024)</li>\n                            <li><strong>[68]</strong> Mingchen Zhuge et al. - Gptswarm: Language agents as optimizable graphs (2024)</li>\n                            <li><strong>[69]</strong> Haoyang Hong et al. - Multi-agent deep research: Training multi-agent systems with m-grpo (2025)</li>\n                            <li><strong>[70]</strong> Alexander Novikov et al. - Alphaevolve: A coding agent for scientific and algorithmic discovery (2025)</li>\n                            <li><strong>[71]</strong> Binfeng Xu et al. - REWOO: Decoupling reasoning from observations for efficient augmented language models (2023)</li>\n                            <li><strong>[72]</strong> Bo Liu et al. - LLM+P: Empowering large language models with optimal planning proficiency (2023)</li>\n                            <li><strong>[73]</strong> Karthik Valmeekam et al. - On the planning abilities of large language models: A critical investigation (2023)</li>\n                            <li><strong>[74]</strong> Maciej Besta et al. - Graph of thoughts: Solving elaborate problems with large language models (2024)</li>\n                            <li><strong>[75]</strong> Bilgehan Sel et al. - Algorithm of thoughts: Enhancing exploration of ideas in large language models (2023)</li>\n                            <li><strong>[76]</strong> Runquan Gui et al. - Hypertree planning: Enhancing LLM reasoning via hierarchical thinking (2025)</li>\n                        </ul>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Multi-Agent and Planning References</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Key Contributions</h5>\n                                <ul>\n                                    <li><strong>Multi-Agent Frameworks:</strong> AutoGen, CAMEL, GPTSwarm</li>\n                                    <li><strong>Planning Methods:</strong> LLM+P, Graph of Thoughts</li>\n                                    <li><strong>Scientific Discovery:</strong> AlphaEvolve</li>\n                                    <li><strong>Reasoning Structures:</strong> Tree, graph, and hypertree approaches</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}