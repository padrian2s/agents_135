{
  "page": 74,
  "title": "Long-Horizon and World Model Challenges",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 8.2-8.3 &middot; Long-Horizon & World Models</div>\n                        <h1>Long-Horizon Reasoning and World Models</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>8.2. Long-horizon Agentic Reasoning from Extended Interaction</h3>\n                        <p>A central open challenge in agentic reasoning is robust long-horizon planning and credit assignment across extended interactions. While methods such as ReAct and Tree of Thought improve short-horizon reasoning [5, 4], errors still compound rapidly in long tasks, as illustrated by embodied agents like Voyager [36]. RL-trained agents such as WebRL and Agent-R1 improve performance in realistic environments but rely on heavily engineered, domain-specific rewards and largely treat episodes independently [437, 28].</p>\n                        <p>More recent process-aware approaches attempt to construct finer-grained credit signals [784, 15, 785], yet remain environment-specific. A core open problem is how to assign credit across tokens, tool calls, skills, and memory updates, and to generalize such learning across a long sequence of episodes and tasks.</p>\n                        <h3>8.3. Agentic Reasoning with World Models</h3>\n                        <p>World-model-based agents [786, 316] aim to mitigate myopic reasoning by enabling internal simulation and lookahead. Model-based RL systems such as DreamerV3 demonstrate the effectiveness of imagined rollouts for long-horizon control [787], while recent LLM-based agents adopt world models to web, code, and GUI environments [788, 786, 789, 790].</p>\n                        <p>However, current designs rely on ad hoc representations and are typically trained on short-horizon or environment-specific data, raising concerns about calibration and generalization. Only a few works explore co-evolving world models and agents over long time scales [610, 791]. An open problem is how to jointly train, update, and evaluate world models in non-stationary environments, and how to assess their causal impact on downstream planning reliability.</p>\n                        <h3>8.4. Multi-agent Collaborative Reasoning and Training</h3>\n                        <p>Multi-agent collaboration has emerged as a powerful paradigm for scaling agentic reasoning through role specialization and division of labor [67, 792, 66]. While debate- and role-based systems often outperform single agents, most collaboration structures are still manually designed. Recent multi-agent RL approaches begin to treat collaboration itself as a trainable skill [409, 413, 26], but credit assignment at the group level remains poorly understood.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Key Open Problems</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Research Gaps</h5>\n                                <ul>\n                                    <li><strong>Credit Assignment:</strong> Attributing success across long horizons</li>\n                                    <li><strong>World Model Training:</strong> Joint optimization with agents</li>\n                                    <li><strong>Calibration:</strong> Model reliability in novel environments</li>\n                                    <li><strong>Group-Level Learning:</strong> Multi-agent credit assignment</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}