{
  "page": 59,
  "title": "Web Agents: Tool-Use and Planning",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 6.5.1 &middot; Web Agents (continued)</div>\n                        <h1>Tool-Use and Advanced Planning for Web Agents</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>Planning (continued)</h3>\n                        <p>For web agents, tool-use abilities underpins execute plans in realistic, dynamic environments. For example, WebVoyager [638] systematizes multi-modal execution by building an end-to-end agent that operates on real websites. On the interaction side, BrowserAgent [639] makes the action space more human-like, defining a compact set of browser primitives (e.g., click, scroll, type) and coupling them with an explicit memory mechanism to maintain key coordinates across steps, yielding strong gains on multi-touch QA benchmarks.</p>\n                        <p>Similarly, methods like WAIT [641] and WebShaper [642] push tool use from mere execution toward tool discovery and data-centric interaction. Specifically, WAIT teaches agents to reverse-engineer reusable tools from website functionality, while WebScanner and WebShaper embed web actions inside multi-turn information-seeking and dataset-synthesis loops, respectively.</p>\n                        <p>Tool use is another core capability for GUI agents, enabling them to invoke system functions and application features as structured tools. As pioneering systems, AutoDroid [643] automatically analyzes Android apps to construct functionality-aware UI abstractions that LLM agents can reason over as capabilities rather than raw layouts, while its successor AutoDroid-V2 [644] re-frames mobile UI automation as LLM-driven code generation, with an on-device small language model emitting executable scripts for a local interpreter.</p>\n                        <p>MobileExperts [645] models each expert as a tool-capable specialist and uses a dual-layer controller to select which expert and its associated tool-set to invoke at different stages of a mobile workflow. AgentStore [646] pushes this idea to the platform level by treating heterogeneous agents themselves as tools: a MetaAgent uses AgentToken to route operating-system subtasks to the most suitable specialized \"tool-agent\" through a unified interface. OS-Copilot [613] and OSCAR [618] integrate rich system-level tools into unified computer-control frameworks, so that complex desktop tasks are expressed as sequences of tool calls.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>GUI Agent Capabilities</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Tool Integration Levels</h5>\n                                <ul>\n                                    <li><strong>Browser Primitives:</strong> Click, scroll, type actions</li>\n                                    <li><strong>Tool Discovery:</strong> Learning new tools from interfaces</li>\n                                    <li><strong>System Integration:</strong> OS-level function invocation</li>\n                                    <li><strong>Agent-as-Tool:</strong> Routing to specialized sub-agents</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}