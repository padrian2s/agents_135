{
  "page": 16,
  "title": "Post-Training Tool Integration",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 3.2.2 &middot; Tool Use</div>\n                        <h1>Post-Training Tool Integration</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <p>Tool integration with post-training techniques has emerged as a key strategy for addressing the inherent limitations of LLMs or LRMs, such as outdated knowledge, limited computational precision, and shallow multi-step reasoning. By learning how to interact with external tools, reasoning models can dynamically access up-to-date information, execute precise symbolic or numerical computations, and decompose complex tasks into grounded, tool-assisted reasoning steps.</p>\n                        <h3>Bootstrapping of Tool Use via SFT</h3>\n                        <p>Early works on tool-integration primarily apply supervised fine-tuning (SFT) over curated tool-use reasoning steps, where models were trained to imitate demonstrations of search queries, code executions, or API calls. The SFT stage provided an initial competency in invoking tools, interpreting tool outputs, and integrating the results into coherent reasoning chains.</p>\n                        <p>For example, <strong>Toolformer</strong> introduces a self-supervised framework in which large language models generate, validate, and retain useful API calls within unlabeled text, followed by fine-tuning on the filtered data to enhance factual accuracy and practical utility. <strong>ToolLLM</strong> further scales SFT training to over 16,000 real-world APIs.</p>\n                        <h3>Mastery of Tool Use via RL</h3>\n                        <p>Recent studies leverage reinforcement learning (RL) during model post-training to go beyond imitation and achieve mastery in tool-integrated reasoning. With the integration of RL, models refine their tool-use strategies through outcome-driven rewards, learning when, how, and which tools to invoke via trial and error.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Training for Tool Mastery</h4>\n                            <div class=\"analysis-item\">\n                                <h5>SFT vs RL</h5>\n                                <ul>\n                                    <li><strong>SFT:</strong> Learn from demonstrations—good for basic competency, fast to implement</li>\n                                    <li><strong>RL:</strong> Learn from outcomes—better for optimization, handles sparse rewards</li>\n                                    <li><strong>Hybrid:</strong> SFT for initialization, then RL for refinement (most common approach)</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}