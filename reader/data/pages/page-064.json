{
  "page": 64,
  "title": "Benchmarks Introduction",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 7 &middot; Benchmarks</div>\n                        <h1>Benchmarks for Agentic Reasoning</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <p>To facilitate autonomous research agents, multi-agent collaboration enables a single model's linear workflow to become a coordinated research group: specialized agents operate in parallel, exchange intermediate artifacts through explicit interfaces, and provide adversarial or complementary feedback to improve both creativity and rigor. For example, AgentRxiv [685] coordinates author, reviewer, and editor agents that iteratively refine manuscripts and share evolving artifacts across virtual \"labs.\" ARIA [529] instantiates a role-structured multi-LLM team that searches, filters, and synthesizes scientific literature into actionable experimental procedures.</p>\n                        <p>Earlier multi-agent designs such as CAMEL [531] demonstrate how cooperative role-play with tool access can enhance hypothesis generation and task decomposition. In experimental sciences, Coscientist [686] integrates planning, robotic instrument control, and analysis into a multi-agent closed loop that autonomously designs and executes wet-lab experiments. Finally, TAIS [539] defines a hierarchical team, namely project manager, data engineer and domain expert, that jointly discovers disease-predictive genes from expression data through coordinated division of labor.</p>\n                        <h3>7. Benchmarks</h3>\n                        <p>Agentic reasoning has been evaluated through a rapidly growing set of benchmarks, but existing suites often differ in what they test as the core capability, such as tool invocation accuracy, memory retention under long contexts, or coordination quality in multi-agent settings. To provide a coherent view, we organize benchmarks from two complementary perspectives.</p>\n                        <p>We first summarize benchmarks that isolate core mechanisms of agentic reasoning, which helps pinpoint where systems succeed or fail at the capability level. We then review application-level benchmarks that evaluate end-to-end agent behavior in realistic domains, capturing the combined effects of perception, planning, tool use, memory, and coordination.</p>\n                        <h3>7.1. Core Mechanisms of Agentic Reasoning</h3>\n                        <p>We begin with benchmarks that target mechanism-level capabilities, aiming to evaluate agentic reasoning in a more controlled and interpretable manner. Concretely, these benchmarks decompose agentic behavior into a small set of recurring primitives, including tool use, search, memory and planning, and multi-agent coordination.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Benchmark Organization</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Two Perspectives</h5>\n                                <ul>\n                                    <li><strong>Mechanism-Level:</strong> Isolates specific capabilities (tool use, memory, planning)</li>\n                                    <li><strong>Application-Level:</strong> Evaluates end-to-end performance in realistic domains</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}