{
  "page": 52,
  "title": "Embodied Agents: Foundational Reasoning",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 6.3.1 &middot; Embodied Agents</div>\n                        <h1>Foundational Agentic Reasoning for Embodied Agents</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <p>Embodied agents also rely on multi-modal reasoning traces that explicitly align perception with action. For example, Embodied CoT [545] trains vision-language-action models to generate reasoning steps incorporating visual features before executing an action. Fast ECoT [546] accelerates this by caching and re-using reasoning segments across time-steps, reducing inference latency while preserving task success.</p>\n                        <p>More recently, Cosmos-Reason1 [547] establishes an ontology of space, time and dynamics that lets CoT sequences encode structured physical priors. CoT-VLA [548] builds a visual chain-of-thought by predicting future image frames as intermediate sub-goals prior to action generation. Finally, Emma-X [549] integrates grounded chain-of-thought with look-ahead spatial reasoning, improving long-horizon embodied task performance.</p>\n                        <h3>Tool-use</h3>\n                        <p>Embodied agents can also be strengthened to interact with external tools to enhance perception and compensate for incomplete observations. GSCE [553], for example, provides a prompt-framework that binds skill APIs and constraints for safe LLM-driven drone control. MineDojo [554] links agents to inter-scale corpus and thus enabling richer affordance grounding.</p>\n                        <p>On the other hand, execution module is one of the most important tool type. It translates high-level language instructions into continuous motor commands, enabling embodied agents to act reliably in physical environments. Early systems such as SayCan [136] uses language to invoke robot pick-and-place skills; while Leo [555] broaden execution to more general manipulation settings and Hi Robot [556] uses a VLM reasoner to process complex prompts and a low-level action policy executes the chosen step.</p>\n                        <p>Beyond single-agent control, hybrid pipelines couple reactive reflexes with language-guided policies to support complex domains. For example, CaRo [559] incorporates an execution phase where agents carry out decomposed sub-tasks and adapt their meta-plan based on progress; COHERENT [560] embeds a robot executor module within its PEFA loop, which ensures each assigned sub-task is acted and refined appropriately.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Embodied Reasoning Patterns</h4>\n                            <div class=\"analysis-item\">\n                                <h5>Key Integration Points</h5>\n                                <ul>\n                                    <li><strong>Visual CoT:</strong> Reasoning traces incorporating visual features</li>\n                                    <li><strong>Skill APIs:</strong> Pre-defined manipulation primitives</li>\n                                    <li><strong>Execution Modules:</strong> Language-to-motor translation</li>\n                                    <li><strong>Hybrid Control:</strong> Reactive + deliberative reasoning</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}