{
  "page": 9,
  "title": "In-Context vs Post-Training Reasoning",
  "content": "<div class=\"article-header\">\n                        <div class=\"section-label\">Section 2.2 &middot; Preliminaries</div>\n                        <h1>Two Optimization Paradigms</h1>\n                    </div>\n                    <div class=\"original-content\">\n                        <h3>In-Context Reasoning: Inference-Time Search</h3>\n                        <p>In this regime, model parameters θ are frozen. The agent optimizes the reasoning trajectory by searching over Z to maximize a heuristic value function V(h_t, z). Methods like ReAct perform greedy decoding over alternating thoughts z and actions a. Tree-of-Thoughts (ToT) and related MCTS-style approaches treat partial thoughts as nodes u ∈ U and search for an optimal path.</p>\n                        <div class=\"highlight-box\">\n                            <h4>Key Equation</h4>\n                            <p style=\"text-align: center; font-family: var(--font-sans);\">τ* ∈ arg max Σ V_θ(u_i)</p>\n                            <p>where V_θ is a heuristic evaluator or verifier. This corresponds to planning in Z without updating the policy parameters.</p>\n                        </div>\n                        <h3>Post-Training: Policy Optimization</h3>\n                        <p>This paradigm optimizes θ to align the policy with long-horizon rewards r_t. Methods include DeepSeek-R1, Search-R1, and reinforcement learning approaches. Group Relative Policy Optimization (GRPO) eliminates the value network by constructing advantages from group-relative rewards.</p>\n                        <h3>Collective Intelligence: Multi-Agent Reasoning</h3>\n                        <p>We extend the single-agent formulation to a decentralized partially observable multi-agent setting. For a system of N agents, the joint policy π is composed of individual policies π^i, where agent i's observation o^i explicitly includes communicative messages c_{-i}→i generated by peers.</p>\n                    </div>\n                    <div class=\"analysis-section\">\n                        <h3>Analysis & Explanation</h3>\n                        <div class=\"analysis-block\">\n                            <h4>Practical Trade-offs</h4>\n                            <div class=\"analysis-item\">\n                                <h5>When to Use Each</h5>\n                                <ul>\n                                    <li><strong>In-context:</strong> Quick deployment, no training needed, but limited by base model capabilities</li>\n                                    <li><strong>Post-training:</strong> Better performance ceiling, but requires compute and data for training</li>\n                                    <li><strong>Multi-agent:</strong> Best for complex tasks requiring diverse expertise, but adds coordination overhead</li>\n                                </ul>\n                            </div>\n                        </div>\n                    </div>"
}