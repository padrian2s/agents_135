<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentic Reasoning for Large Language Models</title>
    <style>
        :root {
            --bg: #fefefe;
            --bg-secondary: #f5f5f5;
            --text: #1a1a1a;
            --text-secondary: #555;
            --text-muted: #888;
            --accent: #2563eb;
            --accent-hover: #1d4ed8;
            --border: #e5e5e5;
            --shadow: rgba(0,0,0,0.1);
            --overlay: rgba(0,0,0,0.5);
            --menu-bg: #ffffff;
            --radius: 12px;
            --font-serif: 'Georgia', 'Times New Roman', serif;
            --font-sans: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-serif);
            background: var(--bg);
            color: var(--text);
            line-height: 1.8;
            font-size: 18px;
            min-height: 100vh;
        }

        /* ==================== MAIN READING AREA ==================== */
        .reader {
            max-width: 900px;
            margin: 0 auto;
            padding: 3rem 2rem 6rem;
            min-height: 100vh;
        }

        /* Page indicator - subtle top bar */
        .page-indicator {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            height: 3px;
            background: var(--border);
            z-index: 100;
        }

        .page-indicator-fill {
            height: 100%;
            background: var(--accent);
            transition: width 0.3s ease;
        }

        /* Current page badge - subtle */
        .current-page-badge {
            position: fixed;
            top: 1rem;
            right: 1rem;
            background: var(--bg);
            border: 1px solid var(--border);
            padding: 0.4rem 0.8rem;
            border-radius: 20px;
            font-family: var(--font-sans);
            font-size: 0.8rem;
            color: var(--text-muted);
            opacity: 0.7;
            transition: opacity 0.3s;
            z-index: 50;
        }

        .current-page-badge:hover {
            opacity: 1;
        }

        /* ==================== IMAGE VIEW ==================== */
        .image-view {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            min-height: 80vh;
        }

        .image-view img {
            max-width: 100%;
            height: auto;
            border-radius: var(--radius);
            box-shadow: 0 4px 20px var(--shadow);
            transition: transform 0.2s ease;
            transform-origin: top center;
        }

        /* Zoom levels */
        .image-view img.zoom-70 { transform: scale(0.7); margin-bottom: -30%; }
        .image-view img.zoom-80 { transform: scale(0.8); margin-bottom: -20%; }
        .image-view img.zoom-90 { transform: scale(0.9); margin-bottom: -10%; }
        .image-view img.zoom-100 { transform: scale(1); margin-bottom: 0; }
        .image-view img.zoom-110 { transform: scale(1.1); margin-bottom: 10%; }
        .image-view img.zoom-120 { transform: scale(1.2); margin-bottom: 20%; }
        .image-view img.zoom-130 { transform: scale(1.3); margin-bottom: 30%; }
        .image-view img.zoom-140 { transform: scale(1.4); margin-bottom: 40%; }

        .image-view.hidden {
            display: none;
        }

        /* Zoom trigger zone - invisible area on left edge */
        .zoom-trigger {
            position: fixed;
            left: 0;
            top: 0;
            width: 40px;
            height: 100%;
            z-index: 99;
        }

        /* Zoom controls */
        .zoom-controls {
            position: fixed;
            left: 0;
            top: 50%;
            transform: translateY(-50%) translateX(-100%);
            display: flex;
            flex-direction: column;
            gap: 0.25rem;
            padding: 0.5rem;
            background: var(--bg);
            border: 1px solid var(--border);
            border-left: none;
            border-radius: 0 12px 12px 0;
            font-family: var(--font-sans);
            font-size: 0.85rem;
            box-shadow: 2px 0 8px var(--shadow);
            opacity: 0;
            transition: transform 0.3s ease, opacity 0.3s ease;
            z-index: 100;
        }

        .zoom-trigger:hover ~ .zoom-controls,
        .zoom-controls:hover {
            transform: translateY(-50%) translateX(0);
            opacity: 1;
        }

        .zoom-controls span {
            color: var(--text-muted);
            font-size: 0.65rem;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            text-align: center;
            padding-bottom: 0.25rem;
            border-bottom: 1px solid var(--border);
            margin-bottom: 0.25rem;
        }

        .zoom-btn {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            color: var(--text);
            padding: 0.35rem 0.5rem;
            border-radius: 8px;
            cursor: pointer;
            font-family: var(--font-sans);
            font-size: 0.75rem;
            transition: all 0.2s;
            min-width: 45px;
            text-align: center;
        }

        .zoom-btn:hover {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }

        .zoom-btn.active {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }

        /* ==================== TEXT VIEW ==================== */
        .text-view {
            display: none;
        }

        .text-view.active {
            display: block;
        }

        /* Article Header */
        .article-header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border);
        }

        .article-header h1 {
            font-size: 2rem;
            font-weight: 600;
            line-height: 1.3;
            margin-bottom: 0.5rem;
            color: var(--text);
        }

        .article-header .section-label {
            font-family: var(--font-sans);
            font-size: 0.85rem;
            color: var(--accent);
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 0.5rem;
        }

        /* Original Content */
        .original-content {
            margin-bottom: 3rem;
        }

        .original-content h2 {
            font-size: 1.5rem;
            margin: 2rem 0 1rem;
            color: var(--text);
        }

        .original-content h3 {
            font-size: 1.25rem;
            margin: 1.5rem 0 0.75rem;
            color: var(--text);
        }

        .original-content p {
            margin-bottom: 1.25rem;
            text-align: justify;
            hyphens: auto;
        }

        .original-content ul, .original-content ol {
            margin: 1rem 0 1.5rem 1.5rem;
        }

        .original-content li {
            margin-bottom: 0.5rem;
        }

        .original-content strong {
            font-weight: 600;
        }

        .original-content em {
            font-style: italic;
        }

        .citation {
            color: var(--accent);
            font-weight: 500;
        }

        /* Special Boxes */
        .definition-box {
            background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
            border-left: 4px solid #10b981;
            padding: 1.25rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 var(--radius) var(--radius) 0;
        }

        .definition-box h4 {
            font-family: var(--font-sans);
            font-size: 0.9rem;
            color: #065f46;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        .highlight-box {
            background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);
            border-left: 4px solid var(--accent);
            padding: 1.25rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 var(--radius) var(--radius) 0;
        }

        .highlight-box h4 {
            font-family: var(--font-sans);
            font-size: 0.9rem;
            color: #1e40af;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        .figure-box {
            background: linear-gradient(135deg, #fffbeb 0%, #fef3c7 100%);
            border-left: 4px solid #f59e0b;
            padding: 1.25rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 var(--radius) var(--radius) 0;
        }

        .figure-box h4 {
            font-family: var(--font-sans);
            font-size: 0.9rem;
            color: #92400e;
            margin-bottom: 0.5rem;
            font-weight: 600;
        }

        /* Tables */
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
            font-family: var(--font-sans);
        }

        .data-table th {
            background: var(--text);
            color: white;
            padding: 0.75rem 1rem;
            text-align: left;
            font-weight: 600;
            font-size: 0.85rem;
        }

        .data-table td {
            padding: 0.75rem 1rem;
            border-bottom: 1px solid var(--border);
        }

        .data-table tr:nth-child(even) {
            background: var(--bg-secondary);
        }

        /* Analysis Section */
        .analysis-section {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
        }

        .analysis-section > h3 {
            font-family: var(--font-sans);
            font-size: 0.9rem;
            color: var(--text-muted);
            text-transform: uppercase;
            letter-spacing: 1px;
            margin-bottom: 1.5rem;
        }

        .analysis-block {
            background: var(--bg-secondary);
            border-radius: var(--radius);
            padding: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .analysis-block h4 {
            font-family: var(--font-sans);
            font-size: 1rem;
            color: var(--text);
            margin-bottom: 1rem;
            font-weight: 600;
        }

        .analysis-item {
            margin-bottom: 1rem;
        }

        .analysis-item:last-child {
            margin-bottom: 0;
        }

        .analysis-item h5 {
            font-family: var(--font-sans);
            font-size: 0.85rem;
            color: var(--accent);
            margin-bottom: 0.3rem;
            font-weight: 600;
        }

        .analysis-item p {
            font-size: 0.95rem;
            color: var(--text-secondary);
            margin-bottom: 0;
            text-align: left;
        }

        .analysis-item ul {
            font-size: 0.95rem;
            color: var(--text-secondary);
            margin: 0.25rem 0 0 1.25rem;
        }

        .analysis-item li {
            margin-bottom: 0.25rem;
        }

        /* Tags */
        .tags {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin-top: 1.5rem;
        }

        .tag {
            font-family: var(--font-sans);
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            padding: 0.3rem 0.75rem;
            border-radius: 20px;
            font-size: 0.8rem;
            color: var(--text-secondary);
        }

        /* ==================== CONTEXT MENU TRIGGER ==================== */
        .menu-trigger {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 56px;
            height: 56px;
            background: var(--text);
            color: white;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            box-shadow: 0 4px 15px var(--shadow);
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
            z-index: 1000;
        }

        .menu-trigger:hover {
            background: var(--accent);
            transform: scale(1.05);
        }

        /* Flash animation to draw attention on page load */
        @keyframes menuFlash {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 4px 15px var(--shadow);
            }
            25% {
                transform: scale(1.15);
                box-shadow: 0 0 0 8px rgba(37, 99, 235, 0.3), 0 4px 20px var(--shadow);
                background: var(--accent);
            }
            50% {
                transform: scale(1);
                box-shadow: 0 4px 15px var(--shadow);
            }
            75% {
                transform: scale(1.15);
                box-shadow: 0 0 0 8px rgba(37, 99, 235, 0.3), 0 4px 20px var(--shadow);
                background: var(--accent);
            }
        }

        .menu-trigger.flash {
            animation: menuFlash 1.5s ease-in-out;
        }

        .menu-trigger svg {
            transition: transform 0.3s ease;
        }

        .menu-trigger.active svg {
            transform: rotate(45deg);
        }

        /* ==================== CONTEXT MENU ==================== */
        .context-menu-overlay {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: var(--overlay);
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            z-index: 999;
        }

        .context-menu-overlay.active {
            opacity: 1;
            visibility: visible;
        }

        .context-menu {
            position: fixed;
            bottom: 6rem;
            right: 2rem;
            background: var(--menu-bg);
            border-radius: var(--radius);
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            min-width: 280px;
            transform: translateY(20px) scale(0.95);
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            z-index: 1001;
            overflow: hidden;
        }

        .context-menu.active {
            transform: translateY(0) scale(1);
            opacity: 1;
            visibility: visible;
        }

        .menu-header {
            padding: 1rem 1.25rem;
            border-bottom: 1px solid var(--border);
            font-family: var(--font-sans);
        }

        .menu-header h3 {
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-muted);
            margin-bottom: 0.25rem;
        }

        .menu-header .page-title {
            font-size: 0.95rem;
            font-weight: 600;
            color: var(--text);
        }

        .menu-section {
            padding: 0.75rem 0;
            border-bottom: 1px solid var(--border);
        }

        .menu-section:last-child {
            border-bottom: none;
        }

        .menu-section-label {
            font-family: var(--font-sans);
            font-size: 0.7rem;
            text-transform: uppercase;
            letter-spacing: 1px;
            color: var(--text-muted);
            padding: 0.25rem 1.25rem 0.5rem;
        }

        .menu-item {
            display: flex;
            align-items: center;
            gap: 0.75rem;
            padding: 0.75rem 1.25rem;
            font-family: var(--font-sans);
            font-size: 0.95rem;
            color: var(--text);
            cursor: pointer;
            transition: background 0.2s;
            border: none;
            background: none;
            width: 100%;
            text-align: left;
        }

        .menu-item:hover {
            background: var(--bg-secondary);
        }

        .menu-item.active {
            color: var(--accent);
        }

        .menu-item svg {
            width: 20px;
            height: 20px;
            color: var(--text-muted);
            flex-shrink: 0;
        }

        .menu-item.active svg {
            color: var(--accent);
        }

        .menu-item .shortcut {
            margin-left: auto;
            font-size: 0.75rem;
            color: var(--text-muted);
            background: var(--bg-secondary);
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
        }

        /* Navigation in menu */
        .nav-controls {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 0.75rem 1.25rem;
        }

        .nav-btn {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            color: var(--text);
            width: 40px;
            height: 40px;
            border-radius: 8px;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.2s;
        }

        .nav-btn:hover:not(:disabled) {
            background: var(--accent);
            color: white;
            border-color: var(--accent);
        }

        .nav-btn:disabled {
            opacity: 0.4;
            cursor: not-allowed;
        }

        .page-input {
            width: 70px;
            text-align: center;
            padding: 0.5rem;
            border: 1px solid var(--border);
            border-radius: 8px;
            font-family: var(--font-sans);
            font-size: 0.95rem;
        }

        .page-input:focus {
            outline: none;
            border-color: var(--accent);
        }

        .page-total {
            font-family: var(--font-sans);
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Search in menu */
        .search-container {
            padding: 0.5rem 1.25rem;
        }

        .search-input {
            width: 100%;
            padding: 0.6rem 1rem;
            border: 1px solid var(--border);
            border-radius: 8px;
            font-family: var(--font-sans);
            font-size: 0.9rem;
        }

        .search-input:focus {
            outline: none;
            border-color: var(--accent);
        }

        .search-results {
            max-height: 200px;
            overflow-y: auto;
            margin-top: 0.5rem;
        }

        .search-result {
            padding: 0.5rem 0;
            border-bottom: 1px solid var(--border);
            cursor: pointer;
        }

        .search-result:hover {
            color: var(--accent);
        }

        .search-result:last-child {
            border-bottom: none;
        }

        .search-result-page {
            font-size: 0.8rem;
            color: var(--text-muted);
        }

        /* ==================== KEYBOARD HINTS ==================== */
        .keyboard-hint {
            position: fixed;
            bottom: 1rem;
            left: 50%;
            transform: translateX(-50%);
            font-family: var(--font-sans);
            font-size: 0.75rem;
            color: var(--text-muted);
            opacity: 0.5;
            transition: opacity 0.3s;
        }

        .keyboard-hint:hover {
            opacity: 1;
            background: rgba(37, 99, 235, 0.9);
            padding: 10px 20px;
            border-radius: 25px;
            color: white;
        }

        .keyboard-hint:hover kbd {
            background: rgba(255, 255, 255, 0.2);
            color: white;
            border-color: rgba(255, 255, 255, 0.4);
        }

        /* Heavy flash animation for keyboard hints */
        @keyframes keyboardFlash {
            0% {
                opacity: 0.5;
                transform: translateX(-50%) scale(1);
            }
            10% {
                opacity: 1;
                transform: translateX(-50%) scale(1.2);
                color: #2563eb;
            }
            20% {
                transform: translateX(-50%) scale(1);
            }
            30% {
                opacity: 1;
                transform: translateX(-50%) scale(1.2);
                color: #2563eb;
            }
            40% {
                transform: translateX(-50%) scale(1);
            }
            50% {
                opacity: 1;
                transform: translateX(-50%) scale(1.2);
                color: #2563eb;
            }
            60% {
                transform: translateX(-50%) scale(1);
            }
            70% {
                opacity: 1;
                transform: translateX(-50%) scale(1.15);
                color: #2563eb;
            }
            100% {
                opacity: 0.5;
                transform: translateX(-50%) scale(1);
                color: var(--text-muted);
            }
        }

        .keyboard-hint.flash {
            animation: keyboardFlash 2.5s ease-in-out;
            background: #2563eb;
            padding: 12px 24px;
            border-radius: 30px;
            box-shadow: 0 4px 30px rgba(37, 99, 235, 0.6);
            color: white !important;
            opacity: 1 !important;
        }

        .keyboard-hint.flash kbd {
            background: rgba(255, 255, 255, 0.2);
            color: white !important;
            border-color: rgba(255, 255, 255, 0.4);
            box-shadow: 0 0 10px rgba(255, 255, 255, 0.3);
            animation: kbdPulse 0.4s ease-in-out 6;
            font-weight: 600;
        }

        @keyframes kbdPulse {
            0%, 100% {
                transform: scale(1);
            }
            50% {
                transform: scale(1.1);
            }
        }

        .keyboard-hint kbd {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            padding: 0.15rem 0.4rem;
            border-radius: 4px;
            margin: 0 0.1rem;
        }

        /* ==================== RESPONSIVE ==================== */
        @media (max-width: 768px) {
            body {
                font-size: 16px;
            }

            .reader {
                padding: 2rem 1.25rem 5rem;
            }

            .menu-trigger {
                bottom: 1.5rem;
                right: 1.5rem;
                width: 50px;
                height: 50px;
            }

            .context-menu {
                right: 1rem;
                left: 1rem;
                bottom: 5rem;
                min-width: auto;
            }

            .keyboard-hint {
                display: none;
            }

            .current-page-badge {
                top: 0.75rem;
                right: 0.75rem;
                font-size: 0.75rem;
            }
        }

        /* ==================== PRINT ==================== */
        @media print {
            .menu-trigger, .context-menu, .context-menu-overlay,
            .page-indicator, .current-page-badge, .keyboard-hint {
                display: none;
            }

            .reader {
                max-width: 100%;
                padding: 0;
            }
        }

        /* ==================== SCROLLBAR ==================== */
        ::-webkit-scrollbar {
            width: 8px;
        }

        ::-webkit-scrollbar-track {
            background: transparent;
        }

        ::-webkit-scrollbar-thumb {
            background: var(--border);
            border-radius: 4px;
        }

        ::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
    </style>
</head>
<body>
    <!-- Progress indicator -->
    <div class="page-indicator">
        <div class="page-indicator-fill" id="progressBar"></div>
    </div>

    <!-- Current page badge -->
    <div class="current-page-badge" id="pageBadge">Page 1 of 135</div>

    <!-- Main reading area -->
    <main class="reader" id="reader">
        <!-- Content loaded dynamically -->
    </main>

    <!-- Keyboard hints -->
    <div class="keyboard-hint">
        <kbd>←</kbd> <kbd>→</kbd> Navigate &nbsp;&middot;&nbsp;
        <kbd>V</kbd> Toggle View &nbsp;&middot;&nbsp;
        <kbd>M</kbd> Menu &nbsp;&middot;&nbsp;
        <kbd>H</kbd> Home
    </div>

    <!-- Menu trigger button -->
    <button class="menu-trigger" id="menuTrigger" aria-label="Open menu">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <line x1="12" y1="5" x2="12" y2="19"></line>
            <line x1="5" y1="12" x2="19" y2="12"></line>
        </svg>
    </button>

    <!-- Context menu overlay -->
    <div class="context-menu-overlay" id="menuOverlay"></div>

    <!-- Context menu -->
    <nav class="context-menu" id="contextMenu">
        <div class="menu-header">
            <h3>Current Page</h3>
            <div class="page-title" id="menuPageTitle">Title Page & Abstract</div>
        </div>

        <div class="menu-section">
            <div class="menu-section-label">Navigation</div>
            <div class="nav-controls">
                <button class="nav-btn" id="prevBtn" aria-label="Previous page">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M15 18l-6-6 6-6"/></svg>
                </button>
                <input type="number" class="page-input" id="pageInput" value="1" min="1" max="135">
                <span class="page-total">/ 135</span>
                <button class="nav-btn" id="nextBtn" aria-label="Next page">
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M9 18l6-6-6-6"/></svg>
                </button>
            </div>
        </div>

        <div class="menu-section">
            <div class="menu-section-label">View</div>
            <button class="menu-item active" id="viewImage" onclick="setView('image')">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><rect x="3" y="3" width="18" height="18" rx="2" ry="2"/><circle cx="8.5" cy="8.5" r="1.5"/><polyline points="21 15 16 10 5 21"/></svg>
                Page Image (PNG)
                <span class="shortcut">V</span>
            </button>
            <button class="menu-item" id="viewText" onclick="setView('text')">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"/><polyline points="14 2 14 8 20 8"/><line x1="16" y1="13" x2="8" y2="13"/><line x1="16" y1="17" x2="8" y2="17"/></svg>
                Text & Analysis
                <span class="shortcut">V</span>
            </button>
        </div>

        <div class="menu-section">
            <div class="menu-section-label">Search</div>
            <div class="search-container">
                <input type="text" class="search-input" id="searchInput" placeholder="Search content...">
                <div class="search-results" id="searchResults"></div>
            </div>
        </div>

        <div class="menu-section">
            <button class="menu-item" onclick="openFullscreen()">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M8 3H5a2 2 0 0 0-2 2v3m18 0V5a2 2 0 0 0-2-2h-3m0 18h3a2 2 0 0 0 2-2v-3M3 16v3a2 2 0 0 0 2 2h3"/></svg>
                Open Image in New Tab
            </button>
            <a href="index.html" class="menu-item">
                <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
                Home
                <span class="shortcut">H</span>
            </a>
        </div>
    </nav>

    <script>
        // ==================== STATE ====================
        const TOTAL_PAGES = 135;
        const ZOOM_LEVELS = [70, 80, 90, 100, 110, 120, 130, 140];
        let currentPage = 1;
        let currentView = localStorage.getItem('readerView') || 'image';
        let currentZoom = parseInt(localStorage.getItem('readerZoom')) || 100;
        let menuOpen = false;

        // ==================== PAGE DATA ====================
        const pageData = {
            1: {
                title: "Title Page & Abstract",
                content: `
                    <div class="article-header">
                        <div class="section-label">Survey Paper</div>
                        <h1>Agentic Reasoning for Large Language Models</h1>
                        <p style="color: var(--text-secondary); font-style: italic;">Foundations &middot; Evolution &middot; Collaboration</p>
                    </div>
                    <div class="original-content">
                        <p><strong>Authors:</strong> Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru Zou, et al.</p>
                        <p><strong>Institutions:</strong> University of Illinois Urbana-Champaign, Meta, Amazon, Google DeepMind, UCSD, Yale</p>
                        <p><strong>arXiv:</strong> 2601.12538v1 [cs.AI] 18 Jun 2026</p>
                        <h2>Abstract</h2>
                        <p>Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, exemplified by standard benchmarks in mathematics and code, they struggle in open-ended and dynamic environments.</p>
                        <p>The emergence of <strong>agentic reasoning</strong> marks a paradigm shift, bridging thought and action by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction.</p>
                        <p>In this survey, we provide a systematic roadmap by organizing agentic reasoning along three complementary dimensions:</p>
                        <ul>
                            <li><strong>Foundational agentic reasoning</strong> establishes core single-agent capabilities, including planning, tool use, and search, that operate in stable environments</li>
                            <li><strong>Self-evolving agentic reasoning</strong> examines how agents refine these capabilities through feedback, memory, and adaptation in evolving settings</li>
                            <li><strong>Collective multi-agent reasoning</strong> extends intelligence to collaborative scenarios where multiple agents coordinate roles, share knowledge, and pursue shared goals</li>
                        </ul>
                        <div class="tags">
                            <span class="tag">Agentic AI</span>
                            <span class="tag">LLM Agent</span>
                            <span class="tag">Agentic Reasoning</span>
                            <span class="tag">Self-evolving</span>
                        </div>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Core Thesis</h4>
                            <div class="analysis-item">
                                <h5>Summary</h5>
                                <p>This paper introduces "agentic reasoning" as a paradigm shift where LLMs transform from passive text generators into autonomous agents capable of planning, acting, and learning through continuous environmental interaction.</p>
                            </div>
                            <div class="analysis-item">
                                <h5>Real-World Applications</h5>
                                <ul>
                                    <li>Autonomous AI assistants completing complex multi-step tasks</li>
                                    <li>Scientific research automation — designing experiments and iterating</li>
                                    <li>Robotics and embodied AI reasoning about physical interactions</li>
                                    <li>Healthcare decision support for diagnostic pathways</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            2: {
                title: "Framework Overview (Figure 1)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 1 &middot; Introduction</div>
                        <h1>Framework Overview</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 1: An Overview of Agentic Reasoning</h4>
                            <p>The diagram shows the complete framework:</p>
                            <p><strong>Process Flow:</strong> User → Propose Task → Agentic Reasoning System → Solve → Generalize → Future Tasks</p>
                            <p><strong>Three Layers:</strong></p>
                            <ul>
                                <li><strong>Foundational (Section 3):</strong> Complex Planning, Tool Use, Web Search</li>
                                <li><strong>Self-evolving (Section 4):</strong> Feedback Loop, Agentic Memory, Self-Evolving</li>
                                <li><strong>Collective (Section 5):</strong> Role Assigning, Collaboration, Co-evolving</li>
                            </ul>
                        </div>
                        <p>Closed-world domains such as mathematical problem solving and code generation have seen significant advances. Empirically, techniques that explicitate intermediate reasoning, such as Chain-of-Thought prompting, decomposition, and program-aided solving, have significantly bolstered inference performance.</p>
                        <p>Yet, these approaches often assume static contexts and short-horizon reasoning. Conventional LLMs lack mechanisms to act, adapt, or improve in open-ended environments where information evolves over time.</p>
                        <p>In this survey, we systematize this evolution under the framework of <em>Agentic Reasoning</em>: rather than passively generating sequences, LLMs are reframed as autonomous reasoning agents that plan, act, and learn through continual interaction with their environment.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Framework Architecture</h4>
                            <div class="analysis-item">
                                <h5>Key Insight</h5>
                                <p>Figure 1 presents the complete architectural overview showing how user tasks flow through the system and how the three main layers interact and build upon each other. This is the visual roadmap for the entire paper.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            3: {
                title: "Definition of Agentic Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 1 &middot; Introduction</div>
                        <h1>Definition of Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <div class="definition-box">
                            <h4>Definition</h4>
                            <p><strong>Agentic reasoning</strong> positions reasoning as the central mechanism of intelligent agents, spanning foundational capabilities (planning, tool use, and search), self-evolving adaptation (feedback and memory-driven adaptation), and collective coordination (multi-agent collaboration), realizable through either in-context orchestration or post-training optimization.</p>
                        </div>
                        <h3>Foundational Agentic Reasoning</h3>
                        <p>Establishes the bedrock of core single-agent capabilities, including planning, tool use, and search, that enable operations within stable, albeit complex, environments. Agents act by decomposing goals, invoking external tools, and verifying results through executable actions.</p>
                        <h3>Self-Evolving Agentic Reasoning</h3>
                        <p>Enables agents to improve continually through cumulative experience. This paradigm extends adaptation to include persistent updates of internal states like memory and policy. Reflection-based frameworks such as Reflexion allow agents to critique and refine their own reasoning processes.</p>
                        <h3>Collective Multi-Agent Reasoning</h3>
                        <p>Scales intelligence from isolated solvers to collaborative ecosystems. Multiple agents coordinate to achieve shared goals through explicit role assignment (e.g., manager-worker-critic), communication protocols, and shared memory systems.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>The Three Pillars</h4>
                            <div class="analysis-item">
                                <h5>Practical Mapping</h5>
                                <ul>
                                    <li><strong>Foundational:</strong> Single-agent task completion (coding assistants, search tools)</li>
                                    <li><strong>Self-Evolving:</strong> Learning systems that improve over time (personalized AI)</li>
                                    <li><strong>Collective:</strong> Team-based AI systems (research automation, software development)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            4: {
                title: "Survey Scope & Contributions",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 1 &middot; Introduction</div>
                        <h1>Survey Scope & Contributions</h1>
                    </div>
                    <div class="original-content">
                        <div class="highlight-box">
                            <h4>Survey Scope</h4>
                            <p>This survey reviews reasoning-empowered agentic systems where reasoning drives adaptive behavior. We analyze these systems through two complementary optimization modes:</p>
                            <ul>
                                <li><strong>In-context Reasoning:</strong> scales inference-time interaction through structured orchestration and planning without parameter updates</li>
                                <li><strong>Post-training Reasoning:</strong> internalizes reasoning strategies into model parameters via reinforcement learning and fine-tuning</li>
                            </ul>
                        </div>
                        <div class="definition-box">
                            <h4>Contributions</h4>
                            <ul>
                                <li><strong>Conceptual framing:</strong> We formalize the paradigm of Agentic Reasoning, spanning foundational, self-evolving, and collective reasoning layers</li>
                                <li><strong>Systematic review:</strong> We analyze single-agent, adaptive, and multi-agent systems, emphasizing reasoning-centered workflow orchestration</li>
                                <li><strong>Applications and evaluation:</strong> We review real-world applications and benchmarks to illustrate the instantiation and evaluation of agentic reasoning mechanisms</li>
                                <li><strong>Future agenda:</strong> We identify emerging challenges in robustness, trustworthiness, and efficiency</li>
                            </ul>
                        </div>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Two Optimization Modes</h4>
                            <div class="analysis-item">
                                <h5>Key Distinction</h5>
                                <p>The in-context vs post-training distinction is crucial: in-context methods work with frozen models (prompt engineering, tool use), while post-training methods modify the model itself (RLHF, SFT). Most production systems use a combination of both.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            5: {
                title: "Table of Contents (Part 1)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Navigation</div>
                        <h1>Table of Contents</h1>
                    </div>
                    <div class="original-content">
                        <h3>1. Introduction (Page 1)</h3>
                        <h3>2. From LLM Reasoning to Agentic Reasoning (Page 7)</h3>
                        <ul>
                            <li>2.1 Positioning Our Survey</li>
                            <li>2.2 Preliminaries</li>
                        </ul>
                        <h3>3. Foundational Agentic Reasoning (Page 10)</h3>
                        <ul>
                            <li>3.1 Planning Reasoning
                                <ul>
                                    <li>3.1.1 In-context Planning</li>
                                    <li>3.1.2 Post-training Planning</li>
                                </ul>
                            </li>
                            <li>3.2 Tool-Use Optimization
                                <ul>
                                    <li>3.2.1 In-Context Tool-integration</li>
                                    <li>3.2.2 Post-training Tool-integration</li>
                                    <li>3.2.3 Orchestration-based Tool-integration</li>
                                </ul>
                            </li>
                            <li>3.3 Agentic Search
                                <ul>
                                    <li>3.3.1 In-Context Search</li>
                                    <li>3.3.2 Post-Training Search</li>
                                </ul>
                            </li>
                        </ul>
                        <h3>4. Self-evolving Agentic Reasoning (Page 20)</h3>
                        <ul>
                            <li>4.1 Agentic Feedback Mechanisms</li>
                            <li>4.2 Agentic Memory</li>
                            <li>4.3 Evolving Foundational Agentic Capabilities</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Paper Structure</h4>
                            <div class="analysis-item">
                                <h5>Navigation Guide</h5>
                                <p>The paper follows a clear progression: Background → Foundational capabilities → Self-improvement → Multi-agent → Applications → Benchmarks → Future directions. Use this TOC to jump to specific topics of interest.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            6: {
                title: "Table of Contents (Part 2)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Navigation</div>
                        <h1>Table of Contents (continued)</h1>
                    </div>
                    <div class="original-content">
                        <h3>5. Collective Multi-agent Reasoning (Page 29)</h3>
                        <ul>
                            <li>5.1 Role Taxonomy of Multi-Agent Systems</li>
                            <li>5.2 Collaboration and Division of Labor</li>
                            <li>5.3 Multi-Agent Evolution</li>
                        </ul>
                        <h3>6. Applications (Page 43)</h3>
                        <ul>
                            <li>6.1 Math Exploration & Vibe Coding Agents</li>
                            <li>6.2 Scientific Discovery Agents</li>
                            <li>6.3 Embodied Agents</li>
                            <li>6.4 Healthcare & Medicine Agents</li>
                            <li>6.5 Autonomous Web Exploration & Research Agents</li>
                        </ul>
                        <h3>7. Benchmarks (Page 64)</h3>
                        <ul>
                            <li>7.1 Core Mechanisms of Agentic Reasoning</li>
                            <li>7.2 Applications of Agentic Reasoning</li>
                        </ul>
                        <h3>8. Open Problems (Page 72)</h3>
                        <ul>
                            <li>8.1 User-centric Agentic Reasoning and Personalization</li>
                            <li>8.2 Long-horizon Agentic Reasoning</li>
                            <li>8.3 Agentic Reasoning with World Models</li>
                            <li>8.4 Multi-agent Collaborative Reasoning and Training</li>
                            <li>8.5 Latent Agentic Reasoning</li>
                            <li>8.6 Governance of Agentic Reasoning</li>
                        </ul>
                        <div class="highlight-box">
                            <h4>Survey Structure</h4>
                            <p>Sec. 2: Preliminaries | Sec. 3: Foundational | Sec. 4: Self-evolving | Sec. 5: Collective | Sec. 6: Applications | Sec. 7: Benchmarks | Sec. 8: Open Problems</p>
                        </div>
                    </div>
                `
            },
            7: {
                title: "From LLM to Agentic Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 2 &middot; Background</div>
                        <h1>From LLM Reasoning to Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <p>Traditional reasoning with large language models (LLMs) is typically formulated as a one-shot or few-shot prediction task over static inputs. These models rely on scaling test-time computation, improving accuracy by increasing model size or inference budget, but without the ability to interact, remember, or adapt to changing goals.</p>
                        <p>Agentic reasoning, in contrast, emphasizes <strong>scaling test-time interaction</strong>. Instead of depending solely on internal parameters, agentic systems reason through action: invoking tools, exploring alternatives, updating memory, and integrating feedback.</p>
                        <div class="highlight-box">
                            <h4>Table 1: Contrasting LLM reasoning and agentic reasoning</h4>
                            <table class="data-table">
                                <tr><th>Dimension</th><th>LLM Reasoning</th><th>Agentic Reasoning</th></tr>
                                <tr><td>Paradigm</td><td>passive, static input</td><td>interactive, dynamic context</td></tr>
                                <tr><td>Computation</td><td>single pass, internal</td><td>multi step, with feedback</td></tr>
                                <tr><td>Statefulness</td><td>context window only</td><td>external memory, state tracking</td></tr>
                                <tr><td>Learning</td><td>offline pretraining, fixed knowledge</td><td>continual improvement, self evolving</td></tr>
                                <tr><td>Goal Orientation</td><td>prompt based, reactive</td><td>explicit goal, planning</td></tr>
                            </table>
                        </div>
                        <h3>2.1 Positioning Our Survey</h3>
                        <p>While several recent surveys have examined LLM reasoning or agent architectures, our work focuses specifically on agentic reasoning as a unified paradigm for understanding reasoning as interaction.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>The Paradigm Shift</h4>
                            <div class="analysis-item">
                                <h5>Decision Framework</h5>
                                <p>Table 1 provides a practical diagnostic: if your application requires ANY of the "Agentic Reasoning" characteristics (interactivity, persistence, adaptation, goal-orientation), then pure LLM reasoning will be insufficient.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            8: {
                title: "Preliminaries & Formalization",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 2.2 &middot; Preliminaries</div>
                        <h1>Formalizing Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <h3>Agentic Reasoning: A Latent-Space View</h3>
                        <p>Standard approaches often conflate the agent's context with the environment state. We model the environment as a <strong>Partially Observable Markov Decision Process (POMDP)</strong> and introduce an internal reasoning variable to expose the "think-act" structure of agentic policies.</p>
                        <div class="definition-box">
                            <h4>Formal Definition</h4>
                            <p>We consider the tuple (X, O, A, Z, M, T, Ω, R, γ), where:</p>
                            <ul>
                                <li><strong>X</strong> is the latent environment state space (unobservable to the agent)</li>
                                <li><strong>O</strong> is the observation space (e.g., user queries, API returns)</li>
                                <li><strong>A</strong> is the external action space (e.g., tool invocation, final answers)</li>
                                <li><strong>Z</strong> is a reasoning trace space (e.g., latent plans, chain-of-thought)</li>
                                <li><strong>M</strong> is the agent's internal memory/context space</li>
                            </ul>
                        </div>
                        <p>At timestep t, the agent conditions on a history h_t = (o_1, z_1, a_1, ..., a_{t-1}). The history can be summarized by an internal memory state m_t ∈ M. We factorize the policy as:</p>
                        <p style="text-align: center; font-family: var(--font-sans);"><strong>π_θ(z_t, a_t | h_t) = π_reason(z_t | h_t) · π_act(a_t | h_t, z_t)</strong></p>
                        <p>This decomposition highlights the core shift in agentic systems: performing computation in Z (thinking) before committing to A (acting).</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Why This Matters</h4>
                            <div class="analysis-item">
                                <h5>Practical Implications</h5>
                                <p>The separation of "thinking" (Z) from "acting" (A) is what enables techniques like Chain-of-Thought, ReAct, and reflection. The agent can reason internally before committing to external actions, allowing for planning and self-correction.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            9: {
                title: "In-Context vs Post-Training Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 2.2 &middot; Preliminaries</div>
                        <h1>Two Optimization Paradigms</h1>
                    </div>
                    <div class="original-content">
                        <h3>In-Context Reasoning: Inference-Time Search</h3>
                        <p>In this regime, model parameters θ are frozen. The agent optimizes the reasoning trajectory by searching over Z to maximize a heuristic value function V(h_t, z). Methods like ReAct perform greedy decoding over alternating thoughts z and actions a. Tree-of-Thoughts (ToT) and related MCTS-style approaches treat partial thoughts as nodes u ∈ U and search for an optimal path.</p>
                        <div class="highlight-box">
                            <h4>Key Equation</h4>
                            <p style="text-align: center; font-family: var(--font-sans);">τ* ∈ arg max Σ V_θ(u_i)</p>
                            <p>where V_θ is a heuristic evaluator or verifier. This corresponds to planning in Z without updating the policy parameters.</p>
                        </div>
                        <h3>Post-Training: Policy Optimization</h3>
                        <p>This paradigm optimizes θ to align the policy with long-horizon rewards r_t. Methods include DeepSeek-R1, Search-R1, and reinforcement learning approaches. Group Relative Policy Optimization (GRPO) eliminates the value network by constructing advantages from group-relative rewards.</p>
                        <h3>Collective Intelligence: Multi-Agent Reasoning</h3>
                        <p>We extend the single-agent formulation to a decentralized partially observable multi-agent setting. For a system of N agents, the joint policy π is composed of individual policies π^i, where agent i's observation o^i explicitly includes communicative messages c_{-i}→i generated by peers.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Practical Trade-offs</h4>
                            <div class="analysis-item">
                                <h5>When to Use Each</h5>
                                <ul>
                                    <li><strong>In-context:</strong> Quick deployment, no training needed, but limited by base model capabilities</li>
                                    <li><strong>Post-training:</strong> Better performance ceiling, but requires compute and data for training</li>
                                    <li><strong>Multi-agent:</strong> Best for complex tasks requiring diverse expertise, but adds coordination overhead</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            10: {
                title: "Foundational Agentic Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3 &middot; Foundational</div>
                        <h1>Foundational Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <p>Agentic reasoning originates from the behavior of a single agent. Before discussing adaptation and collaboration, we focus on how an individual agent translates reasoning into structured action through three core components: <strong>planning</strong>, <strong>search</strong>, and <strong>tool use</strong>.</p>
                        <div class="definition-box">
                            <h4>Core Components</h4>
                            <p>In this setting, the agent is not a passive text generator but an autonomous problem solver that formulates plans, explores alternatives through retrieval or environment search, and leverages tools to execute grounded operations. Together, these mechanisms establish the foundation of agentic reasoning, linking abstract deliberation with verifiable action.</p>
                        </div>
                        <p>A canonical foundational workflow can be viewed as an iterative cycle that interleaves <strong>planning</strong> (goal decomposition and task formulation), <strong>tool use</strong> (invoking external systems or APIs to act on the world) and <strong>search</strong> (retrieval and exploration for decision support). Reasoning serves as the organizing principle across these stages.</p>
                        <h3>3.1 Planning Reasoning</h3>
                        <p>Planning is a central component of intelligent behavior, enabling agents to decompose problems, sequence decisions, and navigate complex environments with foresight. Recent research has increasingly explored planning in the context of large language models (LLMs).</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>The Foundation Triangle</h4>
                            <div class="analysis-item">
                                <h5>How They Work Together</h5>
                                <ul>
                                    <li><strong>Planning:</strong> "What steps do I need to take?" → Decompose the goal</li>
                                    <li><strong>Search:</strong> "What information do I need?" → Gather context</li>
                                    <li><strong>Tool Use:</strong> "How do I execute this?" → Take action</li>
                                </ul>
                                <p>These three components form a continuous loop in agentic systems.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            11: {
                title: "In-Context Planning (Figure 2)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.1.1 &middot; Planning</div>
                        <h1>In-Context Planning</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 2: Overview of Planning Reasoning in LLM Agents</h4>
                            <p>The figure categorizes planning into <strong>In-context Planning</strong> (left) and <strong>Post-training Planning</strong> (right):</p>
                            <ul>
                                <li><strong>In-context:</strong> Workflow Design → Tree Search → Process Formalization → Decomposition → Tool Use</li>
                                <li><strong>Post-training:</strong> Reward Design → Pre-train → Reward Model → Policy Optimization</li>
                            </ul>
                        </div>
                        <h3>Workflow Design</h3>
                        <p>Workflow-based approaches often emphasize structuring the overall planning process into distinct stages (e.g., perception, reasoning, execution, verification), which are either explicitly scaffolded or learned implicitly. Methods like PERIA combine perception, imagination, and action in a unified multimodal workflow.</p>
                        <h3>Tree Search / Algorithm Simulation</h3>
                        <p>Tree-based search strategies, especially BFS, DFS, A*, MCTS, and beam search, have become prominent as interpretable and effective planning scaffolds. Several works simulate tree traversal algorithms to mimic deliberative processes. Tree-of-Thoughts (ToT) and related MCTS-style approaches treat the agent's code as a hypothesis space, using an LLM as a mutation operator to search for superior reasoning algorithms.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Planning Strategies</h4>
                            <div class="analysis-item">
                                <h5>Practical Applications</h5>
                                <ul>
                                    <li><strong>Workflow Design:</strong> Best for well-defined, repeatable processes (customer service, data pipelines)</li>
                                    <li><strong>Tree Search:</strong> Best for exploration and optimization problems (code generation, theorem proving)</li>
                                    <li><strong>Decomposition:</strong> Best for complex, multi-step tasks (research, project planning)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            12: {
                title: "Agentic Planning Methods (Table 2)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.1.1 &middot; Planning</div>
                        <h1>Representative Planning Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="highlight-box">
                            <h4>Table 2: Agentic Planning Systems</h4>
                            <table class="data-table">
                                <tr><th>Method</th><th>Structure</th><th>Format</th><th>Tool</th></tr>
                                <tr><td colspan="4"><strong>Modality I: Language Agents (Search, Code)</strong></td></tr>
                                <tr><td>ReWOO</td><td>Decomposed</td><td>Natural Language</td><td>None</td></tr>
                                <tr><td>Reflexion</td><td>Sequential</td><td>Natural Language</td><td>None</td></tr>
                                <tr><td>LLM+P</td><td>Sequential</td><td>Formal Language</td><td>None</td></tr>
                                <tr><td>ToT</td><td>Tree</td><td>Natural Language</td><td>None</td></tr>
                                <tr><td>GoT</td><td>Graph</td><td>Natural Language</td><td>None</td></tr>
                                <tr><td>HTP</td><td>Hypertree</td><td>Natural Language</td><td>Retrieval</td></tr>
                                <tr><td>Gorilla</td><td>Sequential</td><td>Programming Language</td><td>Retrieval, API</td></tr>
                                <tr><td>CodeNav</td><td>Sequential</td><td>Programming Language</td><td>Code Index, Code Search</td></tr>
                                <tr><td colspan="4"><strong>Modality II: Visual/Multimodal Agents (GUI, Embodied)</strong></td></tr>
                                <tr><td>VisualPredictor</td><td>Tree</td><td>Formal Language</td><td>None</td></tr>
                                <tr><td>LLM-Planner</td><td>Tree</td><td>Formal Language</td><td>Object Detector, KNN</td></tr>
                                <tr><td>Agent-E</td><td>Sequential</td><td>Formal Language</td><td>DOM Grounder, Screenshot</td></tr>
                                <tr><td>AESOP</td><td>Reactive</td><td>Natural Language</td><td>Anomaly Detector</td></tr>
                                <tr><td>BehaviorGPT</td><td>Sequential</td><td>Visual Features</td><td>World Model</td></tr>
                            </table>
                        </div>
                        <p>This search-over-hierarchy view maps cleanly onto domain systems. In the web setting, planner-executor architectures generate high-level subtask trees in natural language and bind leaves to DOM-grounded actions, often with memory to persist context.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Choosing a Planning Method</h4>
                            <div class="analysis-item">
                                <h5>Selection Criteria</h5>
                                <ul>
                                    <li><strong>Sequential:</strong> Simple tasks, linear workflows</li>
                                    <li><strong>Tree/Graph:</strong> Exploration, backtracking needed</li>
                                    <li><strong>With Tools:</strong> Tasks requiring external data or actions</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            13: {
                title: "Post-Training Planning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.1.2 &middot; Planning</div>
                        <h1>Post-Training Planning</h1>
                    </div>
                    <div class="original-content">
                        <h3>Process Formalization</h3>
                        <p>Formalizing planning through symbolic representations, programming languages, or logic frameworks ensures compositionality, interpretability, and generalization. Several works encode plans as code-like artifacts or PDDL programs. PDDL-based formulations explicitly bridge LLM planning with well-established planning ecosystems.</p>
                        <h3>Decoupling / Decomposition</h3>
                        <p>Decoupling strategies aim to modularize complex planning into separable components such as goal recognition, memory retrieval, and plan refinement. Notably, ReWOO explicitly separates observation and reasoning modules to optimize for efficiency. Works like DEPS promote hierarchical thinking through hypertrees.</p>
                        <h3>External Aid / Tool Use</h3>
                        <p>Many systems leverage external structures or tools to aid planning, including retrieval-augmented generation (RAG), knowledge graphs, world models, and general-purpose tool use. RAG-style systems retrieve relevant knowledge to support continual instruction planning. World model-based agents learn or leverage environment models for model-based planning.</p>
                        <h3>Reward Design / Optimal Control</h3>
                        <p>Finally, planning as optimization entails designing suitable reward structures and solving for optimal behavior using RL or control-theoretic tools. Reflexion, Reflect-then-Plan, and Rational Decision Agents incorporate utility-based learning to guide planning behavior. Reward modeling appears in works such as those that emphasize reward shaping.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Training vs Prompting</h4>
                            <div class="analysis-item">
                                <h5>When to Use Post-Training</h5>
                                <p>Post-training is preferred when: (1) you have domain-specific data, (2) you need consistent behavior across many users, (3) prompt engineering isn't achieving required quality, or (4) you want to reduce inference-time compute costs.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            14: {
                title: "Tool-Use Optimization (Table 3)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.2 &middot; Tool Use</div>
                        <h1>Tool-Use Optimization</h1>
                    </div>
                    <div class="original-content">
                        <p>Tool use optimization is the capacity of an agent to augment its intrinsic capabilities by intelligently invoking external modules. This allows agents to overcome limitations such as outdated knowledge, inability to perform precise calculations, or lack of access to private information.</p>
                        <div class="highlight-box">
                            <h4>Table 3: Representative Tool-Use Optimization Systems</h4>
                            <table class="data-table">
                                <tr><th>Method</th><th>Stage</th><th>Learning</th><th>Tool Strategy</th></tr>
                                <tr><td colspan="4"><strong>Modality I: In-Context Integration</strong></td></tr>
                                <tr><td>ReAct</td><td>Inference</td><td>Prompting</td><td>Interleaved reasoning-action</td></tr>
                                <tr><td>ART</td><td>Inference</td><td>Few-shot</td><td>Retrieved multi-step demos</td></tr>
                                <tr><td>ChatCoT</td><td>Inference</td><td>Prompting</td><td>CoT with tool calls</td></tr>
                                <tr><td>GEAR</td><td>Inference</td><td>Delegation</td><td>Light model for tool selection</td></tr>
                                <tr><td>AVATAR</td><td>Inference</td><td>Contrastive</td><td>In-context tool reasoning</td></tr>
                                <tr><td colspan="4"><strong>Modality II: Post-Training Integration</strong></td></tr>
                                <tr><td>Toolformer</td><td>Post-train</td><td>Self-sup. + SFT</td><td>Self-generated API calls</td></tr>
                                <tr><td>ToolLLM</td><td>Post-train</td><td>SFT</td><td>Large-scale API demos</td></tr>
                                <tr><td>ToolAlpaca</td><td>Post-train</td><td>SFT</td><td>Simulated dialogues</td></tr>
                                <tr><td>ReSearch</td><td>Post-train</td><td>RL + Reflec.</td><td>Adaptive retrieval reasoning</td></tr>
                                <tr><td>ToolRL</td><td>Post-train</td><td>RL</td><td>Multi-tool policy learning</td></tr>
                            </table>
                        </div>
                        <h3>3.2.1 In-Context Tool-integration</h3>
                        <p>The in-context demonstration paradigm is a training-free approach to empowering LLMs with new capabilities at inference time. This method leverages the remarkable in-context learning ability of modern LLMs, guiding a frozen, off-the-shelf model to perform complex tasks by providing carefully crafted instructions, examples, and contextual information directly in the prompt.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tool Use Patterns</h4>
                            <div class="analysis-item">
                                <h5>Implementation Approaches</h5>
                                <ul>
                                    <li><strong>ReAct pattern:</strong> Thought → Action → Observation loop</li>
                                    <li><strong>Toolformer pattern:</strong> Model learns when/how to call tools during training</li>
                                    <li><strong>Orchestration pattern:</strong> External system manages tool selection and execution</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            15: {
                title: "Traditional LLM vs Agentic Tool Systems (Figure 3)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.2.1 &middot; Tool Use</div>
                        <h1>Agentic Tool-Use Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 3: Traditional LLM vs Agentic Tool-Use Systems</h4>
                            <p><strong>Traditional LLM (Left):</strong></p>
                            <ul>
                                <li>User → Query → Closed-world reasoning</li>
                                <li>No access to external tools or environments</li>
                                <li>Static, outdated knowledge</li>
                                <li>Prone to hallucination</li>
                                <li>No numerical capability</li>
                            </ul>
                            <p><strong>Agentic Tool System (Right):</strong></p>
                            <ul>
                                <li>Context Aware + Dynamic Tool-Selection + Orchestration</li>
                                <li>Tool Selection → Tool Invocation → Reflection</li>
                                <li>Grounded reasoning with up-to-date knowledge</li>
                                <li>Precise computation via external tools</li>
                            </ul>
                        </div>
                        <h3>Interleaving Reasoning and Tool Use</h3>
                        <p>The foundation of in-context agentic reasoning lies in augmenting the Chain-of-Thought (CoT) process with the ability to take action. ChatCoT formalizes this paradigm by structuring reasoning traces as alternating "thought-tool-observation" steps in natural language, allowing LLMs to reflect on intermediate outputs and dynamically plan the next tool query.</p>
                        <h3>Optimizing Context for Tool Interaction</h3>
                        <p>While the foundational interleaved loop is powerful, its performance degrades when agents must handle large or complex toolsets. A significant branch of research addresses this by optimizing the in-context information provided to the agent. GEAR introduces a computationally efficient, training-free algorithm that delegates the tool selection process to a small language model while reserving the more powerful LLM for the final reasoning step.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>The Tool-Use Revolution</h4>
                            <div class="analysis-item">
                                <h5>Why This Matters</h5>
                                <p>Tool use transforms LLMs from "know-it-alls with hallucinations" to "intelligent orchestrators with real capabilities." The key insight is that LLMs don't need to do everything—they need to know when and how to delegate to specialized tools.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            16: {
                title: "Post-Training Tool Integration",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.2.2 &middot; Tool Use</div>
                        <h1>Post-Training Tool Integration</h1>
                    </div>
                    <div class="original-content">
                        <p>Tool integration with post-training techniques has emerged as a key strategy for addressing the inherent limitations of LLMs or LRMs, such as outdated knowledge, limited computational precision, and shallow multi-step reasoning. By learning how to interact with external tools, reasoning models can dynamically access up-to-date information, execute precise symbolic or numerical computations, and decompose complex tasks into grounded, tool-assisted reasoning steps.</p>
                        <h3>Bootstrapping of Tool Use via SFT</h3>
                        <p>Early works on tool-integration primarily apply supervised fine-tuning (SFT) over curated tool-use reasoning steps, where models were trained to imitate demonstrations of search queries, code executions, or API calls. The SFT stage provided an initial competency in invoking tools, interpreting tool outputs, and integrating the results into coherent reasoning chains.</p>
                        <p>For example, <strong>Toolformer</strong> introduces a self-supervised framework in which large language models generate, validate, and retain useful API calls within unlabeled text, followed by fine-tuning on the filtered data to enhance factual accuracy and practical utility. <strong>ToolLLM</strong> further scales SFT training to over 16,000 real-world APIs.</p>
                        <h3>Mastery of Tool Use via RL</h3>
                        <p>Recent studies leverage reinforcement learning (RL) during model post-training to go beyond imitation and achieve mastery in tool-integrated reasoning. With the integration of RL, models refine their tool-use strategies through outcome-driven rewards, learning when, how, and which tools to invoke via trial and error.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Training for Tool Mastery</h4>
                            <div class="analysis-item">
                                <h5>SFT vs RL</h5>
                                <ul>
                                    <li><strong>SFT:</strong> Learn from demonstrations—good for basic competency, fast to implement</li>
                                    <li><strong>RL:</strong> Learn from outcomes—better for optimization, handles sparse rewards</li>
                                    <li><strong>Hybrid:</strong> SFT for initialization, then RL for refinement (most common approach)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            17: {
                title: "Orchestration-Based Tool Integration",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.2.3 &middot; Tool Use</div>
                        <h1>Orchestration-Based Tool Integration</h1>
                    </div>
                    <div class="original-content">
                        <p>In real-world applications, tool use within complex systems often extends beyond the single-model, single-tool setting, requiring orchestration among multiple tools to complete complex tasks. This orchestration typically involves planning, sequencing, and managing dependencies across tools.</p>
                        <h3>Agentic Pipelines for Tool Orchestration</h3>
                        <p>There are many frameworks designed to enable LLMs to call and orchestrate tools effectively. Most of the current agentic paradigm follows a "plan before action" strategy, where the model first generates a structured plan for tool use and then executes it.</p>
                        <p><strong>ToolPlanner</strong> introduces a two-stage reinforcement learning framework with path planning and feedback, supported by MGToolBench, to bridge the gap between API-heavy training data and real-world user instructions.</p>
                        <p><strong>Tool-MVR</strong> enhances reliability and reflection through meta-verification of tool calls and exploration-based reflection learning. More recently, <strong>OctoTools</strong> provides a training-free, extensible framework with standardized tool cards, a hierarchical planner, and an executor.</p>
                        <h3>Tool Representations for Orchestration</h3>
                        <p>Beyond designing orchestration pipelines, another line of research focuses on optimizing the tools themselves to facilitate more accurate selection, composition, and coordination during orchestration. <strong>ToolExpNet</strong> models tools and their usage experiences as a network that encodes semantic similarity and dependency relations, allowing LLMs to distinguish between similar tools and account for interdependencies during selection.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Orchestration Patterns</h4>
                            <div class="analysis-item">
                                <h5>Common Architectures</h5>
                                <ul>
                                    <li><strong>Sequential:</strong> Tool A → Tool B → Tool C (simple pipelines)</li>
                                    <li><strong>Hierarchical:</strong> Planner → Executors → Verifiers (complex workflows)</li>
                                    <li><strong>Graph-based:</strong> Tools as nodes, dependencies as edges (flexible composition)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            18: {
                title: "Agentic Search (Figure 4)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.3 &middot; Agentic Search</div>
                        <h1>Agentic Search Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 4: Traditional RAG vs Agentic Search Systems</h4>
                            <p><strong>Traditional RAG System (Left):</strong></p>
                            <ul>
                                <li>Query → Data Embedding → Static retrieval over vector database</li>
                                <li>Single-shot retrieval, no iteration</li>
                            </ul>
                            <p><strong>Agentic Search System (Right):</strong></p>
                            <ul>
                                <li>Dynamic search + In-context Search + Search SFT/RL</li>
                                <li>WHEN/WHAT, HOW to Retrieve → Autonomous Agent</li>
                                <li>Tool Use + Reasoning + Critique & Adapt</li>
                                <li>Experience from web → Iterative refinement</li>
                            </ul>
                        </div>
                        <p>Single-agent Agentic Retrieval-Augmented Generation (RAG) systems embed reasoning and control into a centralized agent that governs the entire retrieval-generation loop. Unlike traditional RAG pipelines that perform fixed, one-shot retrieval before generation, agentic RAG agents dynamically control <em>when</em>, <em>what</em>, and <em>how</em> to retrieve based on real-time reasoning needs.</p>
                        <h3>3.3.1 In-Context Search</h3>
                        <h3>Interleaving Reasoning and Search</h3>
                        <p>In-context agentic RAG systems embed retrieval behavior directly into the inference process of language models through carefully designed prompting strategies. A representative example is <strong>ReAct</strong>, which interleaves Chain-of-Thought reasoning with tool-use commands such as &lt;Search&gt; to dynamically invoke external APIs or knowledge sources.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>RAG Evolution</h4>
                            <div class="analysis-item">
                                <h5>From Static to Agentic</h5>
                                <p>Traditional RAG: "Retrieve first, then generate." Agentic RAG: "Reason about what to retrieve, when to retrieve, iterate, and verify." This shift enables handling of complex, multi-hop questions that require iterative refinement.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            19: {
                title: "Agentic Search Methods (Table 4)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 3.3.1 &middot; Search</div>
                        <h1>Representative Agentic Search Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="highlight-box">
                            <h4>Table 4: Representative Agentic Search Systems</h4>
                            <table class="data-table">
                                <tr><th>Method</th><th>Structure</th><th>Format</th><th>Tool</th></tr>
                                <tr><td colspan="4"><strong>Modality I: In-Context Agentic Search</strong></td></tr>
                                <tr><td>ReAct</td><td>Interleaved</td><td>NL + Actions</td><td>Search API</td></tr>
                                <tr><td>Self-Ask</td><td>Decomposed</td><td>NL Queries</td><td>Search API</td></tr>
                                <tr><td>IRCoT</td><td>Sequential</td><td>NL + CoT</td><td>Search API</td></tr>
                                <tr><td>Self-RAG</td><td>Reflective</td><td>NL Self-check</td><td>Conditional Search</td></tr>
                                <tr><td>DeepRAG</td><td>Iterative</td><td>NL Feedback</td><td>Search API</td></tr>
                                <tr><td colspan="4"><strong>Modality II: Post-Training Agentic Search</strong></td></tr>
                                <tr><td>Toolformer</td><td>Sequential</td><td>Tool Tokens</td><td>APIs, Search</td></tr>
                                <tr><td>INTERS</td><td>Sequential</td><td>Instructions</td><td>Search API</td></tr>
                                <tr><td>WebGPT</td><td>Sequential</td><td>NL + Browser</td><td>Web Search</td></tr>
                                <tr><td>Search-R1</td><td>Iterative</td><td>NL + Tokens</td><td>Live Web</td></tr>
                                <tr><td>ReSearch</td><td>Step-wise</td><td>NL Steps</td><td>Search + Verifier</td></tr>
                                <tr><td colspan="4"><strong>Modality III: Structure-Enhanced Agentic Search</strong></td></tr>
                                <tr><td>Agent-G</td><td>Modular</td><td>NL + Graph Ops</td><td>KG Query</td></tr>
                                <tr><td>MC-Search</td><td>Multi-step</td><td>NL</td><td>Multimodal Search</td></tr>
                                <tr><td>GeAR</td><td>Graph</td><td>Graph Ops</td><td>KG Expansion</td></tr>
                                <tr><td>ARG</td><td>Reflective</td><td>NL + Symbols</td><td>KG Traversal</td></tr>
                            </table>
                        </div>
                        <h3>Structure-Enhanced Search</h3>
                        <p>Structure-enhanced agentic RAG systems enhance retrieval-augmented generation by enabling a single agent to reason over symbolic knowledge sources such as knowledge graphs through dynamic querying, tool invocation, and reflective self-monitoring.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Choosing a Search Strategy</h4>
                            <div class="analysis-item">
                                <h5>Selection Guide</h5>
                                <ul>
                                    <li><strong>In-Context:</strong> Quick deployment, no training, works with any LLM</li>
                                    <li><strong>Post-Training:</strong> Better performance, learned retrieval timing</li>
                                    <li><strong>Structure-Enhanced:</strong> When you have knowledge graphs or structured data</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            20: {
                title: "Self-Evolving Agentic Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4 &middot; Self-Evolving</div>
                        <h1>Self-Evolving Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <p>Self-evolving agentic reasoning refers to an agent's capacity to <em>improve its own reasoning process through experience</em>. At the core of this evolution lie two fundamental mechanisms: <strong>feedback</strong> and <strong>memory</strong>.</p>
                        <div class="definition-box">
                            <h4>Key Mechanisms</h4>
                            <ul>
                                <li><strong>Feedback:</strong> Provides evaluative signals for self-correction and refinement, allowing the agent to revise its reasoning strategies based on outcomes or environmental responses</li>
                                <li><strong>Memory:</strong> Acts as a persistent substrate for storing, organizing, and synthesizing past interactions, enabling knowledge accumulation and reuse across tasks</li>
                            </ul>
                        </div>
                        <p>Together, these mechanisms transform reasoning from a static process into a dynamic, adaptive loop capable of continual improvement.</p>
                        <h3>4.1 Agentic Feedback Mechanisms</h3>
                        <p>Agentic feedback mechanisms enable models to iteratively refine their reasoning and actions rather than relying on one-shot responses. By incorporating self-critique, verifier guidance, or validator-based resampling, these methods emulate human trial-and-error learning and form the foundation for autonomous self-improvement.</p>
                        <p>Broadly, they operate through three distinct feedback regimes: (1) <strong>reflective feedback</strong>, where models revise their reasoning through self-critique or verification; (2) <strong>parametric adaptation</strong>, where feedback is consolidated into updated model parameters; and (3) <strong>validator-driven feedback</strong>, where binary outcome signals guide resampling without introspection.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>The Learning Loop</h4>
                            <div class="analysis-item">
                                <h5>Why Self-Evolution Matters</h5>
                                <p>Static agents hit a performance ceiling. Self-evolving agents can: (1) learn from mistakes, (2) accumulate knowledge over time, (3) adapt to new domains, and (4) improve without human intervention. This is the path to truly autonomous AI systems.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            21: {
                title: "Feedback Mechanisms (Figure 5)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.1 &middot; Feedback</div>
                        <h1>Three Forms of Agentic Feedback</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 5: Three Forms of Agentic Feedback Mechanisms</h4>
                            <ul>
                                <li><strong>Reflective Feedback:</strong> Inference-time reflection enables real-time self-critique and revision during reasoning</li>
                                <li><strong>Parametric Adaptation:</strong> Offline adaptation consolidates feedback into model parameters for long-term improvement</li>
                                <li><strong>Validator-Driven Feedback:</strong> Outcome-based feedback relies on validator signals (success or failure) to refine behavior through retry</li>
                            </ul>
                        </div>
                        <h3>4.1.1 Reflective Feedback</h3>
                        <p>Reflective feedback methods improve model reliability by modifying the reasoning process during inference, without updating model parameters. These approaches expose intermediate reasoning outputs, such as chains of thought or partial solutions, and introduce additional assessment steps that directly influence how the model continues its generation.</p>
                        <p>Early self-critique and rationale-refinement methods implement reflection through an explicit generate-critique-revise loop. A model first produces an answer together with its reasoning. The same model, or a separately prompted critic role, then analyzes this output to identify logical errors, unsupported assumptions, or missing steps.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Feedback Trade-offs</h4>
                            <div class="analysis-item">
                                <h5>When to Use Each</h5>
                                <ul>
                                    <li><strong>Reflective:</strong> Best for complex reasoning tasks where self-correction is valuable</li>
                                    <li><strong>Parametric:</strong> Best when you have training data and want persistent improvements</li>
                                    <li><strong>Validator:</strong> Best for tasks with clear success/failure criteria (code, math)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            22: {
                title: "Parametric Adaptation",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.1.2 &middot; Feedback</div>
                        <h1>Parametric Adaptation</h1>
                    </div>
                    <div class="original-content">
                        <p>Parametric adaptation incorporates feedback into a model's parameters through additional training, producing persistent behavioral changes that generalize beyond individual inference episodes. Unlike reflective feedback, these methods transform feedback signals into supervised or preference-based training objectives that update the model's weights.</p>
                        <p>Trajectory-level supervised fine-tuning approaches attach feedback to intermediate reasoning traces rather than only final answers. Models first generate multi-step trajectories, which are then reviewed by humans, auxiliary models, or automated verifiers. Incorrect steps are corrected or replaced, and the resulting feedback-enriched trajectories are used as supervised training data.</p>
                        <h3>4.1.3 Validator-Driven Feedback</h3>
                        <p>Validator-driven feedback improves model outputs using external success or failure signals, without modifying the model's reasoning process or parameters. A validator, such as a unit test, constraint checker, simulator, or environment signal, evaluates candidate outputs and determines whether they satisfy predefined correctness criteria.</p>
                        <p>Retry-based systems implement this paradigm by repeatedly sampling candidate outputs until one passes validation. The model generates a complete solution, submits it to the validator, and discards it if validation fails.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Practical Implementation</h4>
                            <div class="analysis-item">
                                <h5>Key Insight</h5>
                                <p>Parametric adaptation offers durability—improvements persist across sessions. But it requires training infrastructure. Validator-driven feedback is simpler to implement but may require many retries for complex tasks.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            23: {
                title: "Feedback Mechanisms (Table 5)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.1 &middot; Feedback</div>
                        <h1>Representative Feedback Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="highlight-box">
                            <h4>Table 5: Agentic Feedback Mechanisms</h4>
                            <table class="data-table">
                                <tr><th>Method</th><th>Feedback Stage</th><th>Feedback Source</th><th>Update Target</th></tr>
                                <tr><td colspan="4"><strong>I. Reflective Feedback</strong></td></tr>
                                <tr><td>Reflexion</td><td>Inference</td><td>Self-generated critique</td><td>Trajectory</td></tr>
                                <tr><td>Self-Refine</td><td>Inference</td><td>Self-evaluation</td><td>Trajectory</td></tr>
                                <tr><td>Constitutional AI</td><td>Inference</td><td>Normative rules</td><td>Trajectory</td></tr>
                                <tr><td>RLAIF</td><td>Inference</td><td>AI verifier</td><td>Trajectory</td></tr>
                                <tr><td>MM-Verify</td><td>Inference</td><td>Multimodal verifier</td><td>Trajectory</td></tr>
                                <tr><td>ReAct</td><td>Inference</td><td>Action outcomes</td><td>Trajectory</td></tr>
                                <tr><td colspan="4"><strong>II. Parametric Adaptation</strong></td></tr>
                                <tr><td>AgentTuning</td><td>Training</td><td>High-quality trajectories</td><td>Model parameters</td></tr>
                                <tr><td>ReST</td><td>Training</td><td>Critique-revision pairs</td><td>Model parameters</td></tr>
                                <tr><td>ReflectEvo</td><td>Training</td><td>Reflection traces</td><td>Model parameters</td></tr>
                                <tr><td colspan="4"><strong>III. Validator-Driven Feedback</strong></td></tr>
                                <tr><td>CodeRL</td><td>Inference</td><td>Unit tests</td><td>Output only</td></tr>
                                <tr><td>LEVER</td><td>Inference</td><td>Execution results</td><td>Output only</td></tr>
                                <tr><td>SayCan</td><td>Inference</td><td>Environment state</td><td>Output only</td></tr>
                            </table>
                        </div>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Choosing a Feedback Method</h4>
                            <div class="analysis-item">
                                <h5>Selection Guide</h5>
                                <p>The table reveals a spectrum: Reflective methods are flexible but computationally expensive. Parametric methods are durable but require training. Validator methods are efficient but need external oracles.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            24: {
                title: "Agentic Memory (Figure 6)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.2 &middot; Memory</div>
                        <h1>Agentic Memory Overview</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 6: Overview of Agentic Memory in LLM Agents</h4>
                            <p>Three parallel dimensions:</p>
                            <ul>
                                <li><strong>In-context Use:</strong> Conversation (Text, Semantic) + Experience (Workflow, Trajectory)</li>
                                <li><strong>Structured Representation:</strong> Graph Memory (entities, events, facts) + Multimodal Memory</li>
                                <li><strong>Post-training Control:</strong> Control → Update → Memory Reward</li>
                            </ul>
                        </div>
                        <h3>4.2 Agentic Memory</h3>
                        <p>Recent advances in memory-augmented LLM agents have shifted the focus from static memory storage to more dynamic, interactive mechanisms that directly support agentic reasoning. Rather than merely extending the context window or storing historical inputs, memory is increasingly treated as an integral component of the reasoning loop.</p>
                        <p>The agent's reasoning process then operates not only on its immediate context but also on this persistent memory, enabling reflection, generalization, and long-term goal tracking.</p>
                        <h3>4.2.1 Agentic Use of Flat Memory</h3>
                        <p><strong>Factual Memory:</strong> Traditional memory systems for LLM agents typically treat memory as a passive buffer, mainly used to store dialogue histories or recent observations. Examples include dense retrieval methods in LangChain and LlamaIndex, and cache-inspired designs like MemGPT.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Evolution</h4>
                            <div class="analysis-item">
                                <h5>From Passive to Active</h5>
                                <p>The shift from "memory as storage" to "memory as reasoning substrate" is fundamental. Modern agentic memory doesn't just remember—it organizes, prioritizes, and actively participates in decision-making.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            25: {
                title: "Agentic Memory Systems (Table 6)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.2 &middot; Memory</div>
                        <h1>Representative Memory Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="highlight-box">
                            <h4>Table 6: Representative Agentic Memory Systems</h4>
                            <table class="data-table">
                                <tr><th>Method / System</th><th>Setting</th><th>Format</th><th>Memory Type</th></tr>
                                <tr><td colspan="4"><strong>I. Flat Memory (In-Context)</strong></td></tr>
                                <tr><td>LangMem</td><td>In-Context</td><td>Text</td><td>Factual</td></tr>
                                <tr><td>MemGPT</td><td>In-Context</td><td>Text</td><td>Factual</td></tr>
                                <tr><td>MemoryBank</td><td>In-Context</td><td>Semantic</td><td>Factual</td></tr>
                                <tr><td>Workflow Memory</td><td>In-Context</td><td>Workflow</td><td>Experience</td></tr>
                                <tr><td>MemOS</td><td>In-Context</td><td>Semantic</td><td>Factual</td></tr>
                                <tr><td>Evo-Memory</td><td>In-Context</td><td>Semantic</td><td>Semantic</td></tr>
                                <tr><td colspan="4"><strong>II. Structured Memory Representations</strong></td></tr>
                                <tr><td>GraphRAG</td><td>In-Context</td><td>Graph</td><td>Factual</td></tr>
                                <tr><td>MEMO</td><td>In-Context</td><td>Graph</td><td>Factual</td></tr>
                                <tr><td>Zep</td><td>In-Context</td><td>Graph</td><td>Factual</td></tr>
                                <tr><td>RAP</td><td>In-Context</td><td>Multimodal</td><td>Experience</td></tr>
                                <tr><td>Agent-ScanKit</td><td>In-Context</td><td>Multimodal</td><td>Experience</td></tr>
                                <tr><td colspan="4"><strong>III. Post-training Memory Control</strong></td></tr>
                                <tr><td>MemI</td><td>Post-training</td><td>Semantic</td><td>Factual</td></tr>
                                <tr><td>MemAgent</td><td>Post-training</td><td>Semantic</td><td>Factual</td></tr>
                                <tr><td>Memory-R1</td><td>Post-training</td><td>Semantic</td><td>Factual</td></tr>
                                <tr><td>MemRL</td><td>Post-training</td><td>Semantic</td><td>Experience</td></tr>
                            </table>
                        </div>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Architecture Choices</h4>
                            <div class="analysis-item">
                                <h5>Selection Criteria</h5>
                                <ul>
                                    <li><strong>Flat/Text:</strong> Simple, fast, good for short-term context</li>
                                    <li><strong>Graph:</strong> Better for relational reasoning, entity tracking</li>
                                    <li><strong>Multimodal:</strong> Required for embodied agents, visual tasks</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            26: {
                title: "Structured Memory & Experience Memory",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.2.2 &middot; Memory</div>
                        <h1>Structured Use of Memory</h1>
                    </div>
                    <div class="original-content">
                        <h3>Experience Memory</h3>
                        <p>Workflow Memory tracks procedural traces to enable plan recovery and consistent reasoning. Sleep-time Compute enables LLM agents to pre-compute and store anticipated reasoning steps before user interaction, effectively "thinking offline" using memory as a preparatory resource.</p>
                        <p>Dynamic Cheatsheet equips black-box models with external memory to store reusable strategies, reducing redundant reasoning. Other efforts explore complementary paradigms of agentic memory. In parallel, workflow memory has emerged as another structured approach, particularly suited for procedural and tool-augmented tasks.</p>
                        <h3>4.2.2 Structured Use of Memory</h3>
                        <p>Graph-based representations provide a flexible substrate for organizing relational knowledge in agents. GraphRAG serves as a foundational technique that augments retrieval with graph-structured reasoning, enabling more contextually coherent and multi-hop information integration.</p>
                        <p>Building on this foundation, agent systems such as MEMO and Zep organize memory explicitly as dynamic knowledge graphs, allowing agents to store, retrieve, and reason over entities, attributes, and their relations with improved efficiency and semantic grounding.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory as Infrastructure</h4>
                            <div class="analysis-item">
                                <h5>Practical Implications</h5>
                                <p>Structured memory (graphs, workflows) enables agents to reason about relationships and procedures that flat memory cannot capture. This is essential for complex, multi-step tasks requiring consistent state tracking.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            27: {
                title: "Post-Training Memory Control",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.2.3 &middot; Memory</div>
                        <h1>Post-Training Memory Control</h1>
                    </div>
                    <div class="original-content">
                        <p>Conversely, memory systems can also be controlled by the agent's reasoning process itself. Rather than relying on fixed heuristics for reading and writing memory, recent work has explored agent-controllable memory operations, where the agent explicitly decides what to store, when to retrieve, and how to interact with memory. This reframes memory as a policy target, no longer a passive buffer, but a resource that is actively shaped by reasoning.</p>
                        <p><strong>MemAgent</strong> formulates memory overwrite as a reinforcement learning problem: the agent is rewarded for preserving information that proves useful and for discarding irrelevant content. Using a newly proposed DAPO algorithm, the model learns to maintain a constant-sized memory across conversations while maximizing future utility.</p>
                        <p><strong>Memory-R1</strong> further advances this line by introducing a dual-agent design: a Memory Manager that dynamically decides when to add, update, or delete entries in the memory store, and an Answer Agent that distills the most relevant retrieved memories to guide response generation.</p>
                        <h3>4.3 Evolving Foundational Agentic Capabilities</h3>
                        <p>Recent advances view planning not as a fixed reasoning routine but as an evolving capability. Instead of relying on static datasets or human-designed curricula, agents can autonomously generate tasks, learn from their own feedback, and adapt strategies through iterative interaction with the environment.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Learning-Based Memory</h4>
                            <div class="analysis-item">
                                <h5>The Future of Memory</h5>
                                <p>The shift toward learning-based memory control represents a move from "memory as database" to "memory as learnable skill." Agents don't just use memory—they learn optimal memory strategies through experience.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            28: {
                title: "Evolving Capabilities (Figure 7)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.3 &middot; Evolution</div>
                        <h1>Self-Evolving Capabilities</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 7: Evolving Foundational Agentic Capabilities</h4>
                            <p>Three key dimensions:</p>
                            <ul>
                                <li><strong>Self-evolving Planning:</strong> Task Generation → Strategy Refinement</li>
                                <li><strong>Self-evolving Tool-Use:</strong> Tool Synthesis → Tool Creation</li>
                                <li><strong>Self-evolving Search:</strong> Knowledge Synthesis → Dynamic Retrieval</li>
                            </ul>
                        </div>
                        <h3>4.3.1 Self-evolving Planning</h3>
                        <p>A representative direction is self-generated task construction. For example, SCA enables agents to alternate between generating problems and solving them, reusing successful trajectories for fine-tuning. Self-rewarding frameworks further allow agents to assess their own outputs, producing high-quality training signals without human labels.</p>
                        <h3>4.3.2 Self-evolving Tool-use</h3>
                        <p><strong>Creating and Synthesizing Tools:</strong> The culmination of in-context reasoning is the emergent capability of agents to autonomously create new tools. This is achieved not through training, but by prompting a frozen LLM to act as a programmer when it encounters a problem that its existing toolset cannot solve. The LATM framework uses a powerful model as a one-time "tool maker" and a cheaper, lightweight model as a frequent "tool user," thus amortizing the cost of creation.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Autonomous Improvement</h4>
                            <div class="analysis-item">
                                <h5>The Self-Improvement Loop</h5>
                                <p>Self-evolving agents represent the path to truly autonomous AI: systems that identify their own weaknesses, generate training data, and improve without human intervention. This is the key to scaling AI capabilities beyond human oversight.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            29: {
                title: "Self-Evolving Search & Multi-Agent Introduction",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 4.3.3 / Section 5</div>
                        <h1>Self-Evolving Search & Collective Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <h3>4.3.3 Self-evolving Search</h3>
                        <p>Search plays a central role in agentic reasoning, enabling models to retrieve, select, and synthesize relevant knowledge across large and evolving memory spaces. In early systems, search was typically static—built on fixed retrieval heuristics or similarity-based dense retrievers. These methods augmented prompts with retrieved information but lacked adaptive control over how memory evolves or how search strategies are improved over time.</p>
                        <p>Recent research increasingly links search and memory in a <strong>co-evolutionary loop</strong>: agents continuously update their memory base during task execution, while dynamically adjusting how search is performed over this evolving knowledge.</p>
                        <h2>5. Collective Multi-agent Reasoning</h2>
                        <p>Building upon the single-agent foundation, where reasoning supports planning, search, and tool use within a unified perception-action loop, <strong>multi-agent reasoning</strong> extends these principles to collaborative settings. In a multi-agent system (MAS), multiple reasoning agents interact to jointly solve complex tasks.</p>
                        <div class="definition-box">
                            <h4>Key Challenges</h4>
                            <ul>
                                <li><strong>Role differentiation:</strong> how to design static or adaptive roles that align with task structure</li>
                                <li><strong>Collaboration and communication:</strong> how agents exchange intermediate reasoning and divide labor efficiently</li>
                                <li><strong>Collective memory and evolution:</strong> how shared or distributed state supports long-term coordination</li>
                            </ul>
                        </div>
                    </div>
                `
            },
            30: {
                title: "Multi-Agent Role Taxonomy (Figure 8)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.1 &middot; Multi-Agent</div>
                        <h1>Role Taxonomy of Multi-Agent Systems</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 8: Generic Roles and Domain Adaptations</h4>
                            <p><strong>Generic Roles:</strong></p>
                            <ul>
                                <li>Leader/Coordinator</li>
                                <li>Worker/Executor</li>
                                <li>Critic/Evaluator</li>
                                <li>Memory Keeper</li>
                                <li>Communication Facilitator</li>
                            </ul>
                            <p><strong>Domain-Specific Adaptations:</strong> Software Engineering, Finance, Legal Activities, Education, Healthcare, Biomedicine, Music</p>
                        </div>
                        <h3>5.1.1 Generic Roles</h3>
                        <p><strong>Leader/Coordinator:</strong> The leader is responsible for maintaining high-level coherence within the system. This role involves setting global objectives, decomposing tasks into manageable subgoals, and assigning them to appropriate agents. The leader arbitrates conflicts that emerge between agents with overlapping or contradictory outputs.</p>
                        <p><strong>Worker/Executor:</strong> Executors are the operational backbone of MAS. They engage in concrete actions such as invoking external tools, writing or executing code, retrieving documents, or interfacing with the environment.</p>
                        <p><strong>Critic/Evaluator:</strong> This role centers on quality assurance, verifying correctness, testing hypotheses, red-teaming responses, and surfacing potential risks.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Role Design Principles</h4>
                            <div class="analysis-item">
                                <h5>Practical Guidance</h5>
                                <p>Effective multi-agent systems mirror human organizations: clear responsibilities, communication channels, and accountability. The Leader-Worker-Critic pattern maps directly to Manager-Developer-QA in software teams.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            31: {
                title: "Generic & Domain-Specific Roles",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.1.2 &middot; Multi-Agent</div>
                        <h1>Domain-Specific Roles</h1>
                    </div>
                    <div class="original-content">
                        <p><strong>Memory Keeper:</strong> Effective MAS requires persistent memory to accumulate context, prevent repetitive failures, and enable learning across episodes. The memory keeper curates and maintains long-term knowledge structures such as episodic logs, semantic embeddings, retrieval indices, or knowledge graphs.</p>
                        <p><strong>Communication Facilitator:</strong> Communication overhead can easily undermine MAS efficiency. This role governs protocols for inter-agent exchange, including defining message schemas, managing communication bandwidth, enforcing gating mechanisms, and orchestrating consensus-building.</p>
                        <h3>5.1.2 Domain-Specific Roles</h3>
                        <p><strong>Software Engineering:</strong> In software engineering, MAS generally maps onto roles that mirror the software development lifecycle: architects, developers, code reviewers/testers, CI orchestrators, and release managers.</p>
                        <ul>
                            <li>Architects define system-level design principles and establish structural blueprints</li>
                            <li>Developers translate these abstractions into concrete implementations</li>
                            <li>Code reviewers and testers safeguard reliability, checking correctness, maintainability, and functional coverage</li>
                            <li>CI orchestrators automate builds, testing, and artifact pipelines</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Mapping to Real Teams</h4>
                            <div class="analysis-item">
                                <h5>Software Development Example</h5>
                                <p>MetaGPT demonstrated that mapping agents to software roles (PM, Architect, Engineer, QA) produces more coherent code than single-agent approaches. The key is clear interfaces between roles.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            32: {
                title: "Finance, Legal & Education Domains",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.1.2 &middot; Multi-Agent</div>
                        <h1>Domain Applications</h1>
                    </div>
                    <div class="original-content">
                        <h3>Finance</h3>
                        <p>The financial domain can be decomposed into four archetypal roles: analysts, risk managers, traders/execution agents, and compliance officers. This division reflects the established institutional design of financial organizations.</p>
                        <ul>
                            <li>Analysts operate at different levels (fundamental, sentiment, or technical), each extracting distinct signals from raw market or textual data</li>
                            <li>Risk Managers monitor portfolio exposure, apply stress tests, and enforce safeguards to prevent cascading vulnerabilities</li>
                            <li>Traders take responsibility for market interaction, ensuring orders are placed with speed and efficiency</li>
                            <li>Compliance roles ensure activities remain aligned with regulatory requirements</li>
                        </ul>
                        <h3>Legal Activities</h3>
                        <p>Multi-agent systems are designed to model the collaborative and adversarial processes inherent in legal practice, with roles assigned to manage consultation, reasoning, and argumentation:</p>
                        <ul>
                            <li>For legal consultation, frameworks feature a receptionist agent for client intake, specialized lawyer agents for providing advice, and a boss agent for quality control</li>
                            <li>For statutory reasoning, tasks are decomposed between knowledge acquisition agents and knowledge application agents</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Domain Adaptation</h4>
                            <div class="analysis-item">
                                <h5>Key Pattern</h5>
                                <p>Each domain naturally suggests role structures that mirror human expertise divisions. The most successful multi-agent systems leverage existing organizational knowledge rather than inventing new structures.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            33: {
                title: "Healthcare & Biomedicine Domains",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.1.2 &middot; Multi-Agent</div>
                        <h1>Healthcare & Biomedicine</h1>
                    </div>
                    <div class="original-content">
                        <h3>Education</h3>
                        <p>In education, MAS is being developed to provide personalized and adaptive learning experiences by distributing pedagogical functions among specialized agents:</p>
                        <ul>
                            <li>For personalized tutoring, a central tutor agent might engage a student using Socratic dialogue, while a memory dispatcher agent tracks the student's progress and misconceptions</li>
                            <li>For curriculum design, a pipeline of agents collaborates: a research agent gathers relevant information, a planning agent structures it into a coherent course, and other agents generate specific learning activities</li>
                        </ul>
                        <h3>Healthcare</h3>
                        <p>In the healthcare domain, multi-agent systems are structured to mirror clinical and research workflows, distributing complex tasks among specialized AI agents:</p>
                        <ul>
                            <li>For clinical diagnostics and consultation, roles include a triage agent (or moderator) for initial case assessment, various specialist agents (e.g., pathologists, neurologists), a doctor agent for patient interaction, and a measurement agent for test results</li>
                            <li>For autonomous research, roles are modeled after the scientific process, featuring a meta agent for strategic planning, an executor for running analyses, an evaluator for assessing outcomes, and a reflector for synthesizing knowledge</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>High-Stakes Domains</h4>
                            <div class="analysis-item">
                                <h5>Safety Considerations</h5>
                                <p>Healthcare and legal domains require additional verification and human oversight. Multi-agent systems in these domains typically include explicit critic/evaluator roles and audit trails.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            34: {
                title: "Collaboration Patterns (Figure 9)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.2 &middot; Collaboration</div>
                        <h1>Collaboration and Division of Labor</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 9: Overview of Agentic Collaboration</h4>
                            <p><strong>In-context Collaboration:</strong></p>
                            <ul>
                                <li>Cascading: Manually design cascading calls with 1-best decoding</li>
                                <li>Hierarchical: Manually design with manager, worker roles</li>
                                <li>Role-based: Automated assigning from LLM discussions</li>
                            </ul>
                            <p><strong>Post-training Collaboration:</strong></p>
                            <ul>
                                <li>Prompt Opt.: Refine task-specific prompts with multi-agent feedback</li>
                                <li>Graph-based Opt.: Model topology as graphs for optimization</li>
                                <li>Policy-based Opt.: Learn agent-specific update and selection policies</li>
                            </ul>
                        </div>
                        <h3>5.2 Collaboration and Division of Labor</h3>
                        <p>Collaboration and division of labor constitute a central organizing principle in modern multi-agent systems. Instead of treating agents as homogeneous components, recent work emphasizes how responsibilities are decomposed and coordinated across specialized agents to improve efficiency and robustness.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Collaboration Architectures</h4>
                            <div class="analysis-item">
                                <h5>Design Trade-offs</h5>
                                <ul>
                                    <li><strong>Cascading:</strong> Simple, predictable, but rigid</li>
                                    <li><strong>Hierarchical:</strong> Scalable, clear accountability, coordination overhead</li>
                                    <li><strong>Role-based:</strong> Flexible, emergent behavior, harder to control</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            35: {
                title: "In-Context Collaboration",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.2.1 &middot; Collaboration</div>
                        <h1>In-Context Collaboration</h1>
                    </div>
                    <div class="original-content">
                        <h3>5.2.1 In-context Collaboration</h3>
                        <p>In the design of multi-agent systems, several studies have observed that leveraging task-specific in-context information is often sufficient to build highly effective systems without the need for explicit training.</p>
                        <h3>Manually Crafted Pipelines</h3>
                        <p>These approaches rely on predefined hierarchies or fixed collaboration workflows, where agent roles, execution order, and communication rules are determined before execution. Hierarchical systems such as AgentOrchestra, MetaGPT, and SurgRAW feature a central planner or conductor directing subordinate agents through structured subgoals.</p>
                        <p>Cascading pipelines like CoRAG, MA-RAG, Chain of Agents, and AutoAgents process information sequentially, passing intermediate outputs downstream with limited revision.</p>
                        <h3>LLM-Driven Pipelines</h3>
                        <p>This category leverages LLMs as orchestrators that decompose high-level goals into subgoals, route them to role-specialized agents or tools, and iteratively refine workflows based on intermediate feedback until completion.</p>
                        <p><strong>AutoML-Agent</strong> proposes a full-pipeline, orchestrator-led agent team that plans, assigns, and coordinates web/API/code tools through role-specialized micro-agents. <strong>Magnetic-One</strong> presents a generalizable multi-agent system where a central Orchestrator plans, tracks progress, and performs ledger-based routing over specialized agents.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Pipeline Design</h4>
                            <div class="analysis-item">
                                <h5>Implementation Guidance</h5>
                                <p>Start with manually crafted pipelines for well-understood tasks. Move to LLM-driven orchestration when task requirements are dynamic or when you need to handle novel task types.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            36: {
                title: "Post-Training Collaboration",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.2.2 &middot; Collaboration</div>
                        <h1>Post-Training Collaboration</h1>
                    </div>
                    <div class="original-content">
                        <h3>Theory-of-Mind-Augmented Collaboration</h3>
                        <p>Another interesting line of research is Theory of Mind (ToM), which refers to the ability of an agent to infer and reason about the beliefs, intentions, and mental states of other agents. Li et al. first showed that equipping LLM agents with explicit belief-state representations in a cooperative text game improves both collaboration performance and accuracy over ToM-free LLM baselines.</p>
                        <h3>5.2.2 Post-training Collaboration</h3>
                        <p>In multi-agent systems, the design of agent prompts (or personas) and the interaction topology plays a critical role in determining the system's ability to solve complex tasks. Recently, optimizing these components during the post-training phase has emerged as an important research direction.</p>
                        <h3>Multi-agent Prompt Optimization</h3>
                        <p>Prompt optimization in multi-agent systems focuses on how agent roles, workflows, and feedback are encoded in prompts to yield reliable coordination and stronger task performances. For example, AutoAgents extends prompt optimization from single-agent contexts to multi-agent teams, refining role specialization and execution plans through structured dialogue among meta-agents.</p>
                        <h3>Graph-based Topology Generation</h3>
                        <p>A large body of work models multi-agent systems (MAS) as graphs where agents are nodes, and inter-agent communication forms edges. Then MAS design becomes a problem of learning the communication/coordination topology.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Optimization Approaches</h4>
                            <div class="analysis-item">
                                <h5>Key Methods</h5>
                                <ul>
                                    <li><strong>Prompt Optimization:</strong> Learn better role descriptions and instructions</li>
                                    <li><strong>Topology Learning:</strong> Discover optimal communication patterns</li>
                                    <li><strong>Policy Learning:</strong> Train agent-specific decision policies</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            37: {
                title: "Policy-Based Optimization",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.2.2 &middot; Collaboration</div>
                        <h1>Policy-Based Topology Generation</h1>
                    </div>
                    <div class="original-content">
                        <h3>Graph Pruning</h3>
                        <p>These works start from dense collaboration graphs and aim to prune them into compact, task-appropriate pipelines while preserving utility and lowering token and compute costs. For example, AgentPrune lists the MAS problem as a spatial-temporal graph sparsification problem, and then applies one-shot magnitude pruning to learn a sparse and effective pipeline.</p>
                        <h3>Topology Search</h3>
                        <p>This line of research explores the graph space by searching over agentic operators and communication edges to identify effective pipelines. Specifically, AFlow automates multi-agent workflow design with Monte-Carlo Tree Search over a fixed library of operators. MASS pre-defines some influential graph models, such as debating and tool-using, and then implements topology search inside this pruned model subset.</p>
                        <h3>Policy-based Topology Generation</h3>
                        <p>A growing line of research strengthens multi-agent pipeline generation by learning the policy of selecting subsequent agents with advanced training paradigms such as supervised fine-tuning (SFT), and reinforcement learning (RL). These approaches embed auxiliary signals into the optimization process, enabling agents to acquire stronger reasoning skills and more reliable coordination.</p>
                        <p><strong>MAGRPO</strong> proposes a Dec-POMDP formulation for LLM collaboration and replaces centralized critics with a group-relative advantage signal, enabling decentralized training/execution at dialog-turn granularity.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Learning Communication</h4>
                            <div class="analysis-item">
                                <h5>Key Insight</h5>
                                <p>Rather than hand-designing agent communication patterns, these methods learn optimal topologies from data. This enables adaptation to specific task distributions and can discover non-obvious collaboration patterns.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            38: {
                title: "Multi-Agent Evolution",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.3 &middot; Evolution</div>
                        <h1>Multi-Agent Evolution</h1>
                    </div>
                    <div class="original-content">
                        <h3>5.3 Multi-Agent Evolution</h3>
                        <p>While self-evolving agents enable individual models to continuously improve through interaction and feedback, many real-world applications require collective intelligence supported by cooperation among multiple agents. Therefore, recent studies extend self-evolution from single-agent settings including planning, tool-use, and search evolution to <strong>multi-agent co-evolution</strong>, where adaptation emerges across distributed agents.</p>
                        <p>As a result, multi-agent memory must jointly evolve along architecture, topology, content, and management dimensions, supported by hierarchy-structured, role-aware architectures, governed and distributed storage topologies, modular and task-structured memory contents, and active management mechanisms for compression, verification, and continual updating.</p>
                        <h3>5.3.1 From Single-Agent Evolution to Multi-Agent Evolution</h3>
                        <p>While the shift from single-agent evolution to multi-agent co-evolution broadens the spatial dimension of adaptation from an individual model to a collective, the temporal dimension of evolution remains equally crucial. Beyond determining who evolves (a single agent or a population), recent studies also investigate when and how fast agents should adapt during interaction.</p>
                        <p>We summarize these temporal modes of self-evolving behavior as <strong>intra-test-time evolution</strong> and <strong>inter-test-time evolution</strong>.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Collective Learning</h4>
                            <div class="analysis-item">
                                <h5>Evolution Dimensions</h5>
                                <ul>
                                    <li><strong>Spatial:</strong> Single agent vs. collective adaptation</li>
                                    <li><strong>Temporal:</strong> Within-episode vs. across-episode learning</li>
                                    <li><strong>Content:</strong> What knowledge/skills are evolved</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            39: {
                title: "Intra & Inter Test-Time Evolution",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.3.1 &middot; Evolution</div>
                        <h1>Temporal Modes of Evolution</h1>
                    </div>
                    <div class="original-content">
                        <h3>Intra-test-time Evolution</h3>
                        <p>Intra-test-time evolution refers to the ability of agents to adapt and improve during task execution, enabling them to correct failures and refine strategies on the fly when facing unseen states or unexpected feedback. Unlike static inference pipelines, this paradigm embeds self-reflection, dynamic planning, memory rewriting, or even localized fine-tuning into the execution loop.</p>
                        <p>Reflexion allows agents to store distilled reflective feedback for immediate behavior improvement, while AdaPlanner dynamically revises and replans mid-trajectory based on environmental mismatch detection.</p>
                        <h3>Inter-test-time Evolution</h3>
                        <p>Inter-test-time evolution extends the self-improving process to across-task learning, where adaptations made in one task can be consolidated and transferred to future tasks. This enables the accumulation of persistent, generalizable capabilities over a lifelong interaction stream.</p>
                        <p>A prominent paradigm involves offline self-distillation, where the agent generates responses and then refines them via self-evaluation before using them for supervised fine-tuning-such-as in SELF, STaR, and Quest-STaR. These methods turn incorrect initial reasoning into high-quality labeled data for future performance gains.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>When to Evolve</h4>
                            <div class="analysis-item">
                                <h5>Practical Trade-offs</h5>
                                <ul>
                                    <li><strong>Intra-test:</strong> Immediate adaptation, higher compute cost per task</li>
                                    <li><strong>Inter-test:</strong> Persistent learning, requires offline processing</li>
                                    <li><strong>Hybrid:</strong> Best of both—adapt now, consolidate later</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            40: {
                title: "Multi-Agent Memory (Figure 10)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.3.2 &middot; Memory</div>
                        <h1>Multi-Agent Memory Management</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 10: Four Dimensions of Multi-Agent Memory Design</h4>
                            <ul>
                                <li><strong>Architecture:</strong> Hierarchical vs. Flat organization</li>
                                <li><strong>Topology:</strong> Centralized vs. Decentralized storage</li>
                                <li><strong>Content:</strong> Semantic vs. Procedural knowledge</li>
                                <li><strong>Management:</strong> Summarize & Forget vs. Filter & Verify</li>
                            </ul>
                        </div>
                        <h3>Architecture Dimension: Hierarchical and Heterogeneous Designs</h3>
                        <p>Recent work highlighted that prevailing multi-agent memory mechanisms were overly simplistic and lacked per-agent customization. To address this, G-Memory constructs a three-tier graph hierarchy (insight, query, interaction graphs) that separates high-level generalizable insights from fine-grained execution traces.</p>
                        <p>This hierarchical approach enables bi-directional memory traversal for retrieving both abstract lessons and concrete precedents across episodes. However, instead of global aggregation, Intrinsic Memory Agents adopts an opposing strategy by maintaining dedicated role-aligned memory templates for each agent.</p>
                        <h3>Storage Topology and Memory Governance</h3>
                        <p>Systems employ different topologies to balance scalability, privacy, and coherence. SEDM (Self-Evolving Distributed Memory) tackles memory management by turning memory into an active, self-optimizing component through verifiable write admission and utility-based consolidation.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Architecture Trade-offs</h4>
                            <div class="analysis-item">
                                <h5>Design Considerations</h5>
                                <ul>
                                    <li><strong>Hierarchical:</strong> Better for complex reasoning, higher maintenance</li>
                                    <li><strong>Flat:</strong> Simpler, faster access, may miss relationships</li>
                                    <li><strong>Centralized:</strong> Consistent, single point of failure</li>
                                    <li><strong>Decentralized:</strong> Resilient, coordination challenges</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            41: {
                title: "Memory Content and Management Strategies",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.3.2 &middot; Memory (continued)</div>
                        <h1>Memory Content and Management Strategies</h1>
                    </div>
                    <div class="original-content">
                        <h3>Memory Content: Semantic, Task, and Cognitive-Phase Decomposition</h3>
                        <p>Different content decomposition strategies suit different task characteristics, and the choice of content structure fundamentally shapes how agents interact with memory. MIRIX [328] pioneered semantic decomposition by defining six specialized memory types (Core, Episodic, Semantic, Procedural, Resource, Knowledge Vault) managed by distinct agents, achieving a 35% accuracy gain on multimodal QA tasks while reducing storage through flexible routing.</p>
                        <p>Building on this modular principle, LEGOMem [428] instead employs task-based decomposition, breaking execution traces into reusable memory units flexibly assigned to either central planners or specialist task agents. This design shows that orchestrator memory improves task decomposition and delegation, while agent memory enhances subtask execution, effectively narrowing performance gaps between small and large LLM teams.</p>
                        <p>Recently, MAPLE introduced Cognitive-phase Decomposition [145], using specialized agents (Solver, Checker, Reflector, Archiver) to enable systematic error detection and plan repair cycles. The Reflector diagnoses errors after each episode, and the Archiver stores refined plans to avoid repeated mistakes, supporting feedback-driven learning.</p>
                        <h3>Memory Management Strategies</h3>
                        <p>Effective long-term memory requires active management balancing relevance, efficiency, and coherence through different approaches that trade off simplicity against sophistication. Lyte Agents [429] pioneered the forgetting-based approach using Summarize-and-Forget mechanisms to regularly compress memory, retaining only critical context. This strategy is suitable when storage is severely constrained, though it risks losing nuanced details for edge cases.</p>
                        <p>To improve upon simple forgetting, AGENTGB [430] introduced more sophisticated management by organizing procedural traces into structured (entity, action, observation) triples and learning pattern abstractions reusable across tasks. Agents collaborate to retrieve, update, and reason over memory segments, enabling generalization without explicit retraining while central coordination ensures long-term consistency for scalable embodied planning.</p>
                        <h3>Discussions</h3>
                        <p>Despite substantial progress, multi-agent memory systems remain largely unexplored with respect to post-training and model adaptation. Current approaches focus primarily on memory organization and retrieval for pre-trained models, with little investigation into how multiple agents can jointly optimize their memories through post-training procedures such as reinforcement learning or supervised fine-tuning.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Decomposition Approaches</h4>
                            <div class="analysis-item">
                                <h5>Semantic vs Task-Based</h5>
                                <p><strong>Semantic:</strong> Organizes by knowledge type (episodic, procedural, etc.) - good for diverse queries</p>
                                <p><strong>Task-Based:</strong> Organizes by execution traces - good for similar recurring tasks</p>
                                <p><strong>Cognitive-Phase:</strong> Organizes by reasoning stage - good for error detection and learning</p>
                            </div>
                        </div>
                    </div>
                `
            },
            42: {
                title: "Training Multi-Agent to Evolve",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 5.3.3 &middot; Training Multi-agent to Evolve</div>
                        <h1>Training Multi-Agent Systems to Evolve</h1>
                    </div>
                    <div class="original-content">
                        <p>Recent advancements have shifted multi-agent systems from fixed, hand-designed coordination toward training paradigms that enable agents to evolve over time. Training multi-agent systems to evolve represents a critical step toward realizing adaptive, long-horizon intelligence beyond static coordination.</p>
                        <p>In this emerging paradigm, agents improve collectively through interaction, feedback, and shared memory, rather than isolated or independently optimized behaviors. By embedding reasoning into the learning loop, via reinforcement learning, self-play, curriculum evolution, and verifier-driven feedback, multi-agent systems can internalize coordination strategies, address inter-agent credit assignment, and progressively refine divisions of labor.</p>
                        <h3>Co-evolution via Interaction and Intrinsic Feedback</h3>
                        <p>A growing body of work has operationalized multi-agent evolution through explicit training objectives that couple interaction, feedback, and role specialization. For instance, Multi-Agent Evolve [446] instantiates a closed-loop co-evolution framework containing three interacting roles (Proposer, Solver, and Judge), all of which are derived from a shared LLM backbone and jointly optimized via reinforcement learning.</p>
                        <p>This forms a self-improving curriculum that enables collective skill growth without external supervision. In a related spirit, CoMAS [451] emphasizes intrinsic interaction rewards, extracting learning signals directly from multi-agent discussion dynamics through an LLM-based judge, thereby enabling decentralized co-evolution driven purely by collaborative interaction.</p>
                        <h3>Multi-Agent Reinforcement Fine-Tuning for Collective Adaptation</h3>
                        <p>Additional works have focused on principled reinforcement fine-tuning frameworks tailored to LLM-based multi-agent systems. For example, MARFT [447] formalizes multi-agent reinforcement fine-tuning by highlighting key mismatches between classical MARL assumptions and LLM-based agent organizations, such as role heterogeneity, dynamic coordination, and long-horizon dialogue.</p>
                        <h3>Role Specialization and Joint Credit Assignment</h3>
                        <p>Other approaches have explored structured role specialization and joint credit assignment. MALT [452] trains sequential pipelines of heterogeneous agents using trajectory expansion and outcome-based reinforcement signals, allowing each agent to improve its specialized function while optimizing end-to-end collaborative performance.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Evolution Mechanisms</h4>
                            <div class="analysis-item">
                                <h5>Key Training Approaches</h5>
                                <ul>
                                    <li><strong>Co-evolution:</strong> Agents learn together through interaction feedback</li>
                                    <li><strong>Reinforcement Fine-Tuning:</strong> RL-based optimization for multi-agent coordination</li>
                                    <li><strong>Role Specialization:</strong> Agents develop distinct specialized capabilities</li>
                                    <li><strong>Credit Assignment:</strong> Properly attributing success to individual agent contributions</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            43: {
                title: "Applications Overview (Figure 11)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6 &middot; Applications</div>
                        <h1>Applications of Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 11: Overview of Applications of Agentic Reasoning</h4>
                            <p>The figure presents a comprehensive taxonomy of agentic reasoning applications across key domains:</p>
                            <ul>
                                <li><strong>Reasoning Layers:</strong> Foundational, Self-evolving, and Collective</li>
                                <li><strong>Math Exploration and Vibe Coding</strong> (Section 6.1)</li>
                                <li><strong>Scientific Discovery Agents</strong> (Section 6.2)</li>
                                <li><strong>Embodied Agents</strong> (Section 6.3)</li>
                                <li><strong>Healthcare & Medicine Agents</strong> (Section 6.4)</li>
                                <li><strong>Web Exploration and Research Agents</strong> (Section 6.5)</li>
                            </ul>
                        </div>
                        <p>Building on the established three-layer taxonomy (foundational, self-evolving, and collective reasoning) mentioned in previous sections, we now examine how these capabilities manifest across real-world applications. This section surveys representative reasoning-empowered agentic systems across several key domains.</p>
                        <p>Specifically, each domain exhibits distinctive forms of reasoning, influenced by its data modalities and environmental constraints. Accordingly, our discussion in each subsection is organized around three layers: (1) <strong>core abilities</strong> such as planning, tool use and search that span scientific hypothesis generation, embodied control, medical reasoning, automated experimentation and symbolic problem-solving; (2) <strong>self-evolving abilities</strong> that integrate feedback, reflection and memory modules which refine domain-specific competence through iterative experiment loops, lifelong skill learning and clinical adaptation; and (3) <strong>collective multi-agent reasoning</strong> that enables collaboration and specialization from cooperative scientific assistants to coordinated robotic teams, diagnostic ensembles or multi-aspect experts.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Application Domain Organization</h4>
                            <div class="analysis-item">
                                <h5>Three-Layer Framework Applied</h5>
                                <p>Each application domain is analyzed through the lens of the three reasoning layers, showing how foundational capabilities (planning, tools), self-evolving mechanisms (feedback, memory), and collective reasoning (multi-agent coordination) combine to address domain-specific challenges.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            44: {
                title: "Math Exploration & Vibe Coding Agents",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.1 &middot; Math & Code</div>
                        <h1>Math Exploration & Vibe Coding Agents</h1>
                    </div>
                    <div class="original-content">
                        <p>Mathematics and code have traditionally served as two of the most widely used domains for evaluating reasoning in artificial intelligence, as both require structured symbolic manipulation and precise multi-step deduction. Traditional benchmark-driven evaluation in these domains is showing clear limitations. Widely used math datasets such as GSM8K, MATH, and AIME are increasingly saturated, which makes it difficult to distinguish among modern high-performing models.</p>
                        <p>Under the agentic reasoning paradigm, however, both areas are undergoing a substantial shift from static problem solving to dynamic processes that emphasize exploration, adaptation, and collaboration. In mathematics, recent systems demonstrate that agents can engage in competition-level reasoning, building on the success of LLMs in coding tasks. Work in foundational mathematics further shows that agents can search for new problems, propose conjectures, construct auxiliary lemmas, and explore deeper structures in mathematical concepts.</p>
                        <h3>Large Language Models and Vibe Coding</h3>
                        <p>Large Language Models have also reshaped coding through the emerging workflow known as <em>agentic coding</em> and <em>vibe coding</em>. In this paradigm, the model acts as an interactive collaborator that engages in multi-turn natural-language dialogue. Users iteratively design and refine programs while the agent maintains context, adapts to evolving requirements, and continuously self-corrects. Modern tools such as Copilot and Cursor have further popularized this collaborative workflow, making interactive programming a common practice in real-world software development.</p>
                        <h3>6.1.1. Foundational Agentic Reasoning</h3>
                        <p><strong>Planning.</strong> Explicit planning is widely recognized as a core mechanism for enhancing the structured reasoning capabilities of LLMs. In the domain of mathematical discovery, several systems exhibit structures that can be interpreted as forms of planning.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Vibe Coding Paradigm</h4>
                            <div class="analysis-item">
                                <h5>Key Characteristics</h5>
                                <ul>
                                    <li><strong>Interactive:</strong> Multi-turn dialogue for iterative refinement</li>
                                    <li><strong>Context-Aware:</strong> Maintains project context across sessions</li>
                                    <li><strong>Self-Correcting:</strong> Adapts based on feedback and errors</li>
                                    <li><strong>Collaborative:</strong> User and AI work together as partners</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            45: {
                title: "Planning and Tool-Use in Math/Code",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.1.1 &middot; Foundational Reasoning</div>
                        <h1>Planning and Tool-Use in Mathematics and Code</h1>
                    </div>
                    <div class="original-content">
                        <h3>Planning in Mathematics</h3>
                        <p>Trinh et al. [29] solves Olympiad-level geometry problems by decomposing them into sequential stages of construction, lemma generation, and verification, yielding a structured multi-step process that resembles a planned reasoning trajectory. Program-search approaches [30] iteratively refine candidate programs and mathematical structures, a procedure that naturally forms a coarse-to-fine exploration path.</p>
                        <p>Large-scale exploration frameworks [461, 460] also operate through cycles of proposing, testing, and modifying conjectures or geometric objects, which collectively create a procedural structure aligned with planning. Efforts toward more robust mathematical reasoning [459] similarly rely on stepwise reasoning patterns, further reinforcing the presence of implicit planning dynamics.</p>
                        <h3>Planning in Code Agents</h3>
                        <p>In code agents, planning has likewise emerged as an essential component for organizing multi-step reasoning and enabling more structured decision-making. Early systems such as CodeChain [464] and CodeAct [99] introduce explicit planning or action spaces to support modular code construction, while KareCoder [465] integrate external knowledge sources or domain-specific information into the planning process.</p>
                        <h3>Tool-Use</h3>
                        <p>Integrating external computational tools with LLMs has become a central mechanism for extending the reasoning and generation capabilities of single-agent systems. A defining characteristic of many mathematical reasoning systems is their integration with external computational tools. Formal theorem-proving agents such as Thakur et al. [473] operate directly within the Lean proof assistant, selecting tactics and interacting with the underlying prover through in-context guidance.</p>
                        <p>In code agents, external tools have similarly become crucial for extending the capabilities of LLM-based agents beyond pure text generation. Early work such as Toolformer [6] and ToolCoder [477] explored how models can learn to invoke APIs or search tools to obtain missing information during generation.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tool Integration Patterns</h4>
                            <div class="analysis-item">
                                <h5>External Tool Categories</h5>
                                <ul>
                                    <li><strong>Theorem Provers:</strong> Lean, Coq for formal verification</li>
                                    <li><strong>Symbolic Systems:</strong> Computer algebra, satisfiability solvers</li>
                                    <li><strong>Code Execution:</strong> Runtime feedback, test execution</li>
                                    <li><strong>Search APIs:</strong> Documentation, code repositories</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            46: {
                title: "Search and Retrieval for Math/Code",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.1.1 &middot; Foundational Reasoning (continued)</div>
                        <h1>Search and Retrieval in Mathematics and Code</h1>
                    </div>
                    <div class="original-content">
                        <h3>Search and Retrieval</h3>
                        <p>Search and retrieval has emerged as a complementary mechanism that enriches model contexts through external information sources. Search is a recurring mechanism in mathematical discovery. Program-search based systems [30] treat mathematical discovery as navigating a program space in which candidate programs encode conjectures or structural hypotheses, with iterative filtering based on symbolic or numerical checks.</p>
                        <p>Generative modelling approaches [475] explore families of mathematical objects by sampling from flexible distributions that capture structural regularities. Geometric systems such as Trinh et al. [29] and Swirszcz et al. [460] search over constructions, configurations, and high-dimensional polytopes, guided by learned heuristics or structural constraints.</p>
                        <p>In code generation, repository-level retrieval systems such as RepoHyper [482] locate reusable code segments from large-scale code bases to provide more informative contexts for generation. CodeNew [79] dynamically indexes real repositories during generation, retrieving relevant functions and adjusting based on execution feedback. AUTOPATCH [483] applies retrieval to performance optimization, combining historical code examples with control flow graph analysis for context-aware improvements.</p>
                        <h3>6.1.2. Self-evolving Agentic Reasoning</h3>
                        <p><strong>Agentic Feedback and Reflection.</strong> Across mathematical and code reasoning tasks, feedback operates as an external signal that highlights discrepancies, confirms correct inferences, and directs the agent toward more reliable subsequent computations. Feedback mechanisms appear prominently across mathematical discovery systems.</p>
                        <p>In program-search based discovery [30], executing candidate programs and evaluating their outputs against constraints yields counterexamples or confirmations, enabling iterative refinement of conjectures. In geometry, automated checkers validate constructions and derived relationships [29, 476], providing correctness signals that guide subsequent revisions.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Search and Retrieval Strategies</h4>
                            <div class="analysis-item">
                                <h5>Retrieval-Augmented Generation for Code</h5>
                                <p>Modern code agents combine retrieval with generation to leverage existing codebases. Key techniques include repository-level indexing, dynamic context retrieval during generation, and feedback-driven refinement of retrieved contexts.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            47: {
                title: "Memory and Collective Multi-Agent Reasoning in Math/Code",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.1.2-6.1.3 &middot; Self-Evolving & Collective</div>
                        <h1>Memory and Collective Reasoning in Math/Code</h1>
                    </div>
                    <div class="original-content">
                        <h3>Memory</h3>
                        <p>Memory provides agents with a mechanism for retaining and leveraging information from earlier reasoning steps, allowing them to maintain consistency, improve intermediate states, and improve their performance over extended problem-solving horizons. While few systems introduce an explicit memory module, many mathematical agents rely on forms of persistent state that can be viewed as implicit memory.</p>
                        <p>Interactive evaluation frameworks [486] maintain conversational and problem-state context across multiple turns, allowing models to build upon earlier partial derivations. Formal-theorem-proving agents [473] operate over evolving proof states in Lean, which accumulate tactics, subgoals, and intermediate lemmas, functioning as structured persistent information.</p>
                        <p>In code agents, memory increasingly takes the form of explicit structures that maintain coherence over long-horizon generation. Several systems construct shared or structured workspaces: Self-Collaboration [492] introduces a blackboard memory for storing task descriptions, intermediate drafts, and revision records, enabling agents to coordinate through a common representation.</p>
                        <h3>6.1.3. Collective Multi-agent Reasoning</h3>
                        <p>To address the growing complexity of tasks in mathematical discovery and code generation, recent systems increasingly rely on multi-agent or modular designs that decompose problems into cooperating specialized components. Mathematical discovery frameworks often organize reasoning into explicitly defined multi-agent or multi-component workflows that collaborate to explore and validate mathematical ideas.</p>
                        <p>Multi-agent systems for code generation have progressed from simple role-based pipelines to adaptive, collaborative frameworks capable of handling long-horizon software development. Early approaches such as Self-Collaboration [492] and AgentCoder [495] decompose tasks into sequential roles, while hierarchical designs like PairCoder [496] and FlowGen [497] introduce an architecture in which high-level agents handle planning and lower-level agents carry out concrete implementation.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Multi-Agent Code Generation</h4>
                            <div class="analysis-item">
                                <h5>Evolution of Approaches</h5>
                                <ul>
                                    <li><strong>Role-Based:</strong> Fixed roles (planner, coder, tester)</li>
                                    <li><strong>Hierarchical:</strong> High-level planning + low-level execution</li>
                                    <li><strong>Adaptive:</strong> Dynamic role assignment based on task complexity</li>
                                    <li><strong>Collaborative:</strong> Shared memory and iterative refinement</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            48: {
                title: "Scientific Discovery Agents",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.2 &middot; Scientific Discovery</div>
                        <h1>Scientific Discovery Agents</h1>
                    </div>
                    <div class="original-content">
                        <p>Scientific-discovery agents aim to accelerate the entire life cycle for scientific research, from hypothesis generation through experimental execution, by coupling LLMs with domain-specific simulators, laboratory automation and up-to-date literature. These systems ground decision in verifiable processes while handling heterogeneous data, safety constraints and long-horizon goals.</p>
                        <p>In this subsection, we begin with the foundational layer (Section 6.2.1), which encompasses planning under scientific context, tool-augmented interaction with scientific resources, search and retrieval mechanisms including RAG-based systems and execution-time integration with laboratory hardware. Building upon these capabilities, the self-evolving layer (Section 6.2.2) introduces agentic memory, feedback and reflection, which enable scientific agents to refine hypotheses, adapt protocols and learn from experimental outcomes.</p>
                        <p>Finally, the collective layer (Section 6.2.3) explores multi-agent collaboration, where agents coordinate roles, share intermediate knowledge and jointly reason toward complex scientific goals.</p>
                        <h3>6.2.1. Foundational Agentic Reasoning</h3>
                        <p><strong>Planning.</strong> Scientific agents utilize reasoning-enhanced planning ability to decompose a research goal into steps, decides which tool or simulator to call next, then revises the plan as evidence arrives. In short, the <em>chain of thought</em> emerges from LLM reasoning that compiles instructions into rigorous executable plans.</p>
                        <p>For example, ProAgent [507] materializes a planner agent that utilizes LLM reasoning capability to formulate a concrete plan for protein analysis and keep modifying it with feedback from another critic agent, and Eunomia [508] uses ReAct-style workflow to make in-context reasoning: after retrieving a top-k evidence set, the backbone LLM quote a warranting sentence, and that citation drives the next action choice.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Scientific Agent Capabilities</h4>
                            <div class="analysis-item">
                                <h5>Core Functions</h5>
                                <ul>
                                    <li><strong>Hypothesis Generation:</strong> Proposing testable scientific ideas</li>
                                    <li><strong>Experimental Design:</strong> Planning verification procedures</li>
                                    <li><strong>Literature Integration:</strong> Grounding in existing knowledge</li>
                                    <li><strong>Lab Automation:</strong> Interfacing with physical equipment</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            49: {
                title: "Tool-Use and Search in Scientific Discovery",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.2.1 &middot; Foundational (continued)</div>
                        <h1>Tool-Use and Search in Scientific Discovery</h1>
                    </div>
                    <div class="original-content">
                        <h3>Tool-Use</h3>
                        <p>Tool use is an important part of the reasoning loop for scientific agents nowadays. Specifically, rather than following rigid rules, these agents can decide which tool and when to call, how to fill parameters and verify or revise based on evidence. For example, SciAgent [510] formalizes <em>tool-augmented reasoning</em> as a four-step procedure: planning, retrieval, tool-based action and execution. Agents are trained to decide when to call a tool, which one, and how to integrate it into solving scientific tasks.</p>
                        <p>Through domain-specific tools, ChemCrow [33] chains various expert chemistry tools so intermediate calculations become premises in the next reasoning step, which enables end-to-end planning and autonomous syntheses. CACTUS [511] similarly grounds explanations in cheminformatics outputs, reducing reliance on free-form reasoning by language models alone.</p>
                        <p>Other notable examples include ChemToolAgent [512] and CheMatAgent [513]. In particular, ChemToolAgent [512] employs a ReAct-like architecture with multiple specialized chemistry tools, allowing the LLM to choose and parameterize tool calls while CheMatAgent [513] pushes further by learning tool use: it integrates over 100 chemistry/materials tools, curates a tool-specific benchmark, and uses Monte Carlo Tree Search with step-level fine-tuning to learn both which tool to pick and how to fill arguments.</p>
                        <p>For biomedical agents, TxAgent [514] scales therapeutic reasoning across 211 vetted tools and it carries out multi-step reasoning that reconciles drug labels, interactions, and patient context—turning clinical justification into an executable trace. On the other hand, AgentMD [515] builds a two-stage tool memory: it first mines thousands of clinical calculators from literature (i.e. making tools), then selects and applies the right ones at inference (i.e. using tools), pinning predictions to concrete computations.</p>
                        <h3>Search and Retrieval</h3>
                        <p>Beyond simple context stuffing, recent scientific agentic systems elevate retrieval into a deliberate reasoning step: agents decide when and what to fetch, and how to use the evidence before committing to a hypothesis. With retrieval ability, BioDiscoveryAgent [527] pulls literature and interim assay results inside a closed loop so the model's next gene-perturbation choices are conditioned on what was read and measured.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Scientific Tool Integration</h4>
                            <div class="analysis-item">
                                <h5>Domain-Specific Tools</h5>
                                <ul>
                                    <li><strong>Chemistry:</strong> Synthesis planners, property predictors</li>
                                    <li><strong>Biology:</strong> Protein analysis, gene expression tools</li>
                                    <li><strong>Medicine:</strong> Clinical calculators, drug interaction checkers</li>
                                    <li><strong>Materials:</strong> Structure analyzers, property simulators</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            50: {
                title: "Self-Evolving Scientific Agents",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.2.2 &middot; Self-Evolving</div>
                        <h1>Self-Evolving Agentic Reasoning in Scientific Discovery</h1>
                    </div>
                    <div class="original-content">
                        <p>Scientific discovery agents can go beyond static reasoning and acquire the ability to self-evolve, which is to learn from experience, refine their internal representations and improve decision quality over successive interactions. This self-evolving layer equips agents with mechanisms to monitor and revise their own reasoning, retain and reuse intermediate hypotheses and adjust future plans based on external feedback or environmental signals.</p>
                        <h3>Memory</h3>
                        <p>ChemAgent [532] implements a self-updating library. It decomposes chemistry problems into sub-tasks and writes reusable <em>skills</em> (ex: procedures, patterns, solutions) that later prompts can retrieve and adapt, stabilizing long multi-step reasoning without re-deriving everything from scratch. On the other hand, MatAgent [533] emphasizes interpretable generation for inorganic materials, where <em>short-term memory</em> recalls recent compositions and feedback, <em>long-term memory</em> preserves successful designs together with their reasoning traces, and both are reused across iterations to guide proposal refinement and enable transparent audit.</p>
                        <h3>Agentic Feedback and Reflection</h3>
                        <p><em>Firstly, Scientific Generative Agent</em> [525] ties discrete LLM proposals to inner-loop simulations that optimize continuous parameters, advancing only when evidence improves. The reflection ability is driven by measurable loss reductions. Next, ChemReasoner [534] perform heuristic search over the LLM's idea space but scores and rejects candidates with quantum-chemical feedback, turning electronic-structure signals into a principled critique of linguistic hypotheses.</p>
                        <p>Complementing these physics-based signals, Curie [509] embeds rigor check directly into control flow via intra-agent checks, inter-agent gates and an experiment-knowledge module. In parallel, LLMatDesign [535] builds explicit self-reflection into materials workflows, prompting the agent to surface and repair inconsistencies before they propagate to tool calls. Moreover, NovelSeek [536] utilizes reflection as a closed loop, updating code and plans with human-interactive feedback after each round.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Self-Evolution Mechanisms</h4>
                            <div class="analysis-item">
                                <h5>Key Patterns</h5>
                                <ul>
                                    <li><strong>Skill Libraries:</strong> Reusable procedures accumulated over time</li>
                                    <li><strong>Dual Memory:</strong> Short-term (recent) + Long-term (successful patterns)</li>
                                    <li><strong>Feedback Loops:</strong> Simulation results drive hypothesis refinement</li>
                                    <li><strong>Self-Reflection:</strong> Explicit consistency checking before actions</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            51: {
                title: "Collective Multi-Agent Scientific Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.2.3 &middot; Collective</div>
                        <h1>Collective Multi-Agent Reasoning in Science</h1>
                    </div>
                    <div class="original-content">
                        <h3>6.2.3. Collective Multi-agent Reasoning</h3>
                        <p>Multi-agent frameworks for scientific discovery distribute labor across specialized LLM-driven roles, where advanced LLM reasoning not only orchestrates coordination between scientific agents but also adjudicates conflicting evidence to maintain coherence in the process.</p>
                        <p>To illustrate, we introduce some important multi-agent frameworks as follows. Firstly, ProtAgents [507] exemplifies this pattern in protein design. The framework involve agents for literature retrieval, structure analysis, physics simulation, and results analysis. Specifically, the backbone LLM directs reasoning over multi-modal outputs, choosing when to iterate or convergence-check based on feedback signals.</p>
                        <p>PaFlow [538], on the other hand, instantiates reasoning as principle-aware uncertainty reduction with a multi-agent loop in which a Planner agent relays strategy to a Hypothesis agent and a validation loop, explicitly tying multi-agent communication to hypothesis-evidence alignment. AtomAgents [523] also brings similar role specialization to alloy discovery. In particular, the agent uses LLM-guided reasoning to control over when to trigger simulations and how to evaluate multi-modal results, letting reasoning allocate computational resources and prune alloy candidates.</p>
                        <h3>6.3. Embodied Agents</h3>
                        <p>Embodied agents extend reasoning beyond text, anchoring language in robotic perception, manipulation and navigation. By embedding LLMs within robotic and simulated bodies, these embodied agents tackle real-world generalization, continual adaptation and multi-modal grounding.</p>
                        <p>In this subsection, we begin with the foundational layer (Section 6.3.1), which covers long-horizon embodied planning, tool-assisted perception, manipulation and execution. Building upon these capabilities, the self-evolving layer (Section 6.3.2) introduces agentic memory, feedback and self-reflection capabilities enabling robots to refine control policies, adapt to novel environments and improve performance through continual interaction.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Scientific Multi-Agent Patterns</h4>
                            <div class="analysis-item">
                                <h5>Role Distribution</h5>
                                <ul>
                                    <li><strong>Planner:</strong> High-level strategy and goal decomposition</li>
                                    <li><strong>Hypothesis Agent:</strong> Generates and refines scientific hypotheses</li>
                                    <li><strong>Validator:</strong> Checks evidence and confirms/rejects ideas</li>
                                    <li><strong>Specialist:</strong> Domain-specific analysis (structure, simulation)</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            52: {
                title: "Embodied Agents: Foundational Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.3.1 &middot; Embodied Agents</div>
                        <h1>Foundational Agentic Reasoning for Embodied Agents</h1>
                    </div>
                    <div class="original-content">
                        <p>Embodied agents also rely on multi-modal reasoning traces that explicitly align perception with action. For example, Embodied CoT [545] trains vision-language-action models to generate reasoning steps incorporating visual features before executing an action. Fast ECoT [546] accelerates this by caching and re-using reasoning segments across time-steps, reducing inference latency while preserving task success.</p>
                        <p>More recently, Cosmos-Reason1 [547] establishes an ontology of space, time and dynamics that lets CoT sequences encode structured physical priors. CoT-VLA [548] builds a visual chain-of-thought by predicting future image frames as intermediate sub-goals prior to action generation. Finally, Emma-X [549] integrates grounded chain-of-thought with look-ahead spatial reasoning, improving long-horizon embodied task performance.</p>
                        <h3>Tool-use</h3>
                        <p>Embodied agents can also be strengthened to interact with external tools to enhance perception and compensate for incomplete observations. GSCE [553], for example, provides a prompt-framework that binds skill APIs and constraints for safe LLM-driven drone control. MineDojo [554] links agents to inter-scale corpus and thus enabling richer affordance grounding.</p>
                        <p>On the other hand, execution module is one of the most important tool type. It translates high-level language instructions into continuous motor commands, enabling embodied agents to act reliably in physical environments. Early systems such as SayCan [136] uses language to invoke robot pick-and-place skills; while Leo [555] broaden execution to more general manipulation settings and Hi Robot [556] uses a VLM reasoner to process complex prompts and a low-level action policy executes the chosen step.</p>
                        <p>Beyond single-agent control, hybrid pipelines couple reactive reflexes with language-guided policies to support complex domains. For example, CaRo [559] incorporates an execution phase where agents carry out decomposed sub-tasks and adapt their meta-plan based on progress; COHERENT [560] embeds a robot executor module within its PEFA loop, which ensures each assigned sub-task is acted and refined appropriately.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Embodied Reasoning Patterns</h4>
                            <div class="analysis-item">
                                <h5>Key Integration Points</h5>
                                <ul>
                                    <li><strong>Visual CoT:</strong> Reasoning traces incorporating visual features</li>
                                    <li><strong>Skill APIs:</strong> Pre-defined manipulation primitives</li>
                                    <li><strong>Execution Modules:</strong> Language-to-motor translation</li>
                                    <li><strong>Hybrid Control:</strong> Reactive + deliberative reasoning</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            53: {
                title: "Embodied Agents: Self-Evolving Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.3.2 &middot; Self-Evolving Embodied</div>
                        <h1>Self-Evolving Agentic Reasoning for Embodied Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Search and Retrieval</h3>
                        <p>Embodied agents can also use search and retrieval ability to ground language in spatial structure and past experience. Early navigation systems such as L3MVN [562] use LLMs to query a semantic map and select promising frontiers as long-term goals during visual target navigation, while SayNav [563] and SayPlan [541] build 3D scene graphs and then search task-relevant subgraphs so language instructions can be translated into grounded waypoints and sub-tasks in large environments.</p>
                        <p>Long-horizon navigation works like ReMEmbR [564] maintain a structured spatio-temporal memory that can be queried to answer "where" and "when" questions about past robot experience. Additionally, RAG-style systems make retrieval a first-class part of the planning loop; Embodied-RAG [565] and EmbodiedRAG [57] treat an agent's experience and 3D scene graphs as non-parametric memories from which task-relevant episodes or subgraphs are retrieved for navigation and task planning.</p>
                        <h3>6.3.2. Self-evolving Agentic Reasoning</h3>
                        <p>Embodied agents reliably achieve long-horizon autonomy when they can self-evolve over time: monitor their own internal states, store and update task-relevant knowledge and adjust behaviors when plans deviate. In the following paragraphs, we examine how memory modules, feedback signals and agentic reflection enable embodied agents to turn planning from a one-shot process into a continually improving cycle of behavior.</p>
                        <h3>Memory</h3>
                        <p>Effective memory mechanisms enable agents to reuse past experiences and maintain coherent task execution over extended interactions. Many systems cache recent observations in episodic buffers while summarizing long-term semantics in structured graphs, as in household planning [568] and long-horizon agents with hybrid multi-modal memory [507]. Skills and routines can be shared across tasks via indexed memory stores.</p>
                        <p>For example, HELPER-X [569] indexes discovered skills and action scripts, while at future dialogue and can be shared across domains. Spatial navigation methods such as BrainNav [570] maintain biologically inspired dual-map memories linked by a hippocampal hub to reduce hallucinations and drift.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Embodied Memory Systems</h4>
                            <div class="analysis-item">
                                <h5>Memory Types</h5>
                                <ul>
                                    <li><strong>Episodic:</strong> Recent observations and experiences</li>
                                    <li><strong>Semantic:</strong> Long-term knowledge graphs</li>
                                    <li><strong>Spatial:</strong> Maps and navigation history</li>
                                    <li><strong>Skill Library:</strong> Reusable action sequences</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            54: {
                title: "Embodied Agents: Collective Reasoning & Healthcare Intro",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.3.3 & 6.4 &middot; Collective & Healthcare</div>
                        <h1>Collective Embodied Reasoning & Healthcare Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Agentic Feedback and Reflection</h3>
                        <p>Dialogue-based critique, calibrated uncertainty and environment-aware reward shaping refine policies beyond binary success signals. DynamiCare [585] updates multi-agent treatment strategies when newly observed patient state contradicts prior plans. DoctorAgent-RL [584] optimizes questioning policies from consultation rewards; and MedAgentGym [593] enforces correctness by executing and grading generated code. Tool-use pipelines also propagate execution feedback.</p>
                        <p>Robust reflection mechanisms help agents anticipate failures by monitoring their own reasoning and actions and then adjusting plans. Optimus-1 [307] couples a <em>Knowledge-guided Planner</em> with an <em>Experience-Driven Reflector</em> to revise decisions using stored experience, while another recent study [575] defines structured agentic workflows (including self-Reflection, multi-Agent reflection and LLM Ensemble) that enable robots to reflect on and refine LLM-generated object-centered plans, thus reducing reasoning errors.</p>
                        <h3>6.3.3. Collective Multi-agent Reasoning</h3>
                        <p>Multi-agent collaboration enables embodied systems to divide labor and coordinate complex tasks more efficiently, with language often serving as the primary medium for negotiation and role allocation. For instance, SMART-LLM [577] decomposes high-level instructions and allocates sub-tasks across multiple robots, while CaPo [559] optimizes cooperative plans to avoid redundant exploration.</p>
                        <p>For multi-modal frameworks, EMAC+ [576] integrate vision and language modules and continuously refine plans via visual feedback, COMBO [578] integrates vision and language modules and continuously refine plans via visual feedback, and VIKI-R [552] demonstrates reinforcement learning as a scalable coordination mechanism among embodied agents.</p>
                        <h3>6.4. Healthcare & Medicine Agents</h3>
                        <p>Healthcare and medical agents seek to support the full clinical decision pipeline, from initial symptom triage to treatment planning and integrating LLMs with structured patient records, medical ontologies and expert guidelines. Unlike general assistants, these systems must operate under strict safety constraints, multi-modal evidence and legal justification.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Healthcare Agent Requirements</h4>
                            <div class="analysis-item">
                                <h5>Critical Considerations</h5>
                                <ul>
                                    <li><strong>Safety:</strong> Must avoid harmful recommendations</li>
                                    <li><strong>Evidence:</strong> Decisions grounded in medical knowledge</li>
                                    <li><strong>Explainability:</strong> Reasoning must be transparent</li>
                                    <li><strong>Compliance:</strong> Adherence to medical guidelines</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            55: {
                title: "Healthcare Agents: Foundational Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.4.1 &middot; Healthcare Foundational</div>
                        <h1>Foundational Agentic Reasoning in Healthcare</h1>
                    </div>
                    <div class="original-content">
                        <h3>6.4.1. Foundational Agentic Reasoning</h3>
                        <p><strong>Planning.</strong> Planning is a core capability for healthcare agents, which enables them to structure long-horizon clinical pathways into diagnostic and treatment phases, refine workflows dynamically as patient conditions evolve and coordinate across teams and tools toward cohesive care delivery. We discuss several various recent advancements as follows.</p>
                        <p>For instance, a recent agentic clinical system [580] orchestrates specialized tools and guideline citations to support ontology decision-making. EHRAgent [581] decomposes multi-table EHR inference into code-execution steps with feedback learning and PathFinder [365] presents a multi-agent, multi-modal histopathology workflow for diagnostic reasoning.</p>
                        <p>Other frameworks model planning as an explicit orchestration layer across levels of abstraction. For example, MedAgent-Pro [368] proposes a hierarchical workflow which first generates disease-level diagnostic plans from guideline criteria and then dispatches tool-agent modules for execution. MedOrch [582] treats tool invocation itself as a planning primitive across modalities, orchestrating reasoning agents for multi-step diagnostic execution. On the other hand, ClinicalAgent [583] coordinates multi-agent workflows for clinical planning, leveraging LLM reasoning to allocate tools and synthesize evidence.</p>
                        <p>In addition, planning in healthcare agents is increasingly adaptive, responding to new information and evolving contexts. For example, DoctorAgent-RL [584] models clinical consultation as a dynamic decision-making process under uncertainty, optimizing questioning strategies and diagnostic paths via reinforcement learning; while DynamiCare [585] adjusts specialist-agent teams across multi-round interactions as new patient information emerges.</p>
                        <h3>Tool-use</h3>
                        <p>Tool integration significantly expands a healthcare agent's action space, enabling precise calculations, medical image interpretation and access to specialized databases. Recent studies are summarized as follows.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Healthcare Planning Patterns</h4>
                            <div class="analysis-item">
                                <h5>Planning Approaches</h5>
                                <ul>
                                    <li><strong>Hierarchical:</strong> Disease-level to tool-level decomposition</li>
                                    <li><strong>Adaptive:</strong> Dynamic adjustment based on new information</li>
                                    <li><strong>Guideline-Driven:</strong> Plans anchored in clinical guidelines</li>
                                    <li><strong>Multi-Agent:</strong> Coordinated specialist workflows</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            56: {
                title: "Healthcare Agents: Tool-Use and Self-Evolving",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.4.1-6.4.2 &middot; Healthcare Tools & Evolution</div>
                        <h1>Tool-Use and Self-Evolving Healthcare Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Tool-use (continued)</h3>
                        <p>Several systems explicitly foreground extensibility. MedOrch [582] introduces a modular architecture that allows new diagnostic APIs to be incorporated without retraining, while TxAgent [514] integrates over two hundreds pharmacological tools to support therapeutic decision-making across drug-disease-treatment relationships. AgentMD [515] similarly curates and leverages over two thousands executable clinical calculators to learn risk-prediction pipelines.</p>
                        <p>Other approaches focus on structured function calling for safe execution. For example, LLM-based agents can reliably invoke bedside calculators when provided with explicit function signatures, ensuring arithmetic correctness in dosing and risk scoring [586]. MeNTi [587] goes further by enabling nested tool calls across multi-step medical calculators. Complementing these text-based integrations, OSCAR [617], OS-ATLAS [617] and Ufron [619] further emphasize robust cross-application planning: OS-ATLAS offers a platform-agnostic action model for consistent control, OSCAR maintains state-aware plans that adapt as execution unfolds and Ufron unifies offline and online planning within a single general-purpose GUI agent.</p>
                        <h3>6.4.2. Self-evolving Agentic Reasoning</h3>
                        <p>Self-evolving capabilities enable healthcare agents to maintain longitudinal clinical coherence. Representative use cases include accumulating relevant medical context across encounters, updating beliefs as new evidence arrives and revising decisions when inconsistencies surface.</p>
                        <h3>Memory</h3>
                        <p>Persistent memory is essential for tracking medical or patient history and maintaining context across interactions. For instance, epidemic-modeling agents [598] maintain temporal contact histories to trace infection chains over time; while MedAgentSim [595] stores experience histories and refine diagnostic strategies over time. In structured data settings, EHRAgent [581] records intermediate computations over tabular EHRs so subsequent steps can reference prior results.</p>
                        <p>EvoPatient [599] interleaves memory with coevolution maintains evolving clinical state across dialogue phases while AIPatient [594] persists longitudinal EHR-derived variables to drive consistent responses. Multi-agent systems such as MedOrch [582] contain clinical knowledge graph agent which can be considered as external memory that can be queried to retrieve known relationships or diagnostic patterns.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Healthcare Tool Integration</h4>
                            <div class="analysis-item">
                                <h5>Tool Categories</h5>
                                <ul>
                                    <li><strong>Clinical Calculators:</strong> Risk scores, dosing formulas</li>
                                    <li><strong>Diagnostic APIs:</strong> Image analysis, lab interpretation</li>
                                    <li><strong>Knowledge Graphs:</strong> Drug interactions, disease relationships</li>
                                    <li><strong>EHR Systems:</strong> Patient record access and updates</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            57: {
                title: "Healthcare Agents: Collective Reasoning & Web Agents Intro",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.4.3 & 6.5 &middot; Healthcare Collective & Web</div>
                        <h1>Collective Healthcare Reasoning & Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>6.4.3. Collective Multi-agent Reasoning</h3>
                        <p>Multi-agent collaboration is central to healthcare AI, since clinical decision-making often depends on consensus among specialists, negotiation of competing hypotheses and coordination across roles such as physicians, patients and trial designers. In the following, we discuss several strands of research centered around multi-agent capabilities.</p>
                        <p>For collaborative decision-making, notable frameworks include MDAgents [364], which automatically assigns tailored collaboration structures to teams of LLMs depending on medical task complexity, and DoctorAgent-RL [584], which uses a multi-agent reinforcement-learning framework to optimize multi-turn doctor-patient consultation dialogues. In addition, Agent-derived Multi-Specialist Consultation (AMSC) [600] explores staged multi-specialist dialogues for differential diagnosis that mimics the medical scene of a patient consulting with multiple specialists.</p>
                        <p>Other notable works include ClinicalAgent [583], which organizes clinical trial workflows via role-based agent collaboration / LLM reasoning and PathFinder [365], which integrates a diverse set of agents that can gather evidence and provide comprehensive diagnoses with natural language explanations.</p>
                        <p>On the other hand, there are studies focusing on simulation-driven collaboration. These works highlight how multi-agent setups enrich training and evaluation. MedAgentSim [595] co-evolves doctor and patient agents to simulate real-world multi-turn clinical interactions, and EvoPatient [599] uses co-evolution of patient and doctor agents to generate diagnostic dialogue data and therefore gather experience to improve the quality of both questions and answers to enable accurate human doctor training.</p>
                        <h3>6.5. Autonomous Web Exploration & Research Agents</h3>
                        <p>Web agents, GUI agents and autonomous research agents constitute three interlinked but distinct trajectories of agentic reasoning systems. Firstly, web agents specialize in navigating online resources, issuing web API calls or browser actions to retrieve dynamic evidence and steer research direction.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Healthcare Multi-Agent Patterns</h4>
                            <div class="analysis-item">
                                <h5>Collaboration Types</h5>
                                <ul>
                                    <li><strong>Specialist Consultation:</strong> Multiple expert agents confer</li>
                                    <li><strong>Doctor-Patient Simulation:</strong> Training through dialogue</li>
                                    <li><strong>Trial Coordination:</strong> Managing clinical workflows</li>
                                    <li><strong>Differential Diagnosis:</strong> Competing hypothesis evaluation</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            58: {
                title: "Web Agents: Foundational Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.1 &middot; Web Agents Foundational</div>
                        <h1>Foundational Agentic Reasoning for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <p>GUI agents go further by manipulating software interfaces and multi-modal dashboards directly (i.e. clicking, typing, navigating) to execute experiments, data workflows and interface-based tasks. Autonomous research agents sit at the top of this hierarchy, pairing LLM reasoners with scientific workflows, tool-chains and meta-loops to drive hypothesis generation, data synthesis and paper writing.</p>
                        <p>The core connection is a progression of autonomy: first web agents retrieve evidence from online resources, then GUI agents operationalize actions inside software interfaces, and finally autonomous research agents orchestrate full scientific workflows end-to-end. While web agents, GUI agents and autonomous agents share common themes of goal-directed autonomy, tool-use and iterative improvement, they differ in where they act on, how they manipulate their environment and what goal they aim to achieve.</p>
                        <h3>6.5.1. Foundational Agentic Reasoning</h3>
                        <p><strong>Planning.</strong> Planning is essential for web agents because they must decompose long-horizon tasks into manageable steps, adapt to dynamic pages and coordinate tool/invocation strategies. Early work such as WebGPT [258] fine-tuned GPT-3 [603] to answer open-ended questions via a text-based web-browser interface. Then, various web-based methods deepened the planning paradigm: for example, SEEACT [604] explored large multi-modal models as generalists that integrating visual and HTML grounding for web-based tasks, and AutoWebGLM [605] introduced HTML simplification and various learning techniques for open-domain web task decomposition and navigation.</p>
                        <p>These works paved the way for recent systems such as Agent Q [113] that integrate guided MCTS, self-critique and off-policy preference optimization on web-task benchmarks, and set the stage for even more advanced long-horizon web planners such as WebExplorer [606] and WebSailor [41].</p>
                        <p>In addition, reinforcement learning has become a core tool for improving the decision-making and planning behavior of web-based LLM agents. WebRL [437] introduces a self-evolving online curriculum that generates new tasks from unsuccessful attempts and trains an outcome-supervised reward model to guide policy optimization.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Web Agent Hierarchy</h4>
                            <div class="analysis-item">
                                <h5>Agent Progression</h5>
                                <ul>
                                    <li><strong>Web Agents:</strong> Navigate and retrieve from web resources</li>
                                    <li><strong>GUI Agents:</strong> Interact with software interfaces</li>
                                    <li><strong>Research Agents:</strong> Orchestrate full scientific workflows</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            59: {
                title: "Web Agents: Tool-Use and Planning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.1 &middot; Web Agents (continued)</div>
                        <h1>Tool-Use and Advanced Planning for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Planning (continued)</h3>
                        <p>For web agents, tool-use abilities underpins execute plans in realistic, dynamic environments. For example, WebVoyager [638] systematizes multi-modal execution by building an end-to-end agent that operates on real websites. On the interaction side, BrowserAgent [639] makes the action space more human-like, defining a compact set of browser primitives (e.g., click, scroll, type) and coupling them with an explicit memory mechanism to maintain key coordinates across steps, yielding strong gains on multi-touch QA benchmarks.</p>
                        <p>Similarly, methods like WAIT [641] and WebShaper [642] push tool use from mere execution toward tool discovery and data-centric interaction. Specifically, WAIT teaches agents to reverse-engineer reusable tools from website functionality, while WebScanner and WebShaper embed web actions inside multi-turn information-seeking and dataset-synthesis loops, respectively.</p>
                        <p>Tool use is another core capability for GUI agents, enabling them to invoke system functions and application features as structured tools. As pioneering systems, AutoDroid [643] automatically analyzes Android apps to construct functionality-aware UI abstractions that LLM agents can reason over as capabilities rather than raw layouts, while its successor AutoDroid-V2 [644] re-frames mobile UI automation as LLM-driven code generation, with an on-device small language model emitting executable scripts for a local interpreter.</p>
                        <p>MobileExperts [645] models each expert as a tool-capable specialist and uses a dual-layer controller to select which expert and its associated tool-set to invoke at different stages of a mobile workflow. AgentStore [646] pushes this idea to the platform level by treating heterogeneous agents themselves as tools: a MetaAgent uses AgentToken to route operating-system subtasks to the most suitable specialized "tool-agent" through a unified interface. OS-Copilot [613] and OSCAR [618] integrate rich system-level tools into unified computer-control frameworks, so that complex desktop tasks are expressed as sequences of tool calls.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>GUI Agent Capabilities</h4>
                            <div class="analysis-item">
                                <h5>Tool Integration Levels</h5>
                                <ul>
                                    <li><strong>Browser Primitives:</strong> Click, scroll, type actions</li>
                                    <li><strong>Tool Discovery:</strong> Learning new tools from interfaces</li>
                                    <li><strong>System Integration:</strong> OS-level function invocation</li>
                                    <li><strong>Agent-as-Tool:</strong> Routing to specialized sub-agents</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            60: {
                title: "Web Agents: Search and Retrieval",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.1 &middot; Web Agents (continued)</div>
                        <h1>Search and Retrieval for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Search and Retrieval</h3>
                        <p>Search and retrieval lie at the heart of what differentiates web agents from static language models: they must locate, synthesize and reface web-scale information in dynamic environments. WebExplorer [606] tackles this by generating challenging information-seeking trajectories and training agents to interleave a search tool and a browse tool over many turns, resulting in improved multi-step retrieval policies on complex benchmarks.</p>
                        <p>WebSailor [41] likewise focuses on information-seeking under extreme uncertainty: constructing high-uncertainty search tasks and using a two-stage post-training pipeline to instill uncertainty-reducing search strategies for long-horizon web tasks. INFOGENT [652] also performs multi-query search across diverse web sources, enabling comprehensive information retrieval beyond task completion.</p>
                        <p>For retrieval-augmented generation applications, RaDA [653] explicitly disentangles web-agent planning into Retrieval-augmented Task Decomposition and Retrieval-augmented Action Generation, so that each high-level subgoal and concrete action is conditioned on fresh search results while respecting context limits. In addition, GeAR [264] advances retrieval itself by augmenting a base retriever with graph expansion and an agent framework, enabling multi-hop passage retrieval along graph-structured evidence chains. Finally, WebMcAgent [654] exemplifies retrieval-augmented generation for web agents by retrieving past trajectories and external knowledge into a multi-modal RAG policy.</p>
                        <p>Several GUI agents use retrieval capability to inject external experience or knowledge at inference time. Synapse [655] maintains an exemplar memory of abstracted trajectories and, for each new task, retrieves similar past trajectories as in-context plans, substantially improving multi-step decision-making. Learn-Act [656] builds a three-agent pipeline that mines human demonstrations into a knowledge store and retrieves the most relevant instructions to guide mobile GUI execution on unseen and diverse tasks.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Web Search Strategies</h4>
                            <div class="analysis-item">
                                <h5>Retrieval Patterns</h5>
                                <ul>
                                    <li><strong>Multi-Turn Search:</strong> Iterative query refinement</li>
                                    <li><strong>Uncertainty-Guided:</strong> Prioritize high-value information</li>
                                    <li><strong>RAG Integration:</strong> Ground actions in retrieved context</li>
                                    <li><strong>Experience Retrieval:</strong> Learn from past trajectories</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            61: {
                title: "Web Agents: Self-Evolving Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.2 &middot; Self-Evolving Web Agents</div>
                        <h1>Self-Evolving Agentic Reasoning for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>6.5.2. Self-evolving Agentic Reasoning</h3>
                        <p>Effective self-evolving abilities enable these autonomous agents to adapt their behavior over time, retain crucial task context across interaction cycles and incrementally refine planning and execution strategies. The following paragraphs review how memory, feedback and self-reflection mechanisms support this continual improvement across these agent families, turning interaction from a one-shot pipeline into an iterative learning loop.</p>
                        <h3>Memory</h3>
                        <p>Memory modules transform brittle, single-pass web interactions into reusable experience. For example, Agent Workflow Memory (AWM) [298] induces reusable workflows from successful trajectories and retrieves them to guide future tasks, while SCaL [660] distils noisy trajectories into high-level verbal and visual abstractions that are stored as a memory of multimodal experience and later injected into prompts.</p>
                        <p>Control-oriented designs such as BrowserAgent [639] maintain explicit histories of past actions and intermediate conclusions in the agent's context, instead of only re-encoding the current page view. GLM-based agents like AutoWebGLM [605] and AgentOccam [661] emphasize compressed page representations, using HTML simplification and carefully tuned observation spaces so that the agent's prompt contains a shorter, more informative view of the state, with past steps preserved through the usual action-observation history.</p>
                        <p>Recent GUI agents adopt explicit memory modules that store and retrieve task-relevant information during long-horizon execution. Earlier work such as MobileGPT [663] equips a mobile assistant with human-like app memory: it decomposes procedures into modular sub-tasks that are explored, selected, derived, and then stored so they can be recalled and reused instead of being re-discovered from scratch.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Web Agent Memory Patterns</h4>
                            <div class="analysis-item">
                                <h5>Memory Types</h5>
                                <ul>
                                    <li><strong>Workflow Memory:</strong> Reusable task procedures</li>
                                    <li><strong>Multimodal Abstraction:</strong> Visual + verbal experience</li>
                                    <li><strong>Action History:</strong> Sequential interaction records</li>
                                    <li><strong>Compressed State:</strong> Efficient page representations</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            62: {
                title: "Web Agents: Feedback and Reflection",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.2 &middot; Self-Evolving (continued)</div>
                        <h1>Agentic Feedback and Reflection for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>Long-term Memory for Research Agents</h3>
                        <p>Long-term memory is crucial for autonomous research agents because it enables accumulation and reuse of prior knowledge, fostering continuity across research cycles. For example, Agent Laboratory [654] retains prior experiment code, results, and interpretation across its multi-phase workflow, enabling later stages to build on earlier work. GPT Researcher [635] generates reports with embedded citations and provides context for planning and extension of research topics.</p>
                        <p>Chain of Ideas [636] structures relevant literature into a chain scaffold that reflects a field's progression and can be revisited as new evidence arises. The AI Scientist-v2 [530] incorporates a progressive agentic tree-search approach that enables branching, backtracking and follow-up experimentation across iterations.</p>
                        <h3>Agentic Feedback and Reflection</h3>
                        <p>Modern web agents treat interaction as a continual learning process, using feedback signals and reflection modules to refine their reasoning and recover from failures over time. Agent Q [113] combines guided Monte Carlo tree search with a self-critique stage, so that rollouts provide not only action sequences but also preference-style supervision. ReAP [668] makes reflection explicit by treating it as a retrieval problem: it stores task-reflection key-value pairs summarizing what was learned from past trajectories, then, at inference time, retrieves the most relevant reflections and appends them to the agent's prompt to guide planning on new web-navigation tasks.</p>
                        <p>Agent-E [669] introduces an automatic validation pipeline that detects execution errors across text and vision, and then triggers self-refinement, enabling agents to iteratively correct their own workflows. Recon-Act [670] uses a dual-team architecture in which a Reconnaissance team extracts generalized tools from successful and failed trajectories, and an Action team applies these tools to re-plan tasks, forming a closed feedback loop.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Reflection Mechanisms</h4>
                            <div class="analysis-item">
                                <h5>Feedback Patterns</h5>
                                <ul>
                                    <li><strong>Self-Critique:</strong> Monte Carlo tree search with evaluation</li>
                                    <li><strong>Reflection Retrieval:</strong> Past lessons applied to new tasks</li>
                                    <li><strong>Error Detection:</strong> Automatic validation and correction</li>
                                    <li><strong>Dual-Team Learning:</strong> Tool extraction + application</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            63: {
                title: "Web Agents: Collective Multi-Agent Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 6.5.3 &middot; Collective Web Agents</div>
                        <h1>Collective Multi-Agent Reasoning for Web Agents</h1>
                    </div>
                    <div class="original-content">
                        <h3>GUI Agents Reflection</h3>
                        <p>GUI agents also integrate explicit reflection so they can critique and repair their own plans. Early computer-control systems with structured reflection, for example, a zero-shot desktop control agent with structured self-reflection loops [673], provides conceptual templates that later GUI agents adapt to visual, multi-application settings. GUI-Reflection [674] instantiates this idea end-to-end: it builds a reflection-oriented task suite, automatically synthesizes error scenarios from existing successful trajectories, and adds an online reflection-tuning stage so multi-modal GUI models learn to detect failures, reason about causes, and generate corrective actions without human annotation.</p>
                        <p>History-Aware Reasoning (HAR) [675] treats long-horizon GUI automation as a reflective learning problem, constructing reflective learning scenarios, synthesizing tailored correction guidelines, and designing a hybrid RL reward so the agent acquires episodic reasoning knowledge from its own errors and shifts from history-agnostic to history-aware reasoning.</p>
                        <h3>6.5.3. Collective Multi-agent Reasoning</h3>
                        <p>Collective multi-agent reasoning for web agents reframes browser use as cooperation among specialized roles rather than a single monolithic policy. WebPilot [679] models web task execution as a multi-agent system with a global planning agent that decomposes tasks and local MCTS-based executors that solve subtasks, jointly steering search in complex web environments.</p>
                        <p>INFOGENT [652] organizes web information aggregation into a Navigator, Extractor, and Aggregator, so exploration, evidence extraction, and synthesis are handled by distinct cooperating agents with feedback from the Aggregator to guide future navigation.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Multi-Agent Web Patterns</h4>
                            <div class="analysis-item">
                                <h5>Role Specialization</h5>
                                <ul>
                                    <li><strong>Planner:</strong> Global task decomposition</li>
                                    <li><strong>Navigator:</strong> Page exploration and traversal</li>
                                    <li><strong>Extractor:</strong> Information retrieval from pages</li>
                                    <li><strong>Aggregator:</strong> Evidence synthesis and feedback</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            64: {
                title: "Benchmarks Introduction",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7 &middot; Benchmarks</div>
                        <h1>Benchmarks for Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <p>To facilitate autonomous research agents, multi-agent collaboration enables a single model's linear workflow to become a coordinated research group: specialized agents operate in parallel, exchange intermediate artifacts through explicit interfaces, and provide adversarial or complementary feedback to improve both creativity and rigor. For example, AgentRxiv [685] coordinates author, reviewer, and editor agents that iteratively refine manuscripts and share evolving artifacts across virtual "labs." ARIA [529] instantiates a role-structured multi-LLM team that searches, filters, and synthesizes scientific literature into actionable experimental procedures.</p>
                        <p>Earlier multi-agent designs such as CAMEL [531] demonstrate how cooperative role-play with tool access can enhance hypothesis generation and task decomposition. In experimental sciences, Coscientist [686] integrates planning, robotic instrument control, and analysis into a multi-agent closed loop that autonomously designs and executes wet-lab experiments. Finally, TAIS [539] defines a hierarchical team, namely project manager, data engineer and domain expert, that jointly discovers disease-predictive genes from expression data through coordinated division of labor.</p>
                        <h3>7. Benchmarks</h3>
                        <p>Agentic reasoning has been evaluated through a rapidly growing set of benchmarks, but existing suites often differ in what they test as the core capability, such as tool invocation accuracy, memory retention under long contexts, or coordination quality in multi-agent settings. To provide a coherent view, we organize benchmarks from two complementary perspectives.</p>
                        <p>We first summarize benchmarks that isolate core mechanisms of agentic reasoning, which helps pinpoint where systems succeed or fail at the capability level. We then review application-level benchmarks that evaluate end-to-end agent behavior in realistic domains, capturing the combined effects of perception, planning, tool use, memory, and coordination.</p>
                        <h3>7.1. Core Mechanisms of Agentic Reasoning</h3>
                        <p>We begin with benchmarks that target mechanism-level capabilities, aiming to evaluate agentic reasoning in a more controlled and interpretable manner. Concretely, these benchmarks decompose agentic behavior into a small set of recurring primitives, including tool use, search, memory and planning, and multi-agent coordination.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Benchmark Organization</h4>
                            <div class="analysis-item">
                                <h5>Two Perspectives</h5>
                                <ul>
                                    <li><strong>Mechanism-Level:</strong> Isolates specific capabilities (tool use, memory, planning)</li>
                                    <li><strong>Application-Level:</strong> Evaluates end-to-end performance in realistic domains</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            65: {
                title: "Tool Use Benchmarks (Figure 12)",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.1 &middot; Tool Use</div>
                        <h1>Tool Use Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <div class="figure-box">
                            <h4>Figure 12: Overview of Benchmarks on Agentic Reasoning</h4>
                            <p>The figure categorizes benchmarks into two main groups:</p>
                            <ul>
                                <li><strong>Ability-centric Benchmarks (Section 7.1):</strong> Tool Use, Search, Memory and Planning, Multi-Agent, Language-Anchored</li>
                                <li><strong>Application-centric Benchmarks (Section 7.2):</strong> Embodied Agents, Scientific Discovery, Autonomous Research, Medical/Clinical, Web Agents, General Purpose</li>
                            </ul>
                        </div>
                        <h3>7.1.1. Tool Use</h3>
                        <p>Evaluating tool-using models remains an open challenge due to the diversity of tasks, tools, and usage scenarios involved [687]. The key difficulties arise from the wide range of available tools, varying levels of scenario complexity, and the prevalence requirements specifically for the task domain.</p>
                        <p><strong>Single-Turn Tool Use.</strong> While agentic reasoning often focuses on multi-turn or long-horizon interactions, single-turn tool use remains a foundational capability for evaluating LLMs' basic tool invocation skills. ToolQA [688] constructs a dataset of 1,530 dialogues involving 13 specialized tools, designed to assess LLMs' ability to interface with external knowledge sources in a question-answering context.</p>
                        <p>APIBench [78] introduces a large-scale benchmark grounded in real-world APIs from HuggingFace, TorchHub, and TensorHub, comprising 1,645 unique APIs and 16,450 instruction-API pairs. It is used to train and evaluate Gorilla, an LLM capable of invoking a broad range of APIs, emphasizing generalization across diverse tool interfaces.</p>
                        <p><strong>Multi-Turn Tool Use.</strong> Multi-turn tool use offers a more realistic simulation of real-world applications, where agents autonomously select and sequence tools to solve complex tasks. ToolAlpaca [204] is one of the earliest efforts in this direction, using multi-agent simulation to generate 3,938 tool-use instances from over 400 real-world APIs across 50 distinct categories.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tool Use Evaluation</h4>
                            <div class="analysis-item">
                                <h5>Benchmark Categories</h5>
                                <ul>
                                    <li><strong>Single-Turn:</strong> Basic tool invocation skills</li>
                                    <li><strong>Multi-Turn:</strong> Sequential tool selection and composition</li>
                                    <li><strong>Real-World APIs:</strong> Generalization across diverse interfaces</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            66: {
                title: "Search Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.2 &middot; Search</div>
                        <h1>Search Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>7.1.2. Search</h3>
                        <p>To systematically assess an agent's ability to acquire information through interaction, recent benchmarks cast search as a sequential reasoning problem and can be broadly categorized into unimodal and multimodal settings, differing in the nature of evidence sources, interaction spaces, and grounding requirements.</p>
                        <h3>Unimodal Search</h3>
                        <p>Recent benchmarks for single-modal agentic search increasingly frame information seeking as a sequential, decision-driven process, emphasizing planning, interaction, and evidence synthesis. For example, WebWalker [697] emphasizes structured website traversal, explicitly modeling search as coordinated horizontal exploration and vertical drilling across interconnected pages.</p>
                        <p>To reflect realistic open-world information seeking, InfoDeepSeek [698] introduces a dynamic Web setting with verifiable yet non-curated answers, highlighting robustness to noise and distributional shift. Several benchmarks scale along temporal and informational dimensions: Mind2Web 2 [50] focuses on long-horizon browsing and citation-grounded synthesis, whereas RAVine [699] augments answer quality with process-level efficiency and interaction fidelity.</p>
                        <p>Complementarily, WideSearch [700] and DeepWideSearch [701] distinguish between breadth-oriented large-scale fact aggregation and depth-oriented multi-hop reasoning, revealing the difficulty of jointly optimizing coverage and reasoning coherence.</p>
                        <h3>Multimodal Search</h3>
                        <p>Recent benchmarks on multimodal agentic search move beyond static multimodal question answering to systematically evaluate an agent's ability to actively retrieve, browse, and reason over heterogeneous information sources under realistic constraints. Benchmarks such as MMSearch [705] and its extension MMSearch-Plus [706] frame multimodal search as an end-to-end process, where agents must interpret multimodal queries and synthesize answers by jointly leveraging textual and visual evidence, explicitly modeling different input-output modality configurations.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Search Benchmark Dimensions</h4>
                            <div class="analysis-item">
                                <h5>Key Distinctions</h5>
                                <ul>
                                    <li><strong>Unimodal vs Multimodal:</strong> Text-only vs. text+vision</li>
                                    <li><strong>Breadth vs Depth:</strong> Wide fact gathering vs. multi-hop reasoning</li>
                                    <li><strong>Static vs Dynamic:</strong> Fixed corpora vs. real-world web</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            67: {
                title: "Memory and Planning Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.3 &middot; Memory and Planning</div>
                        <h1>Memory and Planning Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>7.1.3. Memory and Planning</h3>
                        <p>A distinctive advantage of agents lies in their ability to leverage memory to achieve accurate long-term performance and strong reasoning capabilities. This ability can be assessed from two complementary perspectives. The first concerns memory management, which reflects how effectively an agent integrates, organizes, and retrieves long-term memories. The second concerns memory utilization, which captures how well an agent exploits historical information to support planning and informed feedback.</p>
                        <h3>Long-Horizon Episodic Memory</h3>
                        <p>This category targets single-episode tasks with partial observability and delayed rewards, requiring agents to store and retrieve information over extended time spans. Benchmarks in this space evaluate memory retention, retrieval, and reasoning across long contexts. PertLTQA [712] simulates personalized dialogue, where agents answer questions using long-term personas and event memories. It includes 8.5K QA pairs and evaluates memory classification, retrieval ranking, and synthesis fidelity.</p>
                        <p>ELITEBench [713] tests QA on noisy meeting transcripts, where relevant evidence may appear far earlier than the query. Models are scored via GPT-4 across various ASR noise levels and dialogue settings. In the meanwhile, Multi-IF [714] and MultiChallenge [715] focus on multi-turn instruction following. Multi-IF [714] spans 4.5K tri-turn conversations in 8 languages, with evaluation based on strict and relaxed instruction accuracy. MultiChallenge [715] tests four memory-intensive phenomena: retention, inference, editing, and coherence, using 273 curated dialogues with binary pass/fail evaluation.</p>
                        <h3>Multi-session Recall</h3>
                        <p>Multi-session Recall focuses on multi-episode tasks where agents must retain and integrate knowledge across separate sessions, supporting lifelong adaptation and mitigating catastrophic forgetting. A range of recent benchmarks systematically probe this capability under realistic, long-term interaction scenarios.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Benchmark Types</h4>
                            <div class="analysis-item">
                                <h5>Evaluation Dimensions</h5>
                                <ul>
                                    <li><strong>Long-Horizon Episodic:</strong> Single-episode retention over time</li>
                                    <li><strong>Multi-Session Recall:</strong> Cross-session knowledge integration</li>
                                    <li><strong>Memory Management:</strong> Organization and retrieval efficiency</li>
                                    <li><strong>Memory Utilization:</strong> Using history for planning</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            68: {
                title: "Planning and Feedback Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.3 &middot; Planning (continued)</div>
                        <h1>Planning and Feedback Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>Planning and Feedback</h3>
                        <p>Benchmarks targeting planning and feedback primarily assess whether agents can effectively utilize memory to support multi-step planning based on environmental feedback, and maintain coherent internal state over extended interactions. First, ALFWorld [48] employs interactive environments to evaluate the consistency of multi-step planning, requiring agents to accumulate observations across actions and maintain latent internal states throughout execution.</p>
                        <p>Moreover, formal planning benchmarks such as PlanBench [723] and ACPBench [724] assess planning capabilities in explicitly defined dynamic environments, testing whether agents can correctly reason about action preconditions, effects, reachability, and overall plan validity. TEXT2WORLD [725] integrates fragmented textual descriptions into a coherent and executable world model, evaluating the capacity to continuously consolidate historical facts into structured planning representations.</p>
                        <p>More recent benchmarks place greater emphasis on feedback integration and planning under non-stationary conditions. For example, REALM-Bench [726] introduces dynamic disturbances in real-world manufacturing scenarios, requiring agents to remember prior commitments and replan when underlying assumptions are violated, while TravelPlanner [727] focuses on accurate itinerary construction under constrained and evolving information.</p>
                        <h3>7.1.4. Multi-Agent System</h3>
                        <p>To evaluate coordination, competition, and decision making beyond isolated reasoning, recent benchmarks situate multi-agent systems in interactive environments. These works broadly span game-based evaluations, simulation-centric real-world scenarios, and language-driven social reasoning tasks.</p>
                        <h3>Game-based Reinforcement Learning Evaluation</h3>
                        <p>Game-based reinforcement learning evaluation benchmarks leverage classical and novel gaming environments to systematically compare the performance of multi-agent RL algorithms under cooperative and adversarial settings. MAgent [730] facilitates massive-scale multi-agent scenarios such as pursuit and resource competition within customizable grid-worlds, evaluating individual cumulative rewards and competitive metrics like resource occupancy rates.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Planning Benchmark Features</h4>
                            <div class="analysis-item">
                                <h5>Key Challenges</h5>
                                <ul>
                                    <li><strong>Multi-Step Consistency:</strong> Maintaining coherent plans</li>
                                    <li><strong>Dynamic Environments:</strong> Adapting to changes</li>
                                    <li><strong>Feedback Integration:</strong> Learning from execution results</li>
                                    <li><strong>Constraint Satisfaction:</strong> Meeting complex requirements</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            69: {
                title: "Multi-Agent System Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.4 &middot; Multi-Agent</div>
                        <h1>Multi-Agent System Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>Game-based Evaluation (continued)</h3>
                        <p>Pommerman [731] adapts the classic Bomberman game for cooperative and adversarial interactions, quantifying performance through win rates, survival duration, and kill-to-suicide ratio. SMAC [732] centers on decentralized micro-management challenges in StarCraft II scenarios, evaluating team success via win rates, average damage output, and formation dispersion.</p>
                        <p>MineLand [733] utilizes Minecraft as a realistic ecological simulation for large-scale multi-agent coordination, with up to 64 agents cooperating to meet physical needs under partial observability. TeamCraft [734] also employs Minecraft to benchmark embodied multi-modal agents tasked with interpreting visual, textual, and environmental prompts to collaboratively achieve 55,000 procedurally generated task instances.</p>
                        <p>Melting Pot [735] assesses agents' zero-shot generalization capabilities in diverse social dilemma environments, utilizing metrics such as per-capita return, social welfare, and inequality indices. BenchMARL [736] provides standardized algorithm comparisons across multiple scenarios (e.g., SMACv2, VMAS, MRL), measuring convergence rates, final performance, and hyperparameter sensitivity. Finally, Arena [737] encompasses a comprehensive suite of cooperative and adversarial games across various complexities, evaluating individual returns, collective social welfare, and emergent communication protocols.</p>
                        <h3>Simulation-centric Real-world Assessment</h3>
                        <p>Simulation-centric real-world benchmarks simulate realistic or pseudo-realistic environments, emphasizing scalability, partial observability, and dynamic planning. SMARTS [738] offers a scalable multi-agent driving platform for real-world traffic scenarios like merges and intersections, with evaluation based on collision rates, task completion, and agent behavior distributions.</p>
                        <p>Nocturne [739] provides high-throughput, partially observable driving simulations using Waymo trajectories, testing coordination and human-like behavior in tasks such as intersections and roundabouts. MARBL [740] benchmarks multi-echelon inventory management, simulating cooperative and competitive retail dynamics, evaluated via profit metrics across diverse inventory settings.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Multi-Agent Benchmark Types</h4>
                            <div class="analysis-item">
                                <h5>Evaluation Environments</h5>
                                <ul>
                                    <li><strong>Game-Based:</strong> StarCraft, Minecraft, Bomberman</li>
                                    <li><strong>Simulation:</strong> Driving, inventory management</li>
                                    <li><strong>Social Dilemmas:</strong> Cooperation vs competition</li>
                                    <li><strong>Partial Observability:</strong> Incomplete information scenarios</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            70: {
                title: "Language and Application Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.1.4-7.2 &middot; Language & Applications</div>
                        <h1>Language Communication and Application Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>Language, Communication, and Social Reasoning</h3>
                        <p>Benchmarks in Language, Communication, and Social Reasoning explore multi-agent communication protocols, Theory-of-Mind reasoning, game-theoretic interactions, and language-driven coordination. LLM-Coordination [744] examines collaborative reasoning and joint-planning abilities of LLM agents through cooperative gameplay (e.g., Hanabi, Overcooked-AI), measured by holistic scores and fine-grained coordination question accuracy.</p>
                        <p>AVALONBENCH [745] leverages the social deduction game Avalon to assess role-conditioned language-based reasoning, with datasets of thousands of five-player dialogues and metrics on win-rate, role accuracy, and voting dynamics. Welfare Diplomacy [746] extends the classic game Diplomacy to general-sum welfare negotiation, using 50-game datasets to quantify coalition stability and welfare-oriented strategic reasoning.</p>
                        <p>MAgIC [747] covers social deduction and classic dilemmas (e.g., Chameleon, Prisoner's Dilemma), employing handcrafted scenario datasets to benchmark reasoning, deception, coordination, and rationality. BattleAgentBench [19] assesses language-based cooperative and competitive dynamics in strategic gameplay environments, scoring navigation accuracy, agent interactions, and exploitability across diverse map datasets.</p>
                        <h3>7.2. Applications of Agentic Reasoning</h3>
                        <p>While mechanism-centric benchmarks help isolate individual capabilities, real-world deployments require these capabilities to work together under realistic constraints, such as partial observability, long-horizon dependencies, and safety-critical decisions. We therefore next review application-level benchmarks that evaluate end-to-end agent performance across representative environments, with tasks that jointly stress perception, reasoning, action execution, and coordination.</p>
                        <h3>7.2.1. Embodied Agents</h3>
                        <p>Benchmarks under this category evaluate agents that interact with physical or simulated environments, requiring grounding, perception, and action planning. AgentX [750] provides a diverse suite of vision-language embodied tasks in driving and sports, where agents must make decisions using multimedia information from videos.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Application-Level Benchmarks</h4>
                            <div class="analysis-item">
                                <h5>Key Requirements</h5>
                                <ul>
                                    <li><strong>End-to-End:</strong> Full pipeline evaluation</li>
                                    <li><strong>Realistic Constraints:</strong> Partial observability, safety</li>
                                    <li><strong>Multi-Capability:</strong> Perception + reasoning + action</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            71: {
                title: "Scientific Discovery and Research Agent Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.2.2-7.2.3 &middot; Scientific & Research</div>
                        <h1>Scientific Discovery and Autonomous Research Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>7.2.2. Scientific Discovery Agents</h3>
                        <p>Scientific benchmarks aim to test agents' capabilities in knowledge acquisition, hypothesis generation, and experimental automation. DISCOVERYWORLD [757] introduces a virtual lab where agents explore scientific phenomena in biology, chemistry, and physics through simulated tools and instruments. ScienceWorld [758] focuses on elementary science experiments using textual instructions and environment interaction, requiring step-by-step hypothesis testing.</p>
                        <p>ScienceAgentBench [759] builds a benchmark from real-world scientific papers, translating tasks like code implementation, figure generation, and variable extraction into executable subtasks, assessing agents' ability to automate the research process. The AI Scientist [651] simulates a full end-to-end research pipeline, where agents perform literature review, method writing, experiment execution, and peer-review simulation.</p>
                        <h3>7.2.3. Autonomous Research Agents</h3>
                        <p>This category benchmarks agents designed for long-horizon workflows across general-purpose research, office, or planning tasks. WorkArena [762] and its extension WorkArena++ [763] propose enterprise task benchmarks where agents must complete ticket-based workflows involving retrieval, summarization, and coordination across documents.</p>
                        <p>OfficeBench [764] simulates a productivity software suite environment with tasks such as creating meeting memos, modifying spreadsheets, and replying to emails, emphasizing goal decomposition and tool selection. PlanBench [723] and FlowBench [728] test general workflow planning skills with abstracted task graphs and structured dependencies. ACPBench [724] evaluates agents in assistant-collaborator-planner triads, tracking performance in a hybrid role hierarchy.</p>
                        <p>TRAIL [765] focuses on multi-agent trace debugging and error attribution [766] in LLM-based systems, providing dense annotations for reasoning chains. CLIN [767] introduces lifelong few-shot learning benchmarks where agents adapt to distribution shift and task evolution. Agent-as-a-Judge [768] studies peer-review style evaluation with agents grading reasoning chains and correctness of other agents' outputs.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Research Agent Evaluation</h4>
                            <div class="analysis-item">
                                <h5>Benchmark Scope</h5>
                                <ul>
                                    <li><strong>Scientific Discovery:</strong> Hypothesis testing, experimentation</li>
                                    <li><strong>Autonomous Research:</strong> Full pipeline automation</li>
                                    <li><strong>Enterprise Tasks:</strong> Office workflows, ticket resolution</li>
                                    <li><strong>Lifelong Learning:</strong> Adaptation over time</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            72: {
                title: "Medical, Clinical, and Web Agent Benchmarks",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 7.2.4-7.2.6 &middot; Medical & Web</div>
                        <h1>Medical, Clinical, and Web Agent Benchmarks</h1>
                    </div>
                    <div class="original-content">
                        <h3>7.2.4. Medical and Clinical Agents</h3>
                        <p>These benchmarks test agents' abilities to reason with clinical knowledge, patient data, and multimodal biomedical sources. AgentClinic [769] introduces a virtual hospital environment where agents make diagnostic decisions based on patient symptoms and medical imaging. MedAgentBench [770] combines medical QA, patient simulation, and retrieval tasks in a multi-format benchmark grounded in standardized exams.</p>
                        <p>MedAgentsBench [771] evaluates multi-hop medical reasoning over structured and unstructured data, scoring agents on correctness and evidence alignment. EHRAgent [581] benchmarks agents working over structured electronic health record (EHR) tables and clinical notes to complete tasks like diagnosis code prediction and medication reasoning.</p>
                        <p>MedBrowseComp [702] focuses on browsing-based medical QA, where agents must retrieve and verify information across web pages. ACC [772] explores trustworthy medical agents with retrieval, hallucination detection, and citation-based support evaluation. MedAgents [601] uses a collaborative multi-agent dialogue setup to simulate patient-doctor-nurse interactions, scoring fluency and factual accuracy. GuardAgent [773] proposes a clinical privacy-safeguard agent with structured risk detection benchmarks on EHR and website forms.</p>
                        <h3>7.2.5. Web Agents</h3>
                        <p>Web agents operate in realistic browsing environments and are benchmarked on their ability to parse layouts, execute actions, and handle dynamic content. WebArena [45] introduces a browser-based benchmark suite containing 90+ realistic websites across domains like shopping and booking, where agents complete tasks with structured goals and click-based APIs. VisualWebArena [46] extends this with visual rendering, requiring agents to parse webpage images and align instructions with rendered components.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Domain-Specific Benchmarks</h4>
                            <div class="analysis-item">
                                <h5>Key Evaluation Areas</h5>
                                <ul>
                                    <li><strong>Medical:</strong> Diagnosis, treatment planning, EHR reasoning</li>
                                    <li><strong>Clinical:</strong> Patient interaction, evidence retrieval</li>
                                    <li><strong>Web:</strong> Navigation, action execution, dynamic content</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            73: {
                title: "Open Problems in Agentic Reasoning",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 8 &middot; Open Problems</div>
                        <h1>Open Problems in Agentic Reasoning</h1>
                    </div>
                    <div class="original-content">
                        <h3>7.2.6. General Tool-Use Agents</h3>
                        <p>This group of benchmarks emphasizes LLM agents' ability to invoke, coordinate, and reason over tools and APIs. GTA [691] presents a realistic tool-use benchmark grounded in user queries and deployed software tools, spanning APIs from image generation to analytics dashboards. NESTFUL [778] evaluates nested API invocation tasks requiring compositional planning across toolchains.</p>
                        <p>CodeAct [99] simulates executable function calling and evaluates agents on parsing, composition, and runtime accuracy. RestGPT [225] connects LLMs with RESTful APIs via coarse-to-fine planning pipelines, tested on 60+ tool types. Search-o1 [23] frames tool use as sequential retrieval, with benchmarks spanning code search, PDF querying, and scientific tool usage. Agentic RL [779] proposes a reinforcement learning agent with access to tool interfaces and evaluation tasks such as calendar scheduling and translation. ActionReasoningBench [780] benchmarks agents' ability to reason about action side effects and downstream consequences using a structured action grammar.</p>
                        <h3>8. Open Problems</h3>
                        <p>In this section, we highlight open problems arising from user-centric personalization, long-horizon interaction and credit assignment, world-model-based reasoning, multi-agent collaboration and training, latent internal reasoning, and the governance of agentic systems operating autonomously in real-world environments.</p>
                        <h3>8.1. User-centric Agentic Reasoning and Personalization</h3>
                        <p>User-centric agentic reasoning [782, 783] refers to an agent's ability to tailor its reasoning and actions to a specific individual user by modeling user characteristics, preferences, and interaction history over time. Rather than optimizing a fixed, task-defined objective, a user-centric agent treats the user as part of the environment and continuously adapts its strategy through extended, multi-turn interaction.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Open Research Directions</h4>
                            <div class="analysis-item">
                                <h5>Key Challenges</h5>
                                <ul>
                                    <li><strong>User-Centric:</strong> Personalization and adaptation</li>
                                    <li><strong>Long-Horizon:</strong> Credit assignment across interactions</li>
                                    <li><strong>World Models:</strong> Internal simulation for reasoning</li>
                                    <li><strong>Governance:</strong> Safety and autonomy control</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            74: {
                title: "Long-Horizon and World Model Challenges",
                content: `
                    <div class="article-header">
                        <div class="section-label">Section 8.2-8.3 &middot; Long-Horizon & World Models</div>
                        <h1>Long-Horizon Reasoning and World Models</h1>
                    </div>
                    <div class="original-content">
                        <h3>8.2. Long-horizon Agentic Reasoning from Extended Interaction</h3>
                        <p>A central open challenge in agentic reasoning is robust long-horizon planning and credit assignment across extended interactions. While methods such as ReAct and Tree of Thought improve short-horizon reasoning [5, 4], errors still compound rapidly in long tasks, as illustrated by embodied agents like Voyager [36]. RL-trained agents such as WebRL and Agent-R1 improve performance in realistic environments but rely on heavily engineered, domain-specific rewards and largely treat episodes independently [437, 28].</p>
                        <p>More recent process-aware approaches attempt to construct finer-grained credit signals [784, 15, 785], yet remain environment-specific. A core open problem is how to assign credit across tokens, tool calls, skills, and memory updates, and to generalize such learning across a long sequence of episodes and tasks.</p>
                        <h3>8.3. Agentic Reasoning with World Models</h3>
                        <p>World-model-based agents [786, 316] aim to mitigate myopic reasoning by enabling internal simulation and lookahead. Model-based RL systems such as DreamerV3 demonstrate the effectiveness of imagined rollouts for long-horizon control [787], while recent LLM-based agents adopt world models to web, code, and GUI environments [788, 786, 789, 790].</p>
                        <p>However, current designs rely on ad hoc representations and are typically trained on short-horizon or environment-specific data, raising concerns about calibration and generalization. Only a few works explore co-evolving world models and agents over long time scales [610, 791]. An open problem is how to jointly train, update, and evaluate world models in non-stationary environments, and how to assess their causal impact on downstream planning reliability.</p>
                        <h3>8.4. Multi-agent Collaborative Reasoning and Training</h3>
                        <p>Multi-agent collaboration has emerged as a powerful paradigm for scaling agentic reasoning through role specialization and division of labor [67, 792, 66]. While debate- and role-based systems often outperform single agents, most collaboration structures are still manually designed. Recent multi-agent RL approaches begin to treat collaboration itself as a trainable skill [409, 413, 26], but credit assignment at the group level remains poorly understood.</p>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Key Open Problems</h4>
                            <div class="analysis-item">
                                <h5>Research Gaps</h5>
                                <ul>
                                    <li><strong>Credit Assignment:</strong> Attributing success across long horizons</li>
                                    <li><strong>World Model Training:</strong> Joint optimization with agents</li>
                                    <li><strong>Calibration:</strong> Model reliability in novel environments</li>
                                    <li><strong>Group-Level Learning:</strong> Multi-agent credit assignment</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            75: {
                title: "References (Part 1)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 1)</h1>
                    </div>
                    <div class="original-content">
                        <p>This page contains the beginning of the paper's extensive reference list, documenting the scholarly works cited throughout the survey on Agentic Reasoning for Large Language Models.</p>
                        <div class="highlight-box">
                            <h4>Reference Format</h4>
                            <p>References are numbered sequentially and include author names, paper titles, publication venues, and years. The references span foundational works on LLMs, chain-of-thought prompting, tool use, multi-agent systems, and various application domains.</p>
                        </div>
                        <h3>Selected Key References</h3>
                        <ul>
                            <li><strong>[1]</strong> Wei et al. - Chain-of-thought prompting elicits reasoning in large language models (2022)</li>
                            <li><strong>[2]</strong> Denny Zhou et al. - Least-to-most prompting enables complex reasoning (2022)</li>
                            <li><strong>[3]</strong> Luyu Gao et al. - PAL: Program-aided language models (2023)</li>
                            <li><strong>[4]</strong> Shunyu Yao et al. - Tree of thoughts: Deliberate problem solving (2023)</li>
                            <li><strong>[5]</strong> Shunyu Yao et al. - ReAct: Synergizing reasoning and acting (2023)</li>
                            <li><strong>[6]</strong> Timo Schick et al. - Toolformer: Language models can teach themselves to use tools (2023)</li>
                            <li><strong>[7]</strong> Yongliang Shen et al. - HuggingGPT: Solving AI tasks with ChatGPT (2023)</li>
                            <li><strong>[8]</strong> Lei Wang et al. - A survey on large language model based autonomous agents (2024)</li>
                            <li><strong>[9]</strong> Aditi Singh et al. - Agentic retrieval-augmented generation: A survey (2025)</li>
                            <li><strong>[10]</strong> Yizheng Huang and Jimmy Huang - A survey on retrieval-augmented text generation (2024)</li>
                            <li><strong>[11]</strong> Xingyao Wang et al. - OpenHands: An open platform for AI software developers as generalist agents (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Reference Overview</h4>
                            <div class="analysis-item">
                                <h5>Key Themes in References</h5>
                                <p>The references cover major advances in reasoning prompting, tool integration, agent architectures, multi-agent coordination, and domain-specific applications. This comprehensive bibliography reflects the interdisciplinary nature of agentic reasoning research.</p>
                            </div>
                        </div>
                    </div>
                `
            },
            76: {
                title: "References (Part 2)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 2)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [24-37]</h3>
                        <ul>
                            <li><strong>[24]</strong> Weijiang Xu et al. - A-mem: Agentic memory for LLM agents (2025)</li>
                            <li><strong>[25]</strong> Tianxin Wei et al. - Evo-memory: Benchmarking LLM agent test-time learning with self-evolving memory (2025)</li>
                            <li><strong>[26]</strong> Hao Ma et al. - Fine-tuning LLM with sequential cooperative multi-agent reinforcement learning (2024)</li>
                            <li><strong>[27]</strong> Bowen Jin et al. - Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning (2025)</li>
                            <li><strong>[28]</strong> Zhepei Wei et al. - Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning (2025)</li>
                            <li><strong>[29]</strong> Trieu H Trinh et al. - Solving olympiad geometry without human demonstrations (2024)</li>
                            <li><strong>[30]</strong> Bernardino Romera-Paredes et al. - Mathematical discoveries from program search with large language models (2024)</li>
                            <li><strong>[31]</strong> Ranjan Sapkota et al. - Vibe coding vs. agentic coding: Fundamentals and practical implications of agentic AI (2025)</li>
                            <li><strong>[32]</strong> Andrej Karpathy - Vibe coding (Wikipedia, 2025)</li>
                            <li><strong>[33]</strong> Andres M Bran et al. - ChemCrow: Augmenting large-language models with chemistry tools (2023)</li>
                            <li><strong>[34]</strong> Fouad Bousetouane - Physical AI agents: Integrating cognitive intelligence with real-world action (2025)</li>
                            <li><strong>[35]</strong> Qianggang Ding et al. - Matexpert: Decomposing materials discovery by mimicking human experts (2024)</li>
                            <li><strong>[36]</strong> Guanzhi Wang et al. - Voyager: An open-ended embodied agent with large language models (2023)</li>
                            <li><strong>[37]</strong> Booker Meghan et al. - Embodiedrag: Dynamic 3d scene graph retrieval for efficient and scalable robot task planning (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Reference Themes</h4>
                            <div class="analysis-item">
                                <h5>Key Topics Covered</h5>
                                <ul>
                                    <li><strong>Memory Systems:</strong> Agentic memory, evolving memory</li>
                                    <li><strong>Reinforcement Learning:</strong> Multi-agent RL, search-based training</li>
                                    <li><strong>Domain Applications:</strong> Chemistry, mathematics, embodied agents</li>
                                    <li><strong>Emerging Paradigms:</strong> Vibe coding, agentic coding</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            77: {
                title: "References (Part 3)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 3)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [38-50]</h3>
                        <ul>
                            <li><strong>[38]</strong> Baining Zhao et al. - Embodied-r: Collaborative framework for activating embodied spatial reasoning in foundation models (2025)</li>
                            <li><strong>[39]</strong> Binxu Li et al. - MindAgent: Learning to use medical tools with multi-modal agent (2024)</li>
                            <li><strong>[40]</strong> Kexin Huang et al. - BioMNI: A general-purpose biomedical AI agent (2025)</li>
                            <li><strong>[41]</strong> Kuan Li et al. - WebSailor: Navigating super-human reasoning for web agents (2025)</li>
                            <li><strong>[42]</strong> Boyuan Zheng et al. - Skillweaver: Web agents can self-improve by discovering and honing skills (2025)</li>
                            <li><strong>[43]</strong> Ranjan Sapkota et al. - AI agents vs. agentic AI: A conceptual taxonomy, applications and challenges (2025)</li>
                            <li><strong>[44]</strong> Zijun Liu et al. - A dynamic LLM-powered agent network for task-oriented agent collaboration (2024)</li>
                            <li><strong>[45]</strong> Shuyan Zhou et al. - WebArena: A realistic web environment for building autonomous agents (2024)</li>
                            <li><strong>[46]</strong> Jing Yu Koh et al. - VisualWebArena: Evaluating multimodal agents on realistic visual web tasks (2024)</li>
                            <li><strong>[47]</strong> Lawrence Jang et al. - Videoweboarena: Evaluating long context multimodal agents with video understanding web tasks (2024)</li>
                            <li><strong>[48]</strong> Mohit Shridhar et al. - ALFWorld: Aligning text and embodied environments for interactive learning (2020)</li>
                            <li><strong>[49]</strong> Xiang Deng et al. - Mind2web: Towards a generalist agent for the web (2023)</li>
                            <li><strong>[50]</strong> Boyu Gou et al. - Mind2web-2: Evaluating agentic search with agent-as-a-judge (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Reference Categories</h4>
                            <div class="analysis-item">
                                <h5>Research Areas</h5>
                                <ul>
                                    <li><strong>Embodied AI:</strong> Spatial reasoning, physical interaction</li>
                                    <li><strong>Medical AI:</strong> Biomedical agents, medical tools</li>
                                    <li><strong>Web Agents:</strong> Browser automation, web navigation</li>
                                    <li><strong>Agent Collaboration:</strong> Multi-agent networks</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            78: {
                title: "References (Part 4)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 4)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [51-63]</h3>
                        <ul>
                            <li><strong>[51]</strong> Jie Huang and Kevin Chen-Chuan Chang - Towards reasoning in large language models: A survey (2022)</li>
                            <li><strong>[52]</strong> Qiguang Chen et al. - Towards reasoning era: A survey of long chain-of-thought for reasoning (2025)</li>
                            <li><strong>[53]</strong> Fengji Xu et al. - Towards large reasoning models: A survey of reinforced reasoning with large language models (2025)</li>
                            <li><strong>[54]</strong> Zixuan Ke et al. - A survey of frontiers in LLM reasoning: Inference scaling, learning to reason, and agentic systems (2025)</li>
                            <li><strong>[55]</strong> Kaiyan Zhang et al. - A survey of reinforcement learning for large reasoning models (2025)</li>
                            <li><strong>[56]</strong> Guibin Zhang et al. - The landscape of agentic reinforcement learning for LLMs: A survey (2025)</li>
                            <li><strong>[57]</strong> Minhua Lin et al. - A comprehensive survey on reinforcement learning-based agentic search (2025)</li>
                            <li><strong>[58]</strong> Jinyuan Fang et al. - A comprehensive survey of self-evolving AI agents: On path to artificial super intelligence (2025)</li>
                            <li><strong>[59]</strong> Huan-ang Gao et al. - A survey of self-evolving agents: On path to artificial super intelligence (2025)</li>
                            <li><strong>[60]</strong> Daya Guo et al. - Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning (2025)</li>
                            <li><strong>[61]</strong> Pengcheng Jiang et al. - Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning (2025)</li>
                            <li><strong>[62]</strong> John Schulman et al. - Proximal policy optimization algorithms (2017)</li>
                            <li><strong>[63]</strong> Zhihong Shao et al. - Deepseekmath: Pushing the limits of mathematical reasoning in open language models (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Survey and Review Papers</h4>
                            <div class="analysis-item">
                                <h5>Core Themes</h5>
                                <ul>
                                    <li><strong>Reasoning Surveys:</strong> Comprehensive reviews of LLM reasoning</li>
                                    <li><strong>Reinforcement Learning:</strong> PPO and reward-based training</li>
                                    <li><strong>Self-Evolution:</strong> Agents that improve over time</li>
                                    <li><strong>Mathematical Reasoning:</strong> DeepSeek series advances</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            79: {
                title: "References (Part 5)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 5)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [64-76]</h3>
                        <ul>
                            <li><strong>[64]</strong> Fanbin Lu et al. - Arpo: End-to-end policy optimization for GUI agents with experience replay (2025)</li>
                            <li><strong>[65]</strong> Qiying Yu et al. - Daxpo: An open-source LLM reinforcement learning system at scale (2025)</li>
                            <li><strong>[66]</strong> Qingyun Wu et al. - Autogen: Enabling next-gen LLM applications via multi-agent conversations (2024)</li>
                            <li><strong>[67]</strong> Guohao Li et al. - Camel: Communicative agents for mind exploration of large language model society (2024)</li>
                            <li><strong>[68]</strong> Mingchen Zhuge et al. - Gptswarm: Language agents as optimizable graphs (2024)</li>
                            <li><strong>[69]</strong> Haoyang Hong et al. - Multi-agent deep research: Training multi-agent systems with m-grpo (2025)</li>
                            <li><strong>[70]</strong> Alexander Novikov et al. - Alphaevolve: A coding agent for scientific and algorithmic discovery (2025)</li>
                            <li><strong>[71]</strong> Binfeng Xu et al. - REWOO: Decoupling reasoning from observations for efficient augmented language models (2023)</li>
                            <li><strong>[72]</strong> Bo Liu et al. - LLM+P: Empowering large language models with optimal planning proficiency (2023)</li>
                            <li><strong>[73]</strong> Karthik Valmeekam et al. - On the planning abilities of large language models: A critical investigation (2023)</li>
                            <li><strong>[74]</strong> Maciej Besta et al. - Graph of thoughts: Solving elaborate problems with large language models (2024)</li>
                            <li><strong>[75]</strong> Bilgehan Sel et al. - Algorithm of thoughts: Enhancing exploration of ideas in large language models (2023)</li>
                            <li><strong>[76]</strong> Runquan Gui et al. - Hypertree planning: Enhancing LLM reasoning via hierarchical thinking (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Multi-Agent and Planning References</h4>
                            <div class="analysis-item">
                                <h5>Key Contributions</h5>
                                <ul>
                                    <li><strong>Multi-Agent Frameworks:</strong> AutoGen, CAMEL, GPTSwarm</li>
                                    <li><strong>Planning Methods:</strong> LLM+P, Graph of Thoughts</li>
                                    <li><strong>Scientific Discovery:</strong> AlphaEvolve</li>
                                    <li><strong>Reasoning Structures:</strong> Tree, graph, and hypertree approaches</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            80: {
                title: "References (Part 6)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 6)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [77-90]</h3>
                        <ul>
                            <li><strong>[77]</strong> Jihwan Jeong et al. - Reflect-then-plan: Offline model-based planning through a doubly bayesian lens (2025)</li>
                            <li><strong>[78]</strong> Shishir G Patil et al. - Gorilla: Large language model connected with massive APIs (2024)</li>
                            <li><strong>[79]</strong> Tanmay Gupta et al. - CodeNav: Beyond tool-use to using real-world codebases with LLM agents (2024)</li>
                            <li><strong>[80]</strong> Liyi Chen et al. - Plan-on-graph: Self-correcting adaptive planning of large language model on knowledge graphs (2024)</li>
                            <li><strong>[81]</strong> Yanming Liu et al. - Tool-planner: Task planning with clusters across multiple tools (2024)</li>
                            <li><strong>[82]</strong> Yichao Liang et al. - Visualpredictor: Learning abstract world models with neuro-symbolic predicates for robot planning (2024)</li>
                            <li><strong>[83]</strong> Chan Hee Song et al. - LLMplanner: Few-shot grounded planning for embodied agents with large language models (2023)</li>
                            <li><strong>[84]</strong> Tamer Abuelsamid et al. - Agent-c: From autonomous web navigation to foundational design principles in agentic systems (2024)</li>
                            <li><strong>[85]</strong> Saaket Agashe et al. - Agent s: An open agentic framework that uses computers like a human (2024)</li>
                            <li><strong>[86]</strong> Minjong Yoo et al. - Exploratory retrieval-augmented planning for continual embodied instruction following (2024)</li>
                            <li><strong>[87]</strong> Rohan Sinha et al. - Real-time anomaly detection and reactive planning with large language models (2025)</li>
                            <li><strong>[88]</strong> Cristina Cornelio et al. - Hierarchical planning for complex tasks with knowledge graph-rag and symbolic verification (2025)</li>
                            <li><strong>[89]</strong> Zikang Zhou et al. - BehaviorGPT: Smart agent simulation for autonomous driving with next-patch prediction (2024)</li>
                            <li><strong>[90]</strong> Gaoyue Zhou et al. - Dino-wm: World models on pre-trained visual features enable zero-shot planning (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Planning and Tool Use References</h4>
                            <div class="analysis-item">
                                <h5>Research Directions</h5>
                                <ul>
                                    <li><strong>API Integration:</strong> Gorilla, CodeNav</li>
                                    <li><strong>Knowledge Graphs:</strong> Plan-on-graph, hierarchical planning</li>
                                    <li><strong>Embodied Planning:</strong> LLMPlanner, retrieval-augmented</li>
                                    <li><strong>World Models:</strong> Dino-wm, BehaviorGPT</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            81: {
                title: "References (Part 7)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 7)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [91-103]</h3>
                        <ul>
                            <li><strong>[91]</strong> Chongkai Gao et al. - Flip: Flow-centric generative planning as general-purpose manipulation world model (2024)</li>
                            <li><strong>[92]</strong> Shibo Hao et al. - LLM reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models (2024)</li>
                            <li><strong>[93]</strong> Lei Wang et al. - Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning (2023)</li>
                            <li><strong>[94]</strong> Tengxiao Liu et al. - Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts (2023)</li>
                            <li><strong>[95]</strong> Fei Ni et al. - Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation (2024)</li>
                            <li><strong>[96]</strong> Lutfi Eren Erdogan et al. - Plan-and-act: Improving planning of agents for long-horizon tasks (2025)</li>
                            <li><strong>[97]</strong> Jiuxin Wen et al. - Codeplan: Unlocking reasoning potential in large language models by scaling code-form planning (2024)</li>
                            <li><strong>[98]</strong> Michael Lutz et al. - Wilbur: Adaptive in-context learning for robust and accurate web agents (2024)</li>
                            <li><strong>[99]</strong> Xingyao Wang et al. - CodeAct: Executable code actions elicit better llm agents (2024)</li>
                            <li><strong>[100]</strong> Asif Rahman et al. - Marco: Multi-agent code optimization with real-time knowledge integration (2025)</li>
                            <li><strong>[101]</strong> Chengbo He et al. - Enhancing llm reasoning with multi-path collaborative reactive and reflection agents (2024)</li>
                            <li><strong>[102]</strong> Mrinal Rawat et al. - Pre-act: Multi-step planning and reasoning improves acting in llm agents (2025)</li>
                            <li><strong>[103]</strong> Renat Aksitov et al. - Rest meets react: Self-improvement for multi-step reasoning llm agent (2023)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Planning and Reasoning References</h4>
                            <div class="analysis-item">
                                <h5>Key Themes</h5>
                                <ul>
                                    <li><strong>Planning Methods:</strong> Plan-and-solve, CodePlan, Pre-act</li>
                                    <li><strong>Multi-Step Reasoning:</strong> Chain-of-thought variations</li>
                                    <li><strong>Self-Improvement:</strong> Rest meets ReAct, reflection</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            82: {
                title: "References (Part 8)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 8)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [104-118]</h3>
                        <ul>
                            <li><strong>[104]</strong> Xue Jiang et al. - Self-planning code generation with large language models (2024)</li>
                            <li><strong>[105]</strong> Dhruv Shah et al. - Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action (2023)</li>
                            <li><strong>[106]</strong> Elan Markowitz et al. - Tree-of-traversals: A zero-shot reasoning algorithm for augmenting black-box language models with knowledge graphs (2024)</li>
                            <li><strong>[107]</strong> Jieyi Long - Large language model guided tree-of-thought (2023)</li>
                            <li><strong>[108]</strong> Jing Yu Koh et al. - Tree search for language model agents (2024)</li>
                            <li><strong>[109]</strong> Chaojie Wang et al. - Q*: Improving multi-step reasoning for llms with deliberative planning (2024)</li>
                            <li><strong>[110]</strong> Silin Meng et al. - Llm-a*: Large language model enhanced incremental heuristic search on path planning (2024)</li>
                            <li><strong>[111]</strong> Gang Liu et al. - Multimodal large language models for inverse molecular design with retrosynthetic planning (2024)</li>
                            <li><strong>[112]</strong> Shibo Hao et al. - Reasoning with language model is planning with world model (2023)</li>
                            <li><strong>[113]</strong> Pranav Putta et al. - Agent q: Advanced reasoning and learning for autonomous AI agents (2024)</li>
                            <li><strong>[114]</strong> Henry W Sprueill et al. - Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design (2024)</li>
                            <li><strong>[115]</strong> Xiao Yu et al. - Prompt-based monte-carlo tree search for goal-oriented dialogue policy planning (2023)</li>
                            <li><strong>[116]</strong> Ziniu Zhao et al. - Large language models as commonsense knowledge for large-scale task planning (2023)</li>
                            <li><strong>[117]</strong> Ruomeng Ding et al. - Everything of thoughts: Defying the law of penrose triangle for thought generation (2023)</li>
                            <li><strong>[118]</strong> Ziru Chen et al. - When is tree search useful for llm planning? It depends on the discriminator (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tree Search and Planning</h4>
                            <div class="analysis-item">
                                <h5>Research Focus</h5>
                                <ul>
                                    <li><strong>Search Methods:</strong> Tree-of-thought, MCTS, A*</li>
                                    <li><strong>Domain Applications:</strong> Robotics, chemistry, dialogue</li>
                                    <li><strong>World Models:</strong> Planning with learned world models</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            83: {
                title: "References (Part 9)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 9)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [119-131]</h3>
                        <ul>
                            <li><strong>[119]</strong> Deqian Kong et al. - Latent plan transformer for trajectory abstraction: Planning as latent space inference (2024)</li>
                            <li><strong>[120]</strong> Xidong Feng et al. - Alphazero-like tree-search can guide large language model decoding and training (2023)</li>
                            <li><strong>[121]</strong> Jaesik Yoon et al. - Monte carlo tree diffusion for system 2 planning (2025)</li>
                            <li><strong>[122]</strong> John Schultz et al. - Mastering board games by external and internal planning with language models (2024)</li>
                            <li><strong>[123]</strong> Zhiliang Chen et al. - Broaden your scope! Efficient multi-turn conversation planning for llms with semantic space (2025)</li>
                            <li><strong>[124]</strong> Yuxi Xie et al. - Self-evaluation guided beam search for reasoning (2024)</li>
                            <li><strong>[125]</strong> Olga Golovneva et al. - Pathfinder: Guided search over multi-step reasoning paths (2023)</li>
                            <li><strong>[126]</strong> Haofu Qian et al. - Discriminator-guided plan for embodied reasoning for llm agent (2025)</li>
                            <li><strong>[127]</strong> Kanishk Gandhi et al. - Stream of search (sos): Learning to search in language (2024)</li>
                            <li><strong>[128]</strong> Swarnadeep Saha et al. - System-1.x: Learning to balance fast and slow planning with language models (2024)</li>
                            <li><strong>[129]</strong> Yanchu Guan et al. - Intelligent virtual assistants with llm-based process automation (2023)</li>
                            <li><strong>[130]</strong> Junjie Chen et al. - Enhancing llm-based agents via global planning and hierarchical execution (2024)</li>
                            <li><strong>[131]</strong> Zican Hu et al. - Divide and conquer: Grounding llms as efficient decision-making agents via offline hierarchical reinforcement learning (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Advanced Planning Methods</h4>
                            <div class="analysis-item">
                                <h5>Key Contributions</h5>
                                <ul>
                                    <li><strong>Diffusion Planning:</strong> Monte Carlo tree diffusion</li>
                                    <li><strong>Hierarchical:</strong> Global planning + local execution</li>
                                    <li><strong>System 1/2:</strong> Fast vs slow reasoning balance</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            84: {
                title: "References (Part 10)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 10)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [132-144]</h3>
                        <ul>
                            <li><strong>[132]</strong> Antonis Antoniades et al. - Swe-search: Enhancing software agents with monte carlo tree search and iterative refinement (2024)</li>
                            <li><strong>[133]</strong> Artem Lykov and Dzmitry Tsetserukoy - Llm-brain: AI-driven fast generation of robot behaviour tree based on large language model (2024)</li>
                            <li><strong>[134]</strong> Yue Cao and CS Lee - Robot behavior-tree-based task generation with large language models (2023)</li>
                            <li><strong>[135]</strong> Riccardo Andrea Izzo et al. - Btgenbot: Behavior tree generation for robotic tasks with lightweight llms (2024)</li>
                            <li><strong>[136]</strong> Michael Ahn et al. - Do as I can, not as I say: Grounding language in robotic affordances (2022)</li>
                            <li><strong>[137]</strong> Wenlong Huang et al. - Inner monologue: Embodied reasoning through planning with language models (2023)</li>
                            <li><strong>[138]</strong> Lin Guan et al. - Leveraging pre-trained large language models to construct and utilize world models for model-based task planning (2024)</li>
                            <li><strong>[139]</strong> Sadegh Mahdavi et al. - Leveraging environment interaction for automated pdrl translation and planning with large language models (2024)</li>
                            <li><strong>[140]</strong> Michael Katz et al. - Thought of search: Planning with language models through the lens of efficiency (2024)</li>
                            <li><strong>[141]</strong> Yilun Hao et al. - Planning anything with rigor: General-purpose zero-shot planning with llm-based formalized programming (2024)</li>
                            <li><strong>[142]</strong> Kaustubh Vyas et al. - From an llm swarm to a pddl-empowered hive: Planning self-executed instructions in a multi-modal jungle (2024)</li>
                            <li><strong>[143]</strong> Yuji Zhang et al. - Atomic reasoning for scientific table claim verification (2025)</li>
                            <li><strong>[144]</strong> Zibin Dong et al. - Diffuserlite: Towards real-time diffusion planning (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Robotics and Embodied Planning</h4>
                            <div class="analysis-item">
                                <h5>Research Areas</h5>
                                <ul>
                                    <li><strong>Behavior Trees:</strong> LLM-generated robot behaviors</li>
                                    <li><strong>World Models:</strong> Pre-trained models for planning</li>
                                    <li><strong>PDDL Integration:</strong> Formal planning with LLMs</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            85: {
                title: "References (Part 11)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 11)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [145-157]</h3>
                        <ul>
                            <li><strong>[145]</strong> Chunlok Lo et al. - Goal-space planning with subgoal models (2024)</li>
                            <li><strong>[146]</strong> Ao Li et al. - Agent-oriented planning in multi-agent systems (2024)</li>
                            <li><strong>[147]</strong> Mianchu Wang et al. - Goplan: Goal-conditioned offline reinforcement learning by planning with learned models (2023)</li>
                            <li><strong>[148]</strong> Chenglong Kang et al. - Retrointext: A multimodal large language model enhanced framework for retrosynthetic planning via in-context representation learning (2025)</li>
                            <li><strong>[149]</strong> Jiacheng Ye et al. - Beyond autoregression: Discrete diffusion for complex reasoning and planning (2024)</li>
                            <li><strong>[150]</strong> Yupeng Zheng et al. - Planagent: A multi-modal large language agent for closed-loop vehicle motion planning (2024)</li>
                            <li><strong>[151]</strong> Sid Nayak et al. - Long-horizon planning for multi-agent robots in partially observable environments (2024)</li>
                            <li><strong>[152]</strong> Tianxin Wei et al. - Latte: Collaborative test-time adaptation of vision-language models in federated learning (2025)</li>
                            <li><strong>[153]</strong> Wenzuun Bao et al. - WAPITI: A watermark for finetuned open-source LLMs (2024)</li>
                            <li><strong>[154]</strong> Zhining Liu et al. - Breaking silos: Adaptive model fusion unlocks better time series forecasting (2025)</li>
                            <li><strong>[155]</strong> Lihui Liu et al. - Logic query of thoughts: Guiding large language models to answer complex logic queries with knowledge graphs (2024)</li>
                            <li><strong>[156]</strong> Zhining Liu et al. - Class-imbalanced graph learning without class rebalancing (2024)</li>
                            <li><strong>[157]</strong> Zhining Liu et al. - Class-imbalanced graph learning without class rebalancing (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Multi-Agent and Specialized Planning</h4>
                            <div class="analysis-item">
                                <h5>Research Directions</h5>
                                <ul>
                                    <li><strong>Goal-Conditioned:</strong> Subgoal models, goal planning</li>
                                    <li><strong>Multi-Agent:</strong> Coordinated robot planning</li>
                                    <li><strong>Domain-Specific:</strong> Chemistry, vehicle motion</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            86: {
                title: "References (Part 12)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 12)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [158-171]</h3>
                        <ul>
                            <li><strong>[158]</strong> Zhining Liu et al. - AIM: Attributing, interpreting, mitigating data unfairness (2025)</li>
                            <li><strong>[159]</strong> Zhining Liu et al. - Topological augmentation for class-imbalanced node classification (2023)</li>
                            <li><strong>[160]</strong> Zhichen Zeng et al. - Pave your own path: Graph gradual domain adaptation on fused Gromov-Wasserstein geodesics (2025)</li>
                            <li><strong>[161]</strong> Zhichen Zeng et al. - Graph mixup on approximate Gromov-Wasserstein geodesics (2024)</li>
                            <li><strong>[162]</strong> Xiao Lin et al. - Moralise: A structured benchmark for moral alignment in visual language models (2025)</li>
                            <li><strong>[163]</strong> Xiao Lin et al. - BackTime: Backdoor attacks on multivariate time series forecasting (2024)</li>
                            <li><strong>[164]</strong> Ruizhong Qiu et al. - Saffron-1: Safety inference scaling (2025)</li>
                            <li><strong>[165]</strong> Ruizhong Qiu et al. - Ask, and it shall be given: On the Turing completeness of prompting (2025)</li>
                            <li><strong>[166]</strong> Ruizhong Qiu et al. - How efficient is LLM-generated code? A rigorous & high-standard benchmark (2024)</li>
                            <li><strong>[167]</strong> Ruizhong Qiu et al. - TUCKET: A tensor time series data structure for efficient and accurate factor analysis over time ranges (2024)</li>
                            <li><strong>[168]</strong> Ruizhong Qiu et al. - Reconstructing graph diffusion history from a single snapshot (2023)</li>
                            <li><strong>[169]</strong> Ruizhong Qiu et al. - DIMES: A differentiable meta solver for combinatorial optimization problems (2022)</li>
                            <li><strong>[170]</strong> Zhe Xu et al. - Discrete-state continuous-time diffusion for graph generation (2024)</li>
                            <li><strong>[171]</strong> Ting-Wei Li et al. - Model-free graph data selection under distribution shift (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Graph Learning and Optimization</h4>
                            <div class="analysis-item">
                                <h5>Research Areas</h5>
                                <ul>
                                    <li><strong>Graph Methods:</strong> Domain adaptation, mixup, diffusion</li>
                                    <li><strong>Safety:</strong> Inference scaling, backdoor detection</li>
                                    <li><strong>Efficiency:</strong> Code generation benchmarks</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            87: {
                title: "References (Part 13)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 13)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [172-184]</h3>
                        <ul>
                            <li><strong>[172]</strong> Jiaru Zou et al. - Transformer copilot: Learning from the mistake log in llm fine-tuning (2025)</li>
                            <li><strong>[173]</strong> Ruizhong Qiu and Hanghang Tong - Gradient compressed sensing: A query-efficient gradient estimator for high-dimensional zeroth-order optimization (2024)</li>
                            <li><strong>[174]</strong> Hyunsik Yoo et al. - Embracing plasticity: Balancing stability and plasticity in continual recommender systems (2025)</li>
                            <li><strong>[175]</strong> Hyunsik Yoo et al. - Generalizable recommender system during temporal popularity distribution shifts (2025)</li>
                            <li><strong>[176]</strong> Hyunsik Yoo et al. - Ensuring user-side fairness in dynamic recommender systems (2024)</li>
                            <li><strong>[177]</strong> Eunice Chan et al. - Group fairness via group consensus (2024)</li>
                            <li><strong>[178]</strong> Ziwei Wu et al. - Fair anomaly detection for imbalanced groups (2024)</li>
                            <li><strong>[179]</strong> Xinyu He et al. - On the sensitivity of individual fairness: Measures and robust algorithms (2024)</li>
                            <li><strong>[180]</strong> Dingsu Wang et al. - Networked time series imputation via position-aware graph enhanced variational autoencoders (2023)</li>
                            <li><strong>[181]</strong> Yue Meng and Chuchu Fan - Telograf: Temporal logic planning via graph-encoded flow matching (2025)</li>
                            <li><strong>[182]</strong> Ruizhe Zhong et al. - Flexplanner: Flexible 3d floorplanning via deep reinforcement learning in hybrid action space with multi-modality representation (2024)</li>
                            <li><strong>[183]</strong> Yangyang Li et al. - Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent (2024)</li>
                            <li><strong>[184]</strong> Jiaru Zou et al. - Rag over tables: Hierarchical memory index, multi-stage retrieval, and benchmarking (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Fairness and RAG Systems</h4>
                            <div class="analysis-item">
                                <h5>Key Topics</h5>
                                <ul>
                                    <li><strong>Fairness:</strong> Individual, group, and dynamic fairness</li>
                                    <li><strong>RAG:</strong> Retrieval augmented generation advances</li>
                                    <li><strong>Temporal:</strong> Time series and dynamic systems</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            88: {
                title: "References (Part 14)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 14)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [185-198]</h3>
                        <ul>
                            <li><strong>[185]</strong> Shuofei Qiao et al. - Agent planning with world knowledge model (2024)</li>
                            <li><strong>[186]</strong> Zichen Liu et al. - Continual reinforcement learning by planning with online world models (2025)</li>
                            <li><strong>[187]</strong> Hang Wang et al. - Adawm: Adaptive world model based planning for autonomous driving (2025)</li>
                            <li><strong>[188]</strong> Yining Ye et al. - Rational decision-making agent with internalized utility judgment (2023)</li>
                            <li><strong>[189]</strong> Zhenfang Chen et al. - Scaling autonomous agents via automatic reward modeling and planning (2024)</li>
                            <li><strong>[190]</strong> Max Ruiz Luyten et al. - Strategic planning: A top-down approach to option generation (2025)</li>
                            <li><strong>[191]</strong> Chang Ma et al. - Non-myopic generation of language models for reasoning and planning (2024)</li>
                            <li><strong>[192]</strong> Ruiqi Ni et al. - Physics-informed temporal difference metric learning for robot motion planning (2024)</li>
                            <li><strong>[193]</strong> Sharath Matada et al. - Generalizable motion planning via operator learning (2024)</li>
                            <li><strong>[194]</strong> Hongjin Su et al. - Toolorchestra: Elevating intelligence via efficient model and tool orchestration (2025)</li>
                            <li><strong>[195]</strong> Amber Xie et al. - Latent diffusion planning for imitation learning (2025)</li>
                            <li><strong>[196]</strong> Wei Xiao et al. - Safediffuser: Safe planning with diffusion probabilistic models (2023)</li>
                            <li><strong>[197]</strong> Yixiang Shan et al. - Contradiff: Planning towards high return states via contrastive learning (2025)</li>
                            <li><strong>[198]</strong> Anian Ruoss et al. - Amortized planning with large-scale transformers: A case study on chess (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>World Models and Diffusion Planning</h4>
                            <div class="analysis-item">
                                <h5>Research Directions</h5>
                                <ul>
                                    <li><strong>World Models:</strong> Adaptive, online planning</li>
                                    <li><strong>Diffusion:</strong> Safe and latent diffusion planning</li>
                                    <li><strong>Tool Orchestration:</strong> Multi-tool coordination</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            89: {
                title: "References (Part 15)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 15)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [199-209]</h3>
                        <ul>
                            <li><strong>[199]</strong> Bhargavi Paranjape et al. - Art: Automatic multi-step reasoning and tool-use for large language models (2023)</li>
                            <li><strong>[200]</strong> Zhipeng Chen et al. - ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models (2023)</li>
                            <li><strong>[201]</strong> Yining Lu et al. - GEAR: Augmenting language models with generalizable and efficient tool resolution (2024)</li>
                            <li><strong>[202]</strong> Shirley Wu et al. - Avatar: Optimizing llm agents for tool usage via contrastive reasoning (2024)</li>
                            <li><strong>[203]</strong> Yujia Qin et al. - ToolLLM: Facilitating large language models to master 16000+ real-world APIs (2024)</li>
                            <li><strong>[204]</strong> Qiaoyu Tang et al. - ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases (2023)</li>
                            <li><strong>[205]</strong> Mingyang Chen et al. - Learning to reason with search via reinforcement learning (2025)</li>
                            <li><strong>[206]</strong> Qinqiu Dong et al. - Autotool: Dynamic tool selection and integration for agentic reasoning (2025)</li>
                            <li><strong>[207]</strong> Yaobo Liang et al. - Taskmatrix.ai: Completing tasks by connecting foundation models with millions of APIs (2023)</li>
                            <li><strong>[208]</strong> Pan Lu et al. - Octotools: An agentic framework with extensible tools for complex reasoning (2025)</li>
                            <li><strong>[209]</strong> Pan Lu et al. - Octotools: An agentic framework with extensible tools for complex reasoning (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tool Learning and Integration</h4>
                            <div class="analysis-item">
                                <h5>Key Systems</h5>
                                <ul>
                                    <li><strong>ToolLLM:</strong> Massive API integration</li>
                                    <li><strong>TaskMatrix:</strong> Foundation model tool connection</li>
                                    <li><strong>Octotools:</strong> Extensible tool frameworks</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            90: {
                title: "References (Part 16)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 16)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [210-220]</h3>
                        <ul>
                            <li><strong>[210]</strong> Zijing Zhang et al. - Toolexpert: Optimizing multi-tool selection in llms with similarity and dependency-aware experience networks (2025)</li>
                            <li><strong>[211]</strong> Yuchen Zhuang et al. - Toolchain*: Efficient action space navigation in large language models with a* search (2024)</li>
                            <li><strong>[212]</strong> Tatsuro Inaba et al. - MultiTool-CoT: GPT-3 can use multiple external tools with chain of thought prompting (2023)</li>
                            <li><strong>[213]</strong> Harsh Trivedi et al. - Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions (2022)</li>
                            <li><strong>[214]</strong> Cheng-Yu Hsieh et al. - Tool documentation enables zero-shot tool-usage with large language models (2023)</li>
                            <li><strong>[215]</strong> Siyu Yuan et al. - Easytool: Enhancing llm-based agents with concise tool instruction (2024)</li>
                            <li><strong>[216]</strong> Changle Qu et al. - Tool learning with large language models: A survey (2025)</li>
                            <li><strong>[217]</strong> Zhengliang Shi et al. - Tool learning in the wild: Empowering language models as automatic tool agents (2024)</li>
                            <li><strong>[218]</strong> Hongru Wang et al. - Empowering large language models: Tool learning for real-world interaction (2024)</li>
                            <li><strong>[219]</strong> Ling Yang et al. - Buffer of thoughts: Thought-augmented reasoning with large language models (2024)</li>
                            <li><strong>[220]</strong> Kanzhi Cheng et al. - Seedclick: Harnessing gui grounding for advanced visual gui agents (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Tool Learning Evolution</h4>
                            <div class="analysis-item">
                                <h5>Key Advances</h5>
                                <ul>
                                    <li><strong>Multi-Tool:</strong> Coordinated tool usage</li>
                                    <li><strong>Zero-Shot:</strong> Documentation-based tool use</li>
                                    <li><strong>Real-World:</strong> Practical tool integration</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            91: {
                title: "References (Part 17)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 17)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [221-233]</h3>
                        <ul>
                            <li><strong>[221]</strong> Jiaru Zou et al. - Reasonflux: Trajectory-aware prms for long chain-of-thought reasoning in llms (2025)</li>
                            <li><strong>[222]</strong> Daye Nam et al. - Using an llm to help with code understanding (2024)</li>
                            <li><strong>[223]</strong> Junde Wu et al. - Agentic reasoning: A streamlined framework for enhancing llm reasoning with agentic tools (2025)</li>
                            <li><strong>[224]</strong> Pan Lu et al. - Chameleon: Plug-and-play compositional reasoning with large language models (2023)</li>
                            <li><strong>[225]</strong> Yifan Song et al. - Restgpt: Connecting large language models with real-world restful APIs (2024)</li>
                            <li><strong>[226]</strong> Archiki Prasad et al. - Adapt: As-needed decomposition and planning with language models (2023)</li>
                            <li><strong>[227]</strong> Da Yin et al. - Agent lumos: Unified and modular training for open-source language agents (2023)</li>
                            <li><strong>[228]</strong> Zhengliang Shi et al. - Learning to use tools via cooperative and interactive agents (2024)</li>
                            <li><strong>[229]</strong> Robert Kirk et al. - Understanding the effects of rlhf on llm generalisation and diversity (2024)</li>
                            <li><strong>[230]</strong> Ziniu Li et al. - Preserving diversity in supervised fine-tuning of large language models (2024)</li>
                            <li><strong>[231]</strong> Laura O'Mahony et al. - Attributing mode collapse in the fine-tuning of large language models (2024)</li>
                            <li><strong>[232]</strong> Yirong Zeng et al. - iTool: Reinforced fine-tuning with dynamic deficiency calibration for advanced tool use (2025)</li>
                            <li><strong>[233]</strong> Zhaochen Yu et al. - Demystifying reinforcement learning in agentic reasoning (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Training and Fine-Tuning</h4>
                            <div class="analysis-item">
                                <h5>Key Topics</h5>
                                <ul>
                                    <li><strong>RLHF:</strong> Effects on generalization</li>
                                    <li><strong>Fine-Tuning:</strong> Diversity preservation</li>
                                    <li><strong>Tool Training:</strong> Dynamic calibration</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            92: {
                title: "References (Part 18)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 18)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [234-245]</h3>
                        <ul>
                            <li><strong>[234]</strong> Yifei Zhou et al. - Sweet-rl: Training multi-turn llm agents on collaborative reasoning tasks (2025)</li>
                            <li><strong>[235]</strong> Yuxiang Wei et al. - Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution (2025)</li>
                            <li><strong>[236]</strong> Zijing Zhang et al. - Rlvmr: Reinforcement learning with verifiable meta-reasoning rewards for robust long-horizon agents (2025)</li>
                            <li><strong>[237]</strong> Jiaru Zou et al. - Autotool: Dynamic tool selection and integration for agentic reasoning (2025)</li>
                            <li><strong>[238]</strong> Jiazhan Feng et al. - Retool: Reinforcement learning for strategic tool use in llms (2025)</li>
                            <li><strong>[239]</strong> Hao Sun et al. - Zerosearch: Incentivize the search capability of llms without searching (2025)</li>
                            <li><strong>[240]</strong> Kimi Team et al. - Kimi k1.5: Scaling reinforcement learning with llms (2025)</li>
                            <li><strong>[241]</strong> Gheorghe Comanici et al. - Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities (2025)</li>
                            <li><strong>[242]</strong> Kimi Team - Kimi k2: Open agentic intelligence (2025)</li>
                            <li><strong>[243]</strong> Aohan Zeng et al. - GLM-4.5: Agentic, reasoning, and coding (arc) foundation models (2025)</li>
                            <li><strong>[244]</strong> Jiaru Zou et al. - Tattoo: Tool-grounded thinking prm for test-time scaling in tabular reasoning (2025)</li>
                            <li><strong>[245]</strong> Shubo Hao et al. - Toolkengpt: Augmenting frozen language models with massive tool embeddings (2023)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Large-Scale RL and Foundation Models</h4>
                            <div class="analysis-item">
                                <h5>Notable Systems</h5>
                                <ul>
                                    <li><strong>Kimi:</strong> K1.5 and K2 agentic models</li>
                                    <li><strong>Gemini 2.5:</strong> Advanced multimodal agentic</li>
                                    <li><strong>GLM-4.5:</strong> Agentic foundation model</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            93: {
                title: "References (Part 19)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 19)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [246-256]</h3>
                        <ul>
                            <li><strong>[246]</strong> Zhiyuan Ma et al. - Advancing tool-augmented large language models via meta-verification and reflection learning (2025)</li>
                            <li><strong>[247]</strong> Mengsong Wu et al. - Chain-of-tools: Utilizing massive unseen tools in the cor reasoning of frozen language models (2025)</li>
                            <li><strong>[248]</strong> Shitian Zhao et al. - Pyvision: Agentic vision with dynamic tooling (2025)</li>
                            <li><strong>[249]</strong> Yunheng Zou et al. - Autoruler: Reasoning chain-of-thought extracted rule-based rewards improve preference learning (2025)</li>
                            <li><strong>[250]</strong> Xing Cui et al. - T*agent A tool-augmented multimodal misinformation detection agent with monte carlo tree search (2025)</li>
                            <li><strong>[251]</strong> Yuanhang Zheng et al. - Toolrerank: Adaptive and hierarchy-aware reranking for tool retrieval (2024)</li>
                            <li><strong>[252]</strong> Patrick Lewis et al. - Retrieval-augmented generation for knowledge-intensive nlp tasks (2020)</li>
                            <li><strong>[253]</strong> Xiao Yang et al. - Crag-comprehensive rag benchmark (2024)</li>
                            <li><strong>[254]</strong> Ofir Press et al. - Measuring and narrowing the compositionality gap in language models (2023)</li>
                            <li><strong>[255]</strong> Akari Asai et al. - Self-rag: Self-reflective retrieval augmented generation (2023)</li>
                            <li><strong>[256]</strong> Xinyan Guan et al. - Deeprag: Thinking to retrieve step by step for large language models (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>RAG and Tool Retrieval</h4>
                            <div class="analysis-item">
                                <h5>Key Systems</h5>
                                <ul>
                                    <li><strong>RAG:</strong> Foundational retrieval-augmented generation</li>
                                    <li><strong>Self-RAG:</strong> Self-reflective retrieval</li>
                                    <li><strong>CRAG:</strong> Comprehensive RAG benchmark</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            94: {
                title: "References (Part 20)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 20)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [257-269]</h3>
                        <ul>
                            <li><strong>[257]</strong> Yutao Zhu et al. - Inters: Unlocking the power of large language models in search with instruction tuning (2024)</li>
                            <li><strong>[258]</strong> Reiichiro Nakano et al. - Webgpt: Browser-assisted question-answering with human feedback (2021)</li>
                            <li><strong>[259]</strong> Jerry Huang et al. - Rag-e: Advancing retrieval-augmented generation via rl and curriculum learning (2025)</li>
                            <li><strong>[260]</strong> Yuxiang Zheng et al. - Deepresearcher: Scaling deep research via reinforcement learning in real-world environments (2025)</li>
                            <li><strong>[261]</strong> Zhongxiang Sun et al. - Rexatter: Retrieval-augmented reasoning with trustworthy process rewarding (2025)</li>
                            <li><strong>[262]</strong> Meng-Chieh Lee et al. - Agent-g: An agentic framework for graph retrieval augmented generation (2024)</li>
                            <li><strong>[263]</strong> Xuying Ning et al. - Mc-search: Benchmarking multimodal agentic rag with structured reasoning chains (2025)</li>
                            <li><strong>[264]</strong> Zhili Shen et al. - Gear: Graph-enhanced agent for retrieval-augmented generation (2025)</li>
                            <li><strong>[265]</strong> Han Zhang et al. - Learning to retrieve and reason on knowledge graph through active self-reflection (2025)</li>
                            <li><strong>[266]</strong> Kelong Mao et al. - Rag-studio: Towards in-domain adaptation of retrieval augmented generation through self-alignment (2024)</li>
                            <li><strong>[267]</strong> Tianjun Zhang et al. - Raft: Adapting language model to domain specific rag (2024)</li>
                            <li><strong>[268]</strong> Xi Victoria Lin et al. - Ra-dit: Retrieval-augmented dual instruction tuning (2023)</li>
                            <li><strong>[269]</strong> Xuan-Phi Nguyen et al. - Sfr-rag: Towards contextually faithful llms (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Advanced RAG Systems</h4>
                            <div class="analysis-item">
                                <h5>Key Advances</h5>
                                <ul>
                                    <li><strong>RL for RAG:</strong> Reinforcement learning enhanced retrieval</li>
                                    <li><strong>Graph RAG:</strong> Knowledge graph integration</li>
                                    <li><strong>Domain Adaptation:</strong> RAFT, Ra-dit tuning</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            95: {
                title: "References (Part 21)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 21)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [270-282]</h3>
                        <ul>
                            <li><strong>[270]</strong> Aman Madaan et al. - Self-refine: Iterative refinement with self-feedback (2024)</li>
                            <li><strong>[271]</strong> Ziqi Wang et al. - Enable language models to implicitly learn self-improvement from data (2024)</li>
                            <li><strong>[272]</strong> Xuezhi Wang et al. - Self-consistency improves chain of thought reasoning in language models (2023)</li>
                            <li><strong>[273]</strong> Wenhu Chen et al. - Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks (2023)</li>
                            <li><strong>[274]</strong> Aohan Zeng et al. - Agenttuning: Enabling generalized agent abilities for llms (2024)</li>
                            <li><strong>[275]</strong> Cheng-Yu Hsieh et al. - Distilling step-by-step! Outperforming larger language models with less training data and smaller model sizes (2023)</li>
                            <li><strong>[276]</strong> Paul F Christiano et al. - Deep reinforcement learning from human preferences (2017)</li>
                            <li><strong>[277]</strong> Rafael Rafailov et al. - Direct preference optimization: Your language model is secretly a reward model (2023)</li>
                            <li><strong>[278]</strong> Yuntao Bai et al. - Constitutional ai: Harmlessness from ai feedback (2022)</li>
                            <li><strong>[279]</strong> Jiaqi Li et al. - Reflecevo: Improving meta introspection of small llms by learning self-reflection (2025)</li>
                            <li><strong>[280]</strong> Zhi Zheng and Wee Sun Lee - Reasoning-cv: Fine-tuning powerful reasoning llms for knowledge-assisted claim verification (2025)</li>
                            <li><strong>[281]</strong> Alan Dao and Thinh Le - Retero: Enhancing llm search ability by trying one-more-time (2025)</li>
                            <li><strong>[282]</strong> Nearcbos Potamitis and Akhil Arora - Are retrials all you need? Enhancing large language model reasoning without verbalized feedback (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Self-Improvement and Alignment</h4>
                            <div class="analysis-item">
                                <h5>Key Methods</h5>
                                <ul>
                                    <li><strong>Self-Refine:</strong> Iterative self-improvement</li>
                                    <li><strong>DPO:</strong> Direct preference optimization</li>
                                    <li><strong>Constitutional AI:</strong> AI feedback alignment</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            96: {
                title: "References (Part 22)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 22)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [283-296]</h3>
                        <ul>
                            <li><strong>[283]</strong> Hung Le et al. - Coderl: Mastering code generation through pretrained models and deep reinforcement learning (2022)</li>
                            <li><strong>[284]</strong> Ansong Ni et al. - Lever: Learning to verify language-to-code generation with execution (2023)</li>
                            <li><strong>[285]</strong> Carlos E. Jimenez et al. - Swe-bench: Can language models resolve real-world github issues? (2024)</li>
                            <li><strong>[286]</strong> Danny Driess et al. - Palm-e: An embodied multimodal model (2023)</li>
                            <li><strong>[287]</strong> Shelly Bensal et al. - Reflect, retry, reward: Self-improving llms via reinforcement learning (2025)</li>
                            <li><strong>[288]</strong> Harrison Lee et al. - Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback (2024)</li>
                            <li><strong>[289]</strong> Potsawee Manakul et al. - Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models (2023)</li>
                            <li><strong>[290]</strong> Jishnu Ray Chowdhury and Cornelia Caragea - Zero-shot verification-guided chain of thoughts (2025)</li>
                            <li><strong>[291]</strong> Dongxu Zhang et al. - Ascot: An adaptive self-correction chain-of-thought method for late-stage fragility in llms (2025)</li>
                            <li><strong>[292]</strong> Linzhuang Sun et al. - Mm-verify: Enhancing multimodal reasoning with chain-of-thought verification (2025)</li>
                            <li><strong>[293]</strong> Charles Packer et al. - Memgpt: Towards llms as operating systems (2023)</li>
                            <li><strong>[294]</strong> Zi-Yi Dou et al. - Re-rest: Reflection-reinforced self-training for language agents (2024)</li>
                            <li><strong>[295]</strong> LangChain AI - Langchain library (2023)</li>
                            <li><strong>[296]</strong> Jerry Liu - LlamaIndex, 11 (2022)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Code, Verification, and Infrastructure</h4>
                            <div class="analysis-item">
                                <h5>Key Systems</h5>
                                <ul>
                                    <li><strong>SWE-bench:</strong> Real-world code benchmarks</li>
                                    <li><strong>MemGPT:</strong> LLMs as operating systems</li>
                                    <li><strong>LangChain/LlamaIndex:</strong> Agent infrastructure</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            97: {
                title: "References (Part 23)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 23)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [297-310]</h3>
                        <ul>
                            <li><strong>[297]</strong> Wanjun Zhong et al. - Memorybank: Enhancing large language models with long-term memory (2023)</li>
                            <li><strong>[298]</strong> Zora Zhiruo Wang et al. - Agent workflow memory (2024)</li>
                            <li><strong>[299]</strong> Jizhan Fang et al. - Lightmem: Lightweight and efficient memory-augmented generation (2025)</li>
                            <li><strong>[300]</strong> Jiayan Nan et al. - Nemori: Self-organizing agent memory inspired by cognitive science (2025)</li>
                            <li><strong>[301]</strong> Qizheng Zhang et al. - Agentic context engineering: Evolving contexts for self-improving language models (2025)</li>
                            <li><strong>[302]</strong> Siru Ouyang et al. - Reasoningbank: Scaling agent self-evolving with reasoning memory (2025)</li>
                            <li><strong>[303]</strong> Mirae Suzgun et al. - Dynamic cheatsheet: Test-time learning with adaptive memory (2025)</li>
                            <li><strong>[304]</strong> Kevin Lin et al. - Sleep-time compute: Beyond inference scaling at test-time (2025)</li>
                            <li><strong>[305]</strong> Darren Edge et al. - From local to global: A graph rag approach to query-focused summarization (2024)</li>
                            <li><strong>[306]</strong> Preston Rasmussen et al. - Zep: A temporal knowledge graph architecture for agent memory (2025)</li>
                            <li><strong>[307]</strong> Zaijing Li et al. - Optimus-1: Hybrid multimodal memory empowered agents excel in long-horizon tasks (2024)</li>
                            <li><strong>[308]</strong> Tomoyuki Kagaya et al. - Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents (2024)</li>
                            <li><strong>[309]</strong> Lin Long et al. - Seeing, listening, remembering, and reasoning: A multimodal agent with long-term memory (2025)</li>
                            <li><strong>[310]</strong> Yuanchen Bei et al. - Mem-gallery: Benchmarking multimodal long-term conversational memory for mllm agents (2026)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Agent Memory Systems</h4>
                            <div class="analysis-item">
                                <h5>Memory Architectures</h5>
                                <ul>
                                    <li><strong>MemoryBank:</strong> Long-term memory enhancement</li>
                                    <li><strong>Workflow Memory:</strong> Task-specific memory</li>
                                    <li><strong>Graph RAG:</strong> Knowledge graph memory</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            98: {
                title: "References (Part 24)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 24)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [311-323]</h3>
                        <ul>
                            <li><strong>[311]</strong> Pengzhou Cheng et al. - Agent-scankit: Unraveling memory and reasoning of multimodal agents via sensitivity perturbations (2025)</li>
                            <li><strong>[312]</strong> Zijian Zhou et al. - MEM1: Learning to synergize memory and reasoning for efficient long-horizon agents (2025)</li>
                            <li><strong>[313]</strong> Yuqiang Zhang et al. - Memory as action: Autonomous context curation for long-horizon agentic tasks (2025)</li>
                            <li><strong>[314]</strong> Hongli Yu et al. - Memagent: Reshaping long-context llm with multi-conv rl-based memory agent (2025)</li>
                            <li><strong>[315]</strong> Yu Wang et al. - Mem-{\\alpha}: Learning memory construction via reinforcement learning (2025)</li>
                            <li><strong>[316]</strong> Kai Zhang et al. - Agentic memory: Learning unified long and short-term memory management for large language model agents (2026)</li>
                            <li><strong>[317]</strong> Shengtao Zhang et al. - Memrl: Self-evolving agents with runtime reinforcement learning on episodic memory (2025)</li>
                            <li><strong>[318]</strong> Akari Asai et al. - Self-rag: Learning to retrieve, generate, and critique through self-reflection (2024)</li>
                            <li><strong>[319]</strong> Akari Asai et al. - Self-rag: Learning to retrieve, generate, and critique through self-reflection (2024)</li>
                            <li><strong>[320]</strong> Rel-lim: Towards a general read-write memory for large language models (2023)</li>
                            <li><strong>[321]</strong> Bing Wang et al. - Scm: Enhancing large language model with self-controlled memory framework (2024)</li>
                            <li><strong>[322]</strong> Adyasha Maharana et al. - Evaluating very long-term conversational memory of llm agents (2024)</li>
                            <li><strong>[323]</strong> Di Wu et al. - Longmemeval: Benchmarking chat assistants on long-term interactive memory (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Memory Learning and Evaluation</h4>
                            <div class="analysis-item">
                                <h5>Research Directions</h5>
                                <ul>
                                    <li><strong>RL Memory:</strong> Learning memory construction</li>
                                    <li><strong>Long-Term:</strong> Conversational memory evaluation</li>
                                    <li><strong>Self-Controlled:</strong> Autonomous memory management</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            99: {
                title: "References (Part 25)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 25)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [324-335]</h3>
                        <ul>
                            <li><strong>[324]</strong> Ruihan Yang et al. - SELFGOAL: Your language agents already know how to achieve high-level goals (2025)</li>
                            <li><strong>[325]</strong> Ajay Patel et al. - Large language models can self-improve at web agent tasks (2024)</li>
                            <li><strong>[326]</strong> Xiaohe Bo et al. - Reflective multi-agent collaboration based on large language models (2024)</li>
                            <li><strong>[327]</strong> Yangyang Yu et al. - Firmem: A performance-enhanced llm trading agent with layered memory and character design (2023)</li>
                            <li><strong>[328]</strong> Yu Wang and Xi Chen - Mirix: Multi-agent memory system for llm-based agents (2025)</li>
                            <li><strong>[329]</strong> Siru Ouyang et al. - Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning (2025)</li>
                            <li><strong>[330]</strong> Alireza Rezazadeh et al. - From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms (2024)</li>
                            <li><strong>[331]</strong> Zelong Li et al. - Autoflow: Automated workflow generation for large language model agents (2024)</li>
                            <li><strong>[332]</strong> Jayi Zhang et al. - Autoflow: Automating agentic workflow generation (2024)</li>
                            <li><strong>[333]</strong> Zhen Zeng et al. - Flowmind: Automatic workflow generation with llms (2023)</li>
                            <li><strong>[334]</strong> Yifei Zhou et al. - Self-challenging language model agents (2025)</li>
                            <li><strong>[335]</strong> Weizhe Yuan et al. - Self-rewarding language models (2024)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Self-Evolution and Workflows</h4>
                            <div class="analysis-item">
                                <h5>Key Topics</h5>
                                <ul>
                                    <li><strong>Self-Improvement:</strong> Autonomous agent enhancement</li>
                                    <li><strong>Workflow Generation:</strong> Automatic task flows</li>
                                    <li><strong>Multi-Agent:</strong> Reflective collaboration</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            },
            100: {
                title: "References (Part 26)",
                content: `
                    <div class="article-header">
                        <div class="section-label">References</div>
                        <h1>References (Part 26)</h1>
                    </div>
                    <div class="original-content">
                        <h3>Continued References [336-349]</h3>
                        <ul>
                            <li><strong>[336]</strong> Toby Simonds et al. - Self rewarding self improving (2025)</li>
                            <li><strong>[337]</strong> Jiangqiao Lu et al. - Self: Self-evolution with language feedback (2023)</li>
                            <li><strong>[338]</strong> Aviral Kumar et al. - Training language models to self-correct via reinforcement learning (2024)</li>
                            <li><strong>[339]</strong> Mert Yuksekgonul et al. - Textgrad: Automatic "differentiation" via text (2024)</li>
                            <li><strong>[340]</strong> Tevin Wang and Chenyu Xiong - Autorule: Reasoning chain-of-thought extracted rule-based rewards improve preference learning (2025)</li>
                            <li><strong>[341]</strong> Mengkang Hu et al. - Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation (2025)</li>
                            <li><strong>[342]</strong> Haotian Sun et al. - Adaplanner: Adaptive planning from feedback with language models (2023)</li>
                            <li><strong>[343]</strong> Maxime Robeyns et al. - A self-improving coding agent (2025)</li>
                            <li><strong>[344]</strong> Zihan Wang et al. - Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning (2025)</li>
                            <li><strong>[345]</strong> Borui Wang et al. - Dystil: Dynamic strategy induction with large language models for reinforcement learning (2025)</li>
                            <li><strong>[346]</strong> Tianle Cai et al. - Large language models as tool makers (2024)</li>
                            <li><strong>[347]</strong> Lifan Yuan et al. - CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets (2024)</li>
                            <li><strong>[348]</strong> Cheng Qian et al. - CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models (2023)</li>
                            <li><strong>[349]</strong> Georg Wölflein et al. - Llm agents making agent tools (2025)</li>
                        </ul>
                    </div>
                    <div class="analysis-section">
                        <h3>Analysis & Explanation</h3>
                        <div class="analysis-block">
                            <h4>Self-Evolution and Tool Creation</h4>
                            <div class="analysis-item">
                                <h5>Key Contributions</h5>
                                <ul>
                                    <li><strong>Self-Correction:</strong> RL-based self-improvement</li>
                                    <li><strong>TextGrad:</strong> Text-based differentiation</li>
                                    <li><strong>Tool Makers:</strong> LLMs creating their own tools</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                `
            }
        };

        // ==================== INITIALIZATION ====================
        document.addEventListener('DOMContentLoaded', init);

        function init() {
            // Check URL hash for page number (e.g., #page=10)
            const hash = window.location.hash;
            if (hash) {
                const match = hash.match(/page=(\d+)/);
                if (match) {
                    const page = parseInt(match[1]);
                    if (page >= 1 && page <= TOTAL_PAGES) {
                        currentPage = page;
                    }
                }
            }
            loadPage(currentPage);
            setView(currentView);
            setupEventListeners();
            updateUI();

            // Flash controls to draw user attention
            setTimeout(() => {
                // Flash the menu button
                const menuTrigger = document.getElementById('menuTrigger');
                menuTrigger.classList.add('flash');
                setTimeout(() => menuTrigger.classList.remove('flash'), 1500);

                // Flash the keyboard hints (heavy)
                const keyboardHint = document.querySelector('.keyboard-hint');
                if (keyboardHint) {
                    keyboardHint.classList.add('flash');
                    setTimeout(() => keyboardHint.classList.remove('flash'), 2500);
                }
            }, 500);

            // Listen for hash changes
            window.addEventListener('hashchange', () => {
                const match = window.location.hash.match(/page=(\d+)/);
                if (match) {
                    goToPage(parseInt(match[1]));
                }
            });
        }

        function setupEventListeners() {
            // Menu trigger
            document.getElementById('menuTrigger').addEventListener('click', toggleMenu);
            document.getElementById('menuOverlay').addEventListener('click', closeMenu);

            // Navigation
            document.getElementById('prevBtn').addEventListener('click', () => navigate(-1));
            document.getElementById('nextBtn').addEventListener('click', () => navigate(1));
            document.getElementById('pageInput').addEventListener('change', (e) => goToPage(parseInt(e.target.value)));

            // Search
            document.getElementById('searchInput').addEventListener('input', (e) => handleSearch(e.target.value));

            // Keyboard shortcuts
            document.addEventListener('keydown', handleKeyboard);
        }

        // ==================== MENU ====================
        function toggleMenu() {
            menuOpen = !menuOpen;
            document.getElementById('menuTrigger').classList.toggle('active', menuOpen);
            document.getElementById('contextMenu').classList.toggle('active', menuOpen);
            document.getElementById('menuOverlay').classList.toggle('active', menuOpen);
        }

        function closeMenu() {
            menuOpen = false;
            document.getElementById('menuTrigger').classList.remove('active');
            document.getElementById('contextMenu').classList.remove('active');
            document.getElementById('menuOverlay').classList.remove('active');
        }

        // ==================== NAVIGATION ====================
        function navigate(delta) {
            const newPage = currentPage + delta;
            if (newPage >= 1 && newPage <= TOTAL_PAGES) {
                goToPage(newPage);
            }
        }

        function goToPage(page) {
            if (page < 1 || page > TOTAL_PAGES) return;
            currentPage = page;
            loadPage(page);
            updateUI();
            window.scrollTo({ top: 0, behavior: 'smooth' });
            // Update URL hash without triggering hashchange
            history.replaceState(null, null, `#page=${page}`);
        }

        function updateUI() {
            // Progress bar
            const progress = (currentPage / TOTAL_PAGES) * 100;
            document.getElementById('progressBar').style.width = `${progress}%`;

            // Page badge
            document.getElementById('pageBadge').textContent = `Page ${currentPage} of ${TOTAL_PAGES}`;

            // Menu page input
            document.getElementById('pageInput').value = currentPage;

            // Menu page title
            const data = pageData[currentPage];
            document.getElementById('menuPageTitle').textContent = data ? data.title : `Page ${currentPage}`;

            // Nav buttons
            document.getElementById('prevBtn').disabled = currentPage === 1;
            document.getElementById('nextBtn').disabled = currentPage === TOTAL_PAGES;
        }

        // ==================== PAGE LOADING ====================
        function loadPage(page) {
            const reader = document.getElementById('reader');
            const data = pageData[page];

            const zoomButtonsHTML = ZOOM_LEVELS.map(level =>
                `<button class="zoom-btn ${currentZoom === level ? 'active' : ''}" onclick="setZoom(${level})">${level}%</button>`
            ).join('');

            const imageHTML = `
                <div class="image-view ${currentView === 'image' ? '' : 'hidden'}" id="imageView">
                    <div class="zoom-trigger"></div>
                    <div class="zoom-controls">
                        <span>Zoom</span>
                        ${zoomButtonsHTML}
                    </div>
                    <img src="agentic_135/agentic_135-${String(page).padStart(3, '0')}.png"
                         alt="Page ${page}"
                         class="zoom-${currentZoom}"
                         id="pageImage">
                </div>
            `;

            const textHTML = data ? `
                <div class="text-view ${currentView === 'text' ? 'active' : ''}" id="textView">
                    ${data.content}
                </div>
            ` : `
                <div class="text-view ${currentView === 'text' ? 'active' : ''}" id="textView">
                    <div class="article-header">
                        <div class="section-label">Page ${page}</div>
                        <h1>Content</h1>
                    </div>
                    <div class="original-content">
                        <p>Full transcription and analysis for this page is available in the image view. Use the menu or press <kbd>V</kbd> to switch views.</p>
                    </div>
                </div>
            `;

            reader.innerHTML = imageHTML + textHTML;
        }

        // ==================== VIEW TOGGLE ====================
        function setView(view) {
            currentView = view;
            localStorage.setItem('readerView', view);

            const imageView = document.getElementById('imageView');
            const textView = document.getElementById('textView');

            if (imageView) imageView.classList.toggle('hidden', view !== 'image');
            if (textView) textView.classList.toggle('active', view === 'text');

            // Update menu items
            document.getElementById('viewImage').classList.toggle('active', view === 'image');
            document.getElementById('viewText').classList.toggle('active', view === 'text');

            closeMenu();
        }

        function toggleView() {
            setView(currentView === 'image' ? 'text' : 'image');
        }

        // ==================== IMAGE ZOOM ====================
        function setZoom(level) {
            currentZoom = level;
            localStorage.setItem('readerZoom', level);

            const img = document.getElementById('pageImage');
            if (img) {
                // Remove all zoom classes
                ZOOM_LEVELS.forEach(l => img.classList.remove(`zoom-${l}`));
                // Add current zoom class
                img.classList.add(`zoom-${level}`);
            }

            // Update button states
            document.querySelectorAll('.zoom-btn').forEach(btn => {
                btn.classList.toggle('active', btn.textContent === `${level}%`);
            });
        }

        function openFullscreen() {
            window.open(`agentic_135/agentic_135-${String(currentPage).padStart(3, '0')}.png`, '_blank');
            closeMenu();
        }

        // ==================== SEARCH ====================
        function handleSearch(query) {
            const container = document.getElementById('searchResults');
            if (query.length < 2) {
                container.innerHTML = '';
                return;
            }

            const results = [];
            const lowerQuery = query.toLowerCase();

            for (const [pageNum, data] of Object.entries(pageData)) {
                if (data.content.toLowerCase().includes(lowerQuery) ||
                    data.title.toLowerCase().includes(lowerQuery)) {
                    results.push({ page: parseInt(pageNum), title: data.title });
                }
            }

            if (results.length > 0) {
                container.innerHTML = results.slice(0, 5).map(r => `
                    <div class="search-result" onclick="goToPage(${r.page}); closeMenu();">
                        <div class="search-result-page">Page ${r.page}</div>
                        <div>${r.title}</div>
                    </div>
                `).join('');
            } else {
                container.innerHTML = '<div class="search-result">No results</div>';
            }
        }

        // ==================== KEYBOARD SHORTCUTS ====================
        function handleKeyboard(e) {
            // Don't trigger if typing in input
            if (e.target.tagName === 'INPUT') return;

            switch(e.key) {
                case 'ArrowLeft':
                    navigate(-1);
                    break;
                case 'ArrowRight':
                    navigate(1);
                    break;
                case 'v':
                case 'V':
                    toggleView();
                    break;
                case 'm':
                case 'M':
                    toggleMenu();
                    break;
                case 'Escape':
                    closeMenu();
                    break;
                case 'h':
                case 'H':
                    window.location.href = 'index.html';
                    break;
            }
        }
    </script>
</body>
</html>
